[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Source data for this project.\n\nhttp://www.cs.cmu.edu/~ark/personas"
  },
  {
    "objectID": "full_network.html",
    "href": "full_network.html",
    "title": "The world of actors",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt, matplotlib as mpl\nimport numpy as np\nimport igraph as ig\nimport networkx as nx\nimport random\nfrom tqdm import tqdm\nimport statsmodels.formula.api as smf\nimport itertools\nimport pickle\n\n\ndf_edges: pd.DataFrame = pd.read_pickle('../data/generated/graph/actor_graph_full.grouped.pkl')\ndf_movies: pd.DataFrame = pd.read_pickle('../data/generated/preprocessed/movies.pkl')\ndf_actors = pd.read_pickle('../data/generated/preprocessed/characters.pkl')\n\n\n\n\n\nBefore we dive head first into computing the graph edges, we need to estimate how much space it will take. An upper bound for the number of edges is \\(\\mathcal{O}(\\text{number of movies} \\times \\text{number of actors per movie}^2)\\), since each movie links two actors together.\nWe compute the number of actors per movie.\n\n# Count the number of actors per movie\ndf_numactors_per_movie = (df_actors\n    .groupby('fb_movie_id')\n    [['fb_movie_id']]\n    .apply('count')\n    .rename(columns={'fb_movie_id': 'numactors_per_movie'})\n    .sort_values('numactors_per_movie', ascending=False)\n    .reset_index()\n)\ndf_numactors_per_movie['movie_name'] = df_movies.set_index('fb_id').loc[df_numactors_per_movie.fb_movie_id].movie_name.values\ndf_numactors_per_movie\n\n\n\n\n\n  \n    \n      \n      fb_movie_id\n      numactors_per_movie\n      movie_name\n    \n  \n  \n    \n      0\n      /m/0cc8620\n      115\n      Hemingway & Gellhorn\n    \n    \n      1\n      /m/04grkmd\n      87\n      Taking Woodstock\n    \n    \n      2\n      /m/02725hs\n      81\n      Captain Corelli's Mandolin\n    \n    \n      3\n      /m/02qr3k8\n      81\n      Terror in the Aisles\n    \n    \n      4\n      /m/02825cv\n      72\n      Walk Hard: The Dewey Cox Story\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      64324\n      /m/02rxj4c\n      1\n      Primo\n    \n    \n      64325\n      /m/0bh8lpd\n      1\n      The Stranger Who Looks Like Me\n    \n    \n      64326\n      /m/0bh8np6\n      1\n      On the Wing\n    \n    \n      64327\n      /m/0bh8q94\n      1\n      Robot Bastard!\n    \n    \n      64328\n      /m/06zjk3b\n      1\n      Incident\n    \n  \n\n64329 rows × 3 columns\n\n\n\n\nax = sns.histplot(df_numactors_per_movie, x='numactors_per_movie', binwidth=1, log_scale=(False, True), element='step')\nax.set_xlabel('Number of actors per movie')\nax.set_ylabel('Number of movies')\nax.set_title('Distribution of actors per movie')\nplt.show()\n\n\n\n\n\nprint(f'mean={df_numactors_per_movie.numactors_per_movie.mean():.2f}')\nprint(f'median={df_numactors_per_movie.numactors_per_movie.median():.2f}')\n\nmean=7.01\nmedian=6.00\n\n\nThe distribution of actors per movie is clearly exponential (with a somewhat long tail due to outliers). Let’s be pessimistic and pick the largest of the median an the mean, and compute the upper bound.\n\nprint(f'num_edges ~ {(float(df_numactors_per_movie.numactors_per_movie.mean())**2 * len(df_actors.fb_movie_id.unique())):.1e}')\n\nnum_edges ~ 3.2e+06\n\n\n\n\n\nWith order \\(10^6\\) edges we will not be doing any full graph visualization, however we can do some analysis on the large graph.\n\nlen(df_edges)\n\n2080273\n\n\n\ng = ig.Graph.DataFrame(df_edges[['actor1_fbid', 'actor2_fbid', 'movie_count']], directed=False, use_vids=False)\nprint(g.summary())\n\nIGRAPH UN-- 135061 2080273 -- \n+ attr: name (v), movie_count (e)\n\n\nActually performing the computation shows the estimation to be very pessimistic, as we end up with around 200k edges. (and around 224k edges if we keep edges between the two same actors distinct).\nThe graph contains around 135k actors, spanning movies released between 1888 and 2016.\n\n\n\n\nAmong other real-life networks, social networks typically exhibit power law distribution of degrees. These types of networks are said to be “scale-free”, due to the scale-invariance of the power law. We compute the distribution for the network at hand.\n\nseries_degrees = pd.Series(g.degree())\nseries_degrees.to_pickle('../data/publication/entire_graph/degree.pkl')\n\n\npdf = series_degrees.value_counts(normalize=True).sort_index()\nccdf = pdf.sort_index(ascending=False).cumsum()\n\n\nmask = (pdf.index > 10)\nmodel = smf.ols(formula=\"logN ~ logk\",\n    data=dict(logN=np.log10(pdf[mask].values), logk=np.log10(pdf[mask].index)))\nres = model.fit()\nres.summary()\ngamma, gamma_lower, gamma_upper = res.params['logk'], *res.conf_int().loc['logk']\n\n\nfig, ax = plt.subplots(constrained_layout=True)\nax.plot(pdf.index, pdf.values, linewidth=1, label='PDF')\nax.set_ylabel('PDF $p(k)$')\nax.set_xlabel('Degree $k$')\nax.set_title('Degree distribution of the whole network')\nax.set_xscale('log')\nax.set_yscale('log')\nax.autoscale(False)\nax.plot(pdf.index, 10**res.predict(dict(logk=np.log10(pdf.index))).values,\n    linewidth=1, label=f'power law fit\\n$p(k) \\\\sim 1/k^{{{-gamma:.2f} \\\\pm {gamma_upper-gamma_lower:.2f}}}$')\nax.legend()\n\nplt.show()\n\n\n\n\nThis degree distribution clearly follows a power law \\(p(k) \\sim k^{-\\gamma}\\), and we recover the exponent \\(\\gamma = 1+\\mu \\approx 2.15 \\pm 0.07\\).\nAccording to the Molly-Reed condition (Molloy and Reed 1995), the entire network is fully connected to a giant component if \\(\\langle k^2 \\rangle < 2 \\langle k \\rangle\\). In our case with a power law of \\(\\mu \\approx 1.15\\), the left hand side integral diverges, and so we expect to find a giant cluster. Although since \\(\\langle k \\rangle < \\infty\\), we expect the giant component has not yet absorbed all clusters (“percolation”), and some smaller localized clusters remain (Bouchaud J. P. 2015).\nTo put this result into perspective, the Internet has in-degree distribution with \\(\\mu \\approx 1\\), scientific papers have \\(\\mu \\approx 2\\), human sexual contacts have \\(\\mu \\approx 2.4\\) (Bouchaud J. P. 2015). We are therefore in the presence of a network in which large clusters play an important role.\n\n\n\nWe compute the connected components of the graph (clusters), as well as their diameter. As the true diameter computation scales as \\(\\mathcal O(\\text{number of nodes})^3\\), we resort to using an approximative algorithm.\n\n# we use networkx here, because igraph does not have the diameter approximation\n# and exact diameter is very slow to compute (O(V³))\ng = nx.from_pandas_edgelist(df_edges, source='actor1_fbid', target='actor2_fbid')\ndf_clusters = pd.DataFrame(\n    [\n        (len(c), nx.approximation.diameter(g.subgraph(c), seed=1))\n        for c in nx.connected_components(g)\n    ],\n    columns=['size', 'approx_diameter']\n)\ndf_clusters.to_pickle('../data/publication/entire_graph/clusters.pkl')\n\n\nprint(f'number of clusters = {len(df_clusters)}')\n\nnumber of clusters = 1272\n\n\n\nprint(f'the giant cluster encompasses approx {df_clusters[\"size\"].max() / g.number_of_nodes():.0%} of the total nodes')\n\nthe giant cluster encompasses approx 94% of the total nodes\n\n\n\nhistd = df_clusters.groupby('size')[['approx_diameter']].value_counts().sort_index().reset_index(name='count')\nax = sns.scatterplot(data=histd, x='size', y='count', hue='approx_diameter', size='approx_diameter', legend='full', hue_norm=(0, 4), sizes=(30, 200))\nsns.move_legend(ax, loc='upper right', title='Cluster diameter')\nax.axvline(100, 0.03, 0.4, linestyle='--', color='gray')\nax.annotate('exponential cutoff', xy=(120, 0.38), xycoords=('data', 'axes fraction'), color='gray')\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_xlabel('Cluster size')\nax.set_ylabel('Number of clusters')\nax.set_title('Distribution of cluster sizes and diameter')\nplt.show()\n\n\n\n\nAs expected, we indeed have a giant cluster containing \\(\\sim 10^5\\) actors, accounting for about \\(94\\%\\) of the total number of actors in the graph.\nWe also remark that even though the cluster size spans orders of magnitude, the cluster diameter spans between 1 and 14. The small clusters contain up to \\(10^2\\) actors have diameter at most 3, and their size seem to roughly follow a power law with exponential cutoff [2]. Albeit its exponentially large size, the giant component has a diameter of 14, which is in-line with the small-world characteristic of scale-free networks : the diameter of the network scales only logarithmically with its size, i.e. \\(d \\sim \\log N\\) [3].\n[3] barabasi_2000"
  },
  {
    "objectID": "belgian.html",
    "href": "belgian.html",
    "title": "The world of actors",
    "section": "",
    "text": "import networkx as nx\nimport igraph as ig\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\nimport numpy as np\nimport random\n\n\n\n\ndf_actors = pd.read_pickle('../data/generated/preprocessed/characters.pkl')\ndf_movies = pd.read_pickle('../data/generated/preprocessed/movies.pkl')\n# we use the grouped edges, since networkx discards repeated edges, even with differents attributes\ndf_edges = pd.read_pickle('../data/generated/graph/actor_graph_full.grouped.pkl')\ndf_edges_ug = pd.read_pickle('../data/generated/graph/actor_graph_full.ungrouped.pkl')\n\n\ng_all = nx.from_pandas_edgelist(df_edges, 'actor1_fbid', 'actor2_fbid', 'movie_count')\n\n\nprint(g_all)\n\nGraph with 135061 nodes and 2080273 edges\n\n\n\n\n\n\ndf_belgian_actors = (df_actors\n    .query('actor_nationality == \"Belgium\"')\n    .dropna(subset='fb_actor_id')\n    .drop_duplicates(subset=['fb_actor_id'])  # because might contain multiple rows where same actor plays different characters\n    .sort_values('fb_actor_id')  # to have reproducible results, since GraphVis FDP layout is non-deterministic\n    .reset_index(drop=True)\n)\n# df_belgian_actors = df_belgian_actors.sample(frac=1, random_state=0).reset_index(drop=True)  # shuffle rows with random seed until I get a nice vis from GraphVis\ndf_belgian_actors\n\n\n\n\n\n  \n    \n      \n      wiki_id\n      fb_movie_id\n      release\n      character_name\n      actor_age\n      fb_char_id\n      fb_actor_id\n      release_year\n      actor_birth_year\n      actor_name\n      actor_birth\n      ethnicity\n      actor_height\n      actor_gender\n      actor_nationality\n      pageviews\n    \n  \n  \n    \n      0\n      11105410\n      /m/02r061p\n      1982\n      NaN\n      31.0\n      /m/04httm4\n      /m/01swrd\n      1982\n      1950\n      Chantal Akerman\n      1950-06-06\n      NaN\n      NaN\n      F\n      Belgium\n      NaN\n    \n    \n      1\n      24827706\n      /m/080q9v1\n      2002-05-17\n      NaN\n      39.0\n      /m/0gc9nqz\n      /m/01wyxjj\n      2002\n      1962\n      Lio\n      1962-06-17\n      NaN\n      NaN\n      F\n      Belgium\n      NaN\n    \n    \n      2\n      5875250\n      /m/0fb84m\n      2006\n      NaN\n      53.0\n      /m/0cfzkk6\n      /m/0266rk5\n      2006\n      1952\n      Ronald Guttman\n      1952-08-12\n      NaN\n      NaN\n      M\n      Belgium\n      99774.0\n    \n    \n      3\n      25839955\n      /m/09v8kcn\n      2009\n      NaN\n      33.0\n      /m/0gcr9sv\n      /m/026nm_f\n      2009\n      1975\n      R.Kan Albay\n      1975-09-01\n      NaN\n      NaN\n      M\n      Belgium\n      NaN\n    \n    \n      4\n      34748638\n      /m/0j3d9tn\n      2012-05-17\n      Alain van Versch\n      34.0\n      /m/0jx6788\n      /m/027ch2s\n      2012\n      1977\n      Matthias Schoenaerts\n      1977-12-08\n      NaN\n      NaN\n      M\n      Belgium\n      127172.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      246\n      22465221\n      /m/05zv1vg\n      1973\n      Deputy\n      54.0\n      /m/0n1pbd0\n      /m/0n1pbdb\n      1973\n      1918\n      René Hainaux\n      1918-04-19\n      NaN\n      NaN\n      M\n      Belgium\n      NaN\n    \n    \n      247\n      21031907\n      /m/05b28z4\n      1949-11-14\n      Elsa\n      20.0\n      /m/0n1t4f9\n      /m/0n1t4fm\n      1949\n      1929\n      Anne Campion\n      1929-01-07\n      NaN\n      NaN\n      F\n      Belgium\n      NaN\n    \n    \n      248\n      16949792\n      /m/04131_9\n      1974-09-11\n      Amandine\n      24.0\n      /m/0n21v7b\n      /m/0n21v7n\n      1974\n      1950\n      Marianne Eggerickx\n      1950-05-03\n      NaN\n      NaN\n      F\n      Belgium\n      68369.0\n    \n    \n      249\n      36035404\n      /m/0j_6phf\n      2012-02-29\n      Radu\n      NaN\n      /m/0n4jkxv\n      /m/0n4jky4\n      2012\n      <NA>\n      Gökhan Girginol\n      NaN\n      NaN\n      NaN\n      M\n      Belgium\n      NaN\n    \n    \n      250\n      21031066\n      /m/05b1qcd\n      1949-09\n      Van Putzeboom\n      NaN\n      /m/0n4yvz1\n      /m/0n4yvzc\n      1949\n      1910\n      Victor Guyau\n      1910-07-29\n      NaN\n      NaN\n      M\n      Belgium\n      NaN\n    \n  \n\n251 rows × 16 columns\n\n\n\n\n# all belgian actors have a name ! we truely live in a society...\nassert not any(df_belgian_actors.actor_name.isna())\n\n\n# we lose quite a few actors since many belgian actors are connected to only non-belgian actors\ng_belgian = g_all.subgraph(df_belgian_actors.fb_actor_id).copy()\nprint(g_belgian)\n\nGraph with 250 nodes and 558 edges\n\n\n\n# filter out self-loops and isolated nodes\ng_belgian.remove_edges_from(nx.selfloop_edges(g_belgian))  # normally there should be no self-loops in the edgelist, but for sanity\ng_belgian.remove_nodes_from(list(nx.isolates(g_belgian)))\nprint(g_belgian)\n\nGraph with 191 nodes and 558 edges\n\n\n\n\n\n\n# create a list of connected components\ngsub_belgian = [g_belgian.subgraph(c).copy() for c in nx.connected_components(g_belgian)]\n\n\nfor G in gsub_belgian:\n    print(G)\n\nGraph with 156 nodes and 526 edges\nGraph with 3 nodes and 3 edges\nGraph with 5 nodes and 10 edges\nGraph with 3 nodes and 3 edges\nGraph with 3 nodes and 3 edges\nGraph with 3 nodes and 2 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 2 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 2 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 2 edges\nGraph with 3 nodes and 2 edges\n\n\n-> There is one large component, and a few small isolated subgraphs\n\ng_belgian = max(gsub_belgian, key=lambda G: G.number_of_nodes())\nprint(g_belgian)\n\nGraph with 156 nodes and 526 edges\n\n\n\n\n\n\n# merge actor names\ndf_edges_belgian = nx.to_pandas_edgelist(g_belgian, source='actor1_fbid', target='actor2_fbid')\ndf_edges_belgian['actor1_name'] = df_belgian_actors.set_index('fb_actor_id').loc[df_edges_belgian.actor1_fbid.values].actor_name.values\ndf_edges_belgian['actor2_name'] = df_belgian_actors.set_index('fb_actor_id').loc[df_edges_belgian.actor2_fbid.values].actor_name.values\ndf_edges_belgian\n\n\n\n\n\n  \n    \n      \n      actor1_fbid\n      actor2_fbid\n      movie_count\n      actor1_name\n      actor2_name\n    \n  \n  \n    \n      0\n      /m/0gc50br\n      /m/0gc4q3w\n      1\n      Véronique Dumont\n      Francois De Brigode\n    \n    \n      1\n      /m/0gc50br\n      /m/0gc4cmz\n      1\n      Véronique Dumont\n      Nicolas Buysse\n    \n    \n      2\n      /m/0gc50br\n      /m/0c0ctvb\n      1\n      Véronique Dumont\n      Alexandre von Sivers\n    \n    \n      3\n      /m/0gc50br\n      /m/0282b9n\n      1\n      Véronique Dumont\n      Bouli Lanners\n    \n    \n      4\n      /m/0gc50br\n      /m/037b56\n      1\n      Véronique Dumont\n      Benoît Poelvoorde\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      521\n      /m/0cm1m3w\n      /m/027ch2s\n      1\n      Robbie Cleiren\n      Matthias Schoenaerts\n    \n    \n      522\n      /m/0mzjrdl\n      /m/0jt8hps\n      1\n      Romain DeConinck\n      Charles Janssens\n    \n    \n      523\n      /m/0mzjrdl\n      /m/0mzflxn\n      1\n      Romain DeConinck\n      Carlos van Lanckere\n    \n    \n      524\n      /m/0gc9zpq\n      /m/0gc47kq\n      1\n      Axel Daeseleire\n      Jaela Cole\n    \n    \n      525\n      /m/0mzflxn\n      /m/0jt8hps\n      1\n      Carlos van Lanckere\n      Charles Janssens\n    \n  \n\n526 rows × 5 columns\n\n\n\n\n# merge movie ids\ndf_edges_belgian = df_edges_belgian.merge(\n    (df_edges_belgian\n        # find the movies in edges i -> j\n        .merge(pd.concat(\n            # include edges i -> j and j -> i\n            [df_edges_ug, df_edges_ug.rename(columns={'actor1_fbid': 'actor2_fbid', 'actor2_fbid': 'actor1_fbid'})],\n            ignore_index=True  # WARNING : this is important, since otherwise there are duplicate indices\n        ), how='left', on=['actor1_fbid', 'actor2_fbid'])\n        # group by edge\n        .groupby(['actor1_fbid', 'actor2_fbid'])\n        # find all the movie_fbids corresponding to that edge\n        [['movie_fbid']]\n        # .agg({'movie_fbid': 'unique'})  # list of movies in common\n        .first()\n        # prepare merge\n        .rename(columns={'movie_fbid': 'first_movie_fbid'})\n        .reset_index()\n    ),\n    how='left', on=['actor1_fbid', 'actor2_fbid'],\n)\ndf_edges_belgian['first_movie_numid'] = pd.factorize(df_edges_belgian.first_movie_fbid)[0]\ndf_edges_belgian\n\n\n\n\n\n  \n    \n      \n      actor1_fbid\n      actor2_fbid\n      movie_count\n      actor1_name\n      actor2_name\n      first_movie_fbid\n      first_movie_numid\n    \n  \n  \n    \n      0\n      /m/0gc50br\n      /m/0gc4q3w\n      1\n      Véronique Dumont\n      Francois De Brigode\n      /m/063zzkb\n      0\n    \n    \n      1\n      /m/0gc50br\n      /m/0gc4cmz\n      1\n      Véronique Dumont\n      Nicolas Buysse\n      /m/063zzkb\n      0\n    \n    \n      2\n      /m/0gc50br\n      /m/0c0ctvb\n      1\n      Véronique Dumont\n      Alexandre von Sivers\n      /m/063zzkb\n      0\n    \n    \n      3\n      /m/0gc50br\n      /m/0282b9n\n      1\n      Véronique Dumont\n      Bouli Lanners\n      /m/063zzkb\n      0\n    \n    \n      4\n      /m/0gc50br\n      /m/037b56\n      1\n      Véronique Dumont\n      Benoît Poelvoorde\n      /m/063zzkb\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      521\n      /m/0cm1m3w\n      /m/027ch2s\n      1\n      Robbie Cleiren\n      Matthias Schoenaerts\n      /m/0c01rxz\n      57\n    \n    \n      522\n      /m/0mzjrdl\n      /m/0jt8hps\n      1\n      Romain DeConinck\n      Charles Janssens\n      /m/05t0jwx\n      8\n    \n    \n      523\n      /m/0mzjrdl\n      /m/0mzflxn\n      1\n      Romain DeConinck\n      Carlos van Lanckere\n      /m/05t0jwx\n      8\n    \n    \n      524\n      /m/0gc9zpq\n      /m/0gc47kq\n      1\n      Axel Daeseleire\n      Jaela Cole\n      /m/05f6jzj\n      19\n    \n    \n      525\n      /m/0mzflxn\n      /m/0jt8hps\n      1\n      Carlos van Lanckere\n      Charles Janssens\n      /m/05t0jwx\n      8\n    \n  \n\n526 rows × 7 columns\n\n\n\n\n# add colors to edges\nedgecmap = plt.cm.gist_rainbow(np.linspace(0, 1, df_edges_belgian.first_movie_fbid.nunique()))\nedgecmap[:, 3] = 0.6  # alpha channel\ndf_edges_belgian['color'] = df_edges_belgian.first_movie_numid.map(lambda n: tuple(edgecmap[n]))\ndf_edges_belgian\n\n\n\n\n\n  \n    \n      \n      actor1_fbid\n      actor2_fbid\n      movie_count\n      actor1_name\n      actor2_name\n      first_movie_fbid\n      first_movie_numid\n      color\n    \n  \n  \n    \n      0\n      /m/0gc50br\n      /m/0gc4q3w\n      1\n      Véronique Dumont\n      Francois De Brigode\n      /m/063zzkb\n      0\n      (1.0, 0.0, 0.16, 0.6)\n    \n    \n      1\n      /m/0gc50br\n      /m/0gc4cmz\n      1\n      Véronique Dumont\n      Nicolas Buysse\n      /m/063zzkb\n      0\n      (1.0, 0.0, 0.16, 0.6)\n    \n    \n      2\n      /m/0gc50br\n      /m/0c0ctvb\n      1\n      Véronique Dumont\n      Alexandre von Sivers\n      /m/063zzkb\n      0\n      (1.0, 0.0, 0.16, 0.6)\n    \n    \n      3\n      /m/0gc50br\n      /m/0282b9n\n      1\n      Véronique Dumont\n      Bouli Lanners\n      /m/063zzkb\n      0\n      (1.0, 0.0, 0.16, 0.6)\n    \n    \n      4\n      /m/0gc50br\n      /m/037b56\n      1\n      Véronique Dumont\n      Benoît Poelvoorde\n      /m/063zzkb\n      0\n      (1.0, 0.0, 0.16, 0.6)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      521\n      /m/0cm1m3w\n      /m/027ch2s\n      1\n      Robbie Cleiren\n      Matthias Schoenaerts\n      /m/0c01rxz\n      57\n      (0.1843563512361468, 0.0, 1.0, 0.6)\n    \n    \n      522\n      /m/0mzjrdl\n      /m/0jt8hps\n      1\n      Romain DeConinck\n      Charles Janssens\n      /m/05t0jwx\n      8\n      (1.0, 0.4313725490196079, 0.0, 0.6)\n    \n    \n      523\n      /m/0mzjrdl\n      /m/0mzflxn\n      1\n      Romain DeConinck\n      Carlos van Lanckere\n      /m/05t0jwx\n      8\n      (1.0, 0.4313725490196079, 0.0, 0.6)\n    \n    \n      524\n      /m/0gc9zpq\n      /m/0gc47kq\n      1\n      Axel Daeseleire\n      Jaela Cole\n      /m/05f6jzj\n      19\n      (0.7207207207207207, 1.0, 0.0, 0.6)\n    \n    \n      525\n      /m/0mzflxn\n      /m/0jt8hps\n      1\n      Carlos van Lanckere\n      Charles Janssens\n      /m/05t0jwx\n      8\n      (1.0, 0.4313725490196079, 0.0, 0.6)\n    \n  \n\n526 rows × 8 columns\n\n\n\n\n# add names to the actors\nnx.set_node_attributes(g_belgian, (df_actors\n    .set_index('fb_actor_id')\n    .loc[pd.concat([df_edges_belgian.actor1_fbid, df_edges_belgian.actor2_fbid]).unique()]\n    .actor_name\n    .map(lambda name: name.replace(' ', '\\n'))\n    .to_dict()\n), 'name')\n\n\n# add names to the actors\nnx.set_edge_attributes(g_belgian, (df_edges_belgian\n    .set_index(['actor1_fbid', 'actor2_fbid'])\n    .color\n    .to_dict()\n), 'color')\n\n\n\n\nmovie_count_hist = df_edges_belgian.movie_count.value_counts().sort_index()\nfig, ax = plt.subplots()\nax.bar(movie_count_hist.index, movie_count_hist.values, width=0.5)\nax.set_xticks(movie_count_hist.index)\nax.set_yscale('log')\nax.set_title('Number of shared movies between belgian actors')\nax.set_xlabel('Number of shared movie')\nax.set_ylabel('Number of edges')\nplt.show()\n\n\n\n\n\nprint(f'number of movies displayed in the graph : {df_edges_belgian.first_movie_fbid.nunique()}')\n\nnumber of movies displayed in the graph : 72\n\n\nMost of the actors feature together in one movie. We list out the ones that feature together in more than 3 movies\n\ndf_edges_belgian.query('movie_count >= 3')\n\n\n\n\n\n  \n    \n      \n      actor1_fbid\n      actor2_fbid\n      movie_count\n      actor1_name\n      actor2_name\n      first_movie_fbid\n      first_movie_numid\n      color\n    \n  \n  \n    \n      48\n      /m/0c08q1\n      /m/04mkd6k\n      3\n      Lucas Belvaux\n      Patrick Descamps\n      /m/05308hq\n      11\n      (1.0, 0.6645468998410176, 0.0, 0.6)\n    \n    \n      95\n      /m/0668qk\n      /m/0gqt7h\n      4\n      Olivier Gourmet\n      Jérémie Renier\n      /m/03y7fp2\n      22\n      (0.487546369899311, 1.0, 0.0, 0.6)\n    \n    \n      469\n      /m/084ckj\n      /m/0gcxfmm\n      3\n      Yolande Moreau\n      Serge Larivière\n      /m/05sxh20\n      64\n      (0.7171781756180736, 0.0, 1.0, 0.6)\n    \n    \n      474\n      /m/0282b9n\n      /m/037b56\n      3\n      Bouli Lanners\n      Benoît Poelvoorde\n      /m/063zzkb\n      0\n      (1.0, 0.0, 0.16, 0.6)\n    \n  \n\n\n\n\n\n\n\n\nWe visualize the actor graph, coloring edges based on the movie featuring the actors it connects, and making edges larger for more movies played together.\n\ndef draw_network(g, ax, vertex_size: str | float = 3.0, vertex_color: str = '#0003', min_vertex_size: float = 1.0):\n    gig = ig.Graph.from_networkx(g)\n    random.seed(7)  # good seeds : 7, 10, 16, 42, 56\n\n    vertex_order = None\n    if isinstance(vertex_size, str):\n        vertex_size = np.array(gig.vs[vertex_size])*50\n        vertex_order = np.argsort(vertex_size)[::-1]\n        vertex_size = np.maximum(min_vertex_size, vertex_size)\n\n    if not vertex_color.startswith('#'):\n        vertex_color=gig.vs[vertex_color]\n\n    ig.plot(\n        gig, target=ax,\n        layout='davidson_harel',\n        vertex_label=gig.vs['name'],\n        vertex_label_size=6.5, vertex_size=vertex_size, vertex_color=vertex_color,\n        vertex_frame_color='white', vertex_frame_width=2.0,\n        vertex_order=vertex_order,\n        edge_color=gig.es['color'], edge_width=gig.es['movie_count'],\n        background='black'\n    )\n\n\nfig, ax = plt.subplots(figsize=(15, 15))\ndraw_network(g_belgian, ax)\nplt.show()\n\n\n\n\nVisually already, communities start to appear, and we can get a sense of centrality. For example, Jan Decleir definitely stands out.\nOn the bottom of the graph, we can also visually inspect a small clique composed by Yolande Moreau, Serge Larivière, Benoît Poelvoorde, and Bouli Lanners, who have featured together multiple times.\nJean-Claude Van Damme, a well-known actor, appears marginal here. This is an effect of slicing the original graph, where Jean-Claude Van Damme was well-connected, but appeared in more international movies, with non-belgian actors.\n\n\n\nWe test how feasible some algorithms proposed in Methods are.\n\ndegree_centrality = nx.degree_centrality(g_belgian)\neigenvector_centrality = nx.eigenvector_centrality(g_belgian)\nbetweenness_centrality = nx.betweenness_centrality(g_belgian)\n\ndf_centrality = pd.DataFrame(degree_centrality.items(), columns=['fb_actor_id', 'degree_centrality'])\ndf_centrality = df_centrality.merge(pd.DataFrame(eigenvector_centrality.items(), columns=['fb_actor_id', 'eigenvector_centrality']), on='fb_actor_id', how='left')\ndf_centrality = df_centrality.merge(pd.DataFrame(betweenness_centrality.items(), columns=['fb_actor_id', 'betweenness_centrality']), on='fb_actor_id', how='left')\ndf_centrality['actor_name'] = df_belgian_actors.set_index('fb_actor_id').loc[df_centrality.fb_actor_id].actor_name.values\n\n\n\n\ndf_centrality.sort_values('degree_centrality', ascending=False)[['degree_centrality', 'actor_name', 'fb_actor_id']].head(5)\n\n\n\n\n\n  \n    \n      \n      degree_centrality\n      actor_name\n      fb_actor_id\n    \n  \n  \n    \n      2\n      0.264516\n      Jan Decleir\n      /m/04qjng\n    \n    \n      126\n      0.129032\n      Jérémie Renier\n      /m/0gqt7h\n    \n    \n      8\n      0.122581\n      François Beukelaers\n      /m/0bq9q76\n    \n    \n      152\n      0.116129\n      Matthias Schoenaerts\n      /m/027ch2s\n    \n    \n      134\n      0.116129\n      Filip Peeters\n      /m/0bl5pwj\n    \n  \n\n\n\n\n\ndf_centrality.sort_values('eigenvector_centrality', ascending=False)[['eigenvector_centrality', 'actor_name']].head(5)\n\n\n\n\n\n  \n    \n      \n      eigenvector_centrality\n      actor_name\n    \n  \n  \n    \n      2\n      0.408367\n      Jan Decleir\n    \n    \n      127\n      0.209502\n      Werner De Smedt\n    \n    \n      103\n      0.205922\n      Bert Haelvoet\n    \n    \n      134\n      0.204257\n      Filip Peeters\n    \n    \n      31\n      0.193717\n      Gert Portael\n    \n  \n\n\n\n\n\ndf_centrality.sort_values('betweenness_centrality', ascending=False)[['betweenness_centrality', 'actor_name']].head(5)\n\n\n\n\n\n  \n    \n      \n      betweenness_centrality\n      actor_name\n    \n  \n  \n    \n      2\n      0.395843\n      Jan Decleir\n    \n    \n      152\n      0.254062\n      Matthias Schoenaerts\n    \n    \n      8\n      0.116199\n      François Beukelaers\n    \n    \n      126\n      0.110377\n      Jérémie Renier\n    \n    \n      107\n      0.108920\n      Bouli Lanners\n    \n  \n\n\n\n\nJan Decleir comes out on top each time. Matthias Schoenaerts, François Beukelaers as well as Filip Peeters appear two times.\n\nfig, ax = plt.subplots(figsize=(12, 4), ncols=3, constrained_layout=True)\nsns.histplot(df_centrality, x='degree_centrality', ax=ax[0], bins=40)\nsns.histplot(df_centrality, x='eigenvector_centrality', ax=ax[1], bins=40)\nsns.histplot(df_centrality, x='betweenness_centrality', ax=ax[2], bins=40)\nfig.suptitle('Distribution of different centrality measures')\nplt.show()\n\n\n\n\nThe histograms reveal that eigenvector centrality produces a powerlaw-like distribution : only a few actors are “central”\nThere are many nodes with zero betweenness centrality : these might reveal actors who have only played in one movie, and therefore do not contribute to the general connectedness ?\nDegree centrality only yields local information, since it is just proportional to the local node degree. Therefore, it might not be as insightful\n\nnx.set_node_attributes(g_belgian, df_centrality.set_index('fb_actor_id').degree_centrality.to_dict(), 'degree')\nnx.set_node_attributes(g_belgian, df_centrality.set_index('fb_actor_id').eigenvector_centrality.to_dict(), 'eigenvector')\nnx.set_node_attributes(g_belgian, df_centrality.set_index('fb_actor_id').betweenness_centrality.to_dict(), 'betweenness')\n\n\nfig, axs = plt.subplots(figsize=(30, 15), ncols=3, dpi=200)\nfor vsize, ax in zip(['degree', 'eigenvector', 'betweenness'], axs.flat):\n    draw_network(g_belgian, ax, vertex_size=vsize)\n    ax.set_title(f'{vsize.capitalize()} centrality')\nplt.show()\n\n\n\n\nDegree centrality\nThe simplest measure of centrality yields the result that more connected seem more important\nEigenvector centrality\nA striking difference here is that there appears a clear “core”, but Jan Decleir, Jérémie Renier are still top-2 actors\nNodes connected to the high eigenvector centrality nodes are more likely to have a high eigenvector centrality.\nFurthermore, eigenvector centrality not only picks up on the “local” connectedness of an actor, but also its place in the whole graph -> probably more relevant for our analysis ?\nBetweenness centrality\nThe separation is much more extreme : Jean Decleir, Matthieu Schoenaerts, etc. appear as essential hubs to the graph. Note the interesting case of Senne Rouffaer, who is the only connection to a small cluster on the upper left, to the rest of the graph. This metric might be more important to interpret separation of communities. This is clearly visualized here\n\n\n\n\n\n\nThis graph is relatively small. We can get from any actor to any other in less than 8 hops.\n\nnx.diameter(g_belgian)\n\n8\n\n\n\n\n\nWe test the resilience of the graph to having nodes removed.\nTODO : do this with the average path length as a measure\n\ndef repeat_minimum_node_cut(g, max_iter: int = 20):\n    g = g.copy()\n    number_of_nodes = [g.number_of_nodes()]\n    centrality_of_nodes_removed = [np.array(np.nan)]\n    for i in range(max_iter):\n        nodes_to_remove = nx.minimum_node_cut(g)\n        g.remove_nodes_from(nodes_to_remove)\n        df_removed = df_centrality.set_index('fb_actor_id').loc[list(nodes_to_remove)]\n        centrality_of_nodes_removed.append(df_removed.betweenness_centrality.values)\n        gsub = [g.subgraph(c).copy() for c in nx.connected_components(g)]\n        g = max(gsub, key=lambda G: G.number_of_nodes())\n        number_of_nodes.append(g.number_of_nodes())\n    return number_of_nodes, centrality_of_nodes_removed\n\n\ng_belgian_without_Decleir = g_belgian.copy()\ng_belgian_without_Decleir.remove_node('/m/04qjng')  # Remove Jan Decleir\nnumber_of_nodes0, centrality_removed0 = repeat_minimum_node_cut(g_belgian)\nnumber_of_nodes1, centrality_removed1 = repeat_minimum_node_cut(g_belgian_without_Decleir)\n\n\nfig, ax = plt.subplots()\nx = list(range(len(number_of_nodes0)))\nax.plot(x, number_of_nodes0, '--', color='tab:blue', label='original graph', zorder=1)\nax.plot(x, number_of_nodes1, '--', color='tab:orange', label='graph without Decleir', zorder=1)\nax.scatter(x, number_of_nodes0, s=np.hstack(centrality_removed0)*2e3, color='tab:blue', zorder=2)\nax.scatter(x, number_of_nodes1, s=np.hstack(centrality_removed1)*2e3, color='tab:orange', zorder=2)\nax.set_xticks(x)\nax.legend()\nax.set_xlabel('Iteration')\nax.set_ylabel('Number of nodes')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport networkx.algorithms.community as nxcommunity\nimport itertools\n\n\nnodecmap = [ mpl.colors.to_rgba(c, alpha=0.7) for c in plt.rcParams['axes.prop_cycle'].by_key()['color'] ]\n\n\n\nWe use the Clauset-Newman-Moore greedy modularity maximization to blindly try to find some communities.\n\ncommunities = nxcommunity.greedy_modularity_communities(g_belgian)\nlen(communities)\n\nnode2community = {}\nfor i, com in enumerate(communities):\n    for node in com:\n        node2community[node] = i\n\nnode2color = { node: nodecmap[node2community[node]] for node in g_belgian.nodes }\n\nnx.set_node_attributes(g_belgian, node2color, 'newman_color')\n\n9 communities have been found.\nNote : we can manually set an upper bound for the number of communities using the best_n kwarg\n\n\n\nThe louvain algorithm works on nodes (actors) instead of edges (movies), which is a much more natural way to think about the problem.\n\ncommunities = nxcommunity.louvain_communities(g_belgian, seed=0)\nlen(communities)\n\nnode2community = {}\nfor i, com in enumerate(communities):\n    for node in com:\n        node2community[node] = i\n\nnode2color = { node: nodecmap[node2community[node]] for node in g_belgian.nodes }\n\nnx.set_node_attributes(g_belgian, node2color, 'louvain_color')\n\n\n\n\n\nfig, axs = plt.subplots(figsize=(30, 15), ncols=2)\nfor color, ax in zip(['newman_color', 'louvain_color'], axs.flat):\n    draw_network(g_belgian, ax, vertex_size='eigenvector', vertex_color=color)\naxs[0].set_title('Clauset-Newman-Moore clustering')\naxs[1].set_title('Louvain clustering')\nplt.show()\n\n\n\n\nWe set a minimum vertex size, so that we can still see the clustering\nThe Newman communities found are not visually convicing, espectially the green one. I think this is because the modularity is a measure on the “edge importance”, and that in our case this does not make sense. Edges generated by each movie should intuitively have the same importance, rather, the actors should be the ones who are important.\nWe see that the Louvain algorithm generates visually more convincing results, which group actors together close in the force-directed graph\n\n\n\n\nIn the above graph, nodes are sized according to eigenvalue centrality and are colored according to (Louvain) community, and edges are colored according to the movie.\nKey observations : - Communities often group actors which seem to have similar eigenvalue centrality, and have played in the same movies. For instance, the green community is well connected by majoritarily two movies, colored cyan and purple. - Three communities stand out : red, gray and brown. With all three, they represent a large majority of the well ranked actors (according to eigenvalue centrality). These can be interpreted as the “popular” Belgian actors - The five other communities are that of smaller actors. These might correspond to more “niche” communities. - Smaller communities seem to rely on one or two entrypoint-actors. The pink community in the upper-right holds together solely through actor Senne Rouffaer. The green community has two entry-points : Johan Heldenberg, who provides a bridge with the (popular) brown community, and Matthias Schoenaerts, bridges with the more niche purple and blue communities."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "growth.html",
    "href": "growth.html",
    "title": "The world of actors",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt, matplotlib as mpl\nimport numpy as np\nimport igraph as ig\nimport random\nfrom tqdm import tqdm\nimport statsmodels.formula.api as smf\nimport itertools\nimport pickle\n\n\ndf_edges: pd.DataFrame = pd.read_pickle('../data/generated/graph/actor_graph_full.ungrouped.pkl')\ndf_movies: pd.DataFrame = pd.read_pickle('../data/generated/preprocessed/movies.pkl')\n\n\n\n\n\ndf_edges['movie_release'] = df_movies.set_index('fb_id').loc[df_edges.movie_fbid].movie_release.values\ndf_edges.sort_values('movie_release', inplace=True)\n\nprint(f'length before purge : {len(df_edges)}')\ndf_edges.dropna(subset='movie_release', inplace=True)  # some movies do not have dates associated to them\n# df_edges.drop(index=df_edges[((df_edges.movie_release <= min(years_bins)) | (df_edges.movie_release > max(years_bins)))].index, inplace=True)\n# df_edges.drop(index=df_edges[(df_edges.movie_release > 2015)].index, inplace=True)\nprint(f'length after purge : {len(df_edges)}')\n\n# assert min(years_bins) < df_edges.movie_release.min()\n# assert df_edges.movie_release.max() <= max(years_bins)\n\ndf_edges['movie_release_year'] = df_edges['movie_release'].map(lambda t: t.year).astype(pd.Int64Dtype())\n\n# we bin the years together to reduce noise on the subsequent analyses\n# we consider the graph at t=0 at year 1920\ndf_edges['year_bin'] = pd.cut(df_edges.query('movie_release_year > 1920').movie_release_year, range(1920, 2010+1, 10), precision=0)\ndf_edges['year_qbin'] = pd.qcut(df_edges.query('movie_release_year > 1920').movie_release_year, len(range(1920, 2010+1, 10))-1, precision=0)\n\ndf_edges\n\nlength before purge : 2241657\nlength after purge : 2224839\n\n\n\n\n\n\n  \n    \n      \n      actor1_fbid\n      actor2_fbid\n      movie_fbid\n      movie_release\n      movie_release_year\n      year_bin\n      year_qbin\n    \n  \n  \n    \n      678016\n      /m/0k2pyv\n      /m/0k2pz0\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n    \n    \n      678017\n      /m/0k2pyv\n      /m/0k2pz6\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n    \n    \n      678018\n      /m/0k2pyv\n      /m/0k2pzd\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n    \n    \n      678019\n      /m/0k2pz0\n      /m/0k2pz6\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n    \n    \n      678020\n      /m/0k2pz0\n      /m/0k2pzd\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1118305\n      /m/06k6ns\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n    \n    \n      1118306\n      /m/06k6ns\n      /m/0fdc74\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n    \n    \n      1118307\n      /m/0fdc74\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n    \n    \n      1118296\n      /m/0736qr\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n    \n    \n      1118302\n      /m/06k6ns\n      /m/0g7dfl\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n    \n  \n\n2224839 rows × 7 columns\n\n\n\n\nyears = df_edges.movie_release_year.unique().astype(int)\n\n\n\n\nWe consider the initial actor network at the year 1900\nhttps://barabasi.com/f/622.pdf\nIs there preferential attachment ?\n\n\nWe use numerical indices instead of freebase ids, in order to index into numpy arrays later\n\ncodes, uniques = pd.factorize(pd.concat([df_edges.actor1_fbid, df_edges.actor2_fbid]))\ndf_edges['actor1_numid'] = codes[:len(df_edges)]\ndf_edges['actor2_numid'] = codes[len(df_edges):]\ndf_edges\n\n\n\n\n\n  \n    \n      \n      actor1_fbid\n      actor2_fbid\n      movie_fbid\n      movie_release\n      movie_release_year\n      year_bin\n      year_qbin\n      actor1_numid\n      actor2_numid\n    \n  \n  \n    \n      678016\n      /m/0k2pyv\n      /m/0k2pz0\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n      0\n      1\n    \n    \n      678017\n      /m/0k2pyv\n      /m/0k2pz6\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n      0\n      2\n    \n    \n      678018\n      /m/0k2pyv\n      /m/0k2pzd\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n      0\n      119615\n    \n    \n      678019\n      /m/0k2pz0\n      /m/0k2pz6\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n      1\n      2\n    \n    \n      678020\n      /m/0k2pz0\n      /m/0k2pzd\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n      1\n      119615\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1118305\n      /m/06k6ns\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n      61010\n      52747\n    \n    \n      1118306\n      /m/06k6ns\n      /m/0fdc74\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n      61010\n      80735\n    \n    \n      1118307\n      /m/0fdc74\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n      80735\n      52747\n    \n    \n      1118296\n      /m/0736qr\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n      41038\n      52747\n    \n    \n      1118302\n      /m/06k6ns\n      /m/0g7dfl\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n      61010\n      74039\n    \n  \n\n2224839 rows × 9 columns\n\n\n\n\nprint(f'total {uniques.nunique()} nodes (actors)')\n\ntotal 134649 nodes (actors)\n\n\n\n\n\n\ndef get_edges_year(year: int) -> pd.DataFrame:\n    \"\"\"Return weighted edges for all movies released before and on ``year``\"\"\"\n    return (df_edges\n        .query(f'movie_release_year <= {year}')\n        .groupby(['actor1_numid', 'actor2_numid'])\n        [['movie_fbid']]\n        .nunique()\n        .reset_index()\n        .rename(columns={'movie_fbid': 'movie_count'})\n    )\n\n# test the function\nrandom.seed(0)  # seed the random layout\nfig, ax = plt.subplots()\ng0 = ig.Graph.DataFrame(get_edges_year(years[10])[['actor1_numid', 'actor2_numid', 'movie_count']], directed=False, use_vids=False)\nig.plot(g0, target=ax,\n    vertex_label=g0.vs['name'], vertex_label_size=7,\n    edge_width=g0.es['movie_count']\n);\n\n\n\n\n\n\nNote : actors keep their degree even after they die !\n\nif False:  # recompute\n    degrees = np.zeros((uniques.nunique(), df_edges.movie_release_year.nunique()))\n\n    for iyear, year in tqdm(enumerate(years), total=len(years)):\n        g = ig.Graph.DataFrame(get_edges_year(year)[['actor1_numid', 'actor2_numid', 'movie_count']], directed=False, use_vids=False)\n        # print(g.summary())\n        for node, k in zip(g.vs, g.degree()):\n            degrees[node['name'], iyear] = k\n    \n    np.savez('../data/generated/graph/network_growth/actor_graph_full_degrees.npz', degrees)\n\nelse:  # load from precomputed\n    degrees = np.load('../data/generated/graph/network_growth/actor_graph_full_degrees.npz')['arr_0']\n\n\ndelta_degrees = np.diff(degrees, axis=-1)\n\n\n# k = 0 corresponds to actors not yet born\ndegrees[degrees == 0] = np.nan\n# Delta k = 0 corresponds to actors that don't evolve\ndelta_degrees[delta_degrees == 0] = np.nan\n\n\n# convert the data to a dataframe\ndf_degree = pd.DataFrame(degrees, columns=years).melt(var_name='year', value_name='k', ignore_index=False)\ndf_degree.dropna(inplace=True)\ndf_degree.reset_index(inplace=True, names='actor_numid')\n\ndf_delta_degree = pd.DataFrame(delta_degrees, columns=years[:-1]).melt(var_name='year', value_name='dk', ignore_index=False)\ndf_delta_degree.dropna(inplace=True)\ndf_delta_degree.reset_index(inplace=True, names='actor_numid')\n\ndf_pref_attachment_delta = pd.merge(df_delta_degree, df_degree, how='inner', on=['actor_numid', 'year'])\n\n\ndf_pref_attachment_delta\n\n\n\n\n\n  \n    \n      \n      actor_numid\n      year\n      dk\n      k\n    \n  \n  \n    \n      0\n      7\n      1898\n      4.0\n      1.0\n    \n    \n      1\n      12\n      1898\n      4.0\n      1.0\n    \n    \n      2\n      7\n      1901\n      6.0\n      5.0\n    \n    \n      3\n      11\n      1901\n      6.0\n      5.0\n    \n    \n      4\n      12\n      1901\n      6.0\n      5.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      195683\n      51342\n      2015\n      1.0\n      149.0\n    \n    \n      195684\n      52861\n      2015\n      4.0\n      57.0\n    \n    \n      195685\n      61062\n      2015\n      1.0\n      114.0\n    \n    \n      195686\n      73797\n      2015\n      1.0\n      51.0\n    \n    \n      195687\n      79054\n      2015\n      1.0\n      76.0\n    \n  \n\n195688 rows × 4 columns\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nnp.random.seed(0)\n# we plot only 256 actors because plotting all 1.3M of them would take ages !!\nax.plot(years, degrees[np.random.randint(0, len(degrees), 256)].T, color='black', alpha=0.5, linewidth=1)\nax.set_xlabel('Year')\nax.set_ylabel('Degree')\nax.set_title('Actor degree as a function of time')\nplt.show()\n\n\n\n\nThe degree seems to grow linearly in time, but faster when actors enter late into the graph !\nSee the preliminary evidence based on degree growth\n\nfig, axs = plt.subplots(figsize=(6*3, 2*3), ncols=6, nrows=2, sharex=True, sharey=True, constrained_layout=True)\n\ndf_pref_attachment_delta['year_bin'] = pd.cut(df_pref_attachment_delta.year, range(1900, 2020+1, 10))\n\nfor (year_bin, df_pref_attachment_delta_bin), ax in zip(df_pref_attachment_delta.groupby('year_bin'), axs.flat):\n    # ax.plot(df_pref_attachment_delta_bin.k, df_pref_attachment_delta_bin.dk, '.', markersize=2, alpha=0.2)\n    ax.hist2d(\n        np.log10(df_pref_attachment_delta_bin.k), np.log10(df_pref_attachment_delta_bin.dk),\n        bins=[30,30], norm='log', vmin=1, vmax=100, cmap='viridis'\n    )\n    # NOTE : we don't use year_bin.right because then we get \"From 2011 to 2020\" \n    ax.set_title(f'From {year_bin.left+1} to {df_pref_attachment_delta_bin.year.max()}', fontsize='medium')\n    # ax.set_xscale('log')\n    # ax.set_yscale('log')\n    ax.set_xlabel('Log degree $\\\\log k$')\n    ax.set_ylabel('Log degree evolution $\\\\log \\\\Delta k$')\n    ax.label_outer()\n\nfig.suptitle('Preliminary evidence of preferential attachment through degree evolution')\n\nplt.show()\n\n\n\n\n\n\n\nSee also Jeong et al, 2003\nWe consider adding actors to the graph progressively, and not considering edges created between existing actors.\nEach actor appears for the first time in a movie, and using the first movie release date we can prune the edges that do not correspond to the first addition.\nWe want to estimate\n\\(\\Pi(k_i) = \\frac{k_i}{\\sum_j k_j} \\approx \\frac{\\text{number of new connections to node } i}{\\text{total number of new connections}}\\)\n\nTake the distribution of edges at t0\nAdd edges, and see where they connect\nThe probability Pi(k) = [histogrammed probability of new connections @ k] / [histogram probability of degree distribution @ k],\nthen normalize Pi(k)\nThe denominator normalizes the fact high degrees are more rare, but nodes proportionally connect more to them\n\nNote : we do not consider internal links\n\n\nIn order to have a good estimate, we batch edges together by movie release year, so that the probability estimate is not too noisy\ncut : binning every 10 years qcut : binning 12 quartiles\n\nif False:  # recompute\n    for binning_method in ['year_bin', 'year_qbin']:\n        g = ig.Graph.DataFrame(df_edges.query('movie_release_year <= 1920')[['actor1_fbid', 'actor2_fbid']], directed=False, use_vids=False)\n        pa_internal = []\n        pa_external = []\n        degree_distribution = {}\n\n        for bin, df_edges_bin in tqdm(df_edges.query('1920 < movie_release_year').groupby(binning_method, sort=False)):\n            degrees = g.degree()\n            degree_lookup = dict(zip([ v['name'] for v in g.vs ], degrees))\n            degree_distribution[bin] = pd.Series(degrees).value_counts(normalize=True).sort_index()\n            new_actors = set()\n\n            for _, edge in df_edges_bin.iterrows():\n                actor1_degree = degree_lookup.get(edge.actor1_fbid, 0)\n                actor2_degree = degree_lookup.get(edge.actor2_fbid, 0)\n                \n                if actor1_degree == 0:\n                    new_actors.add(edge.actor1_fbid)\n                if actor2_degree == 0:\n                    new_actors.add(edge.actor2_fbid)\n\n                # both added at the same time. ignore\n                if actor1_degree == 0 and actor2_degree == 0:\n                    continue\n\n                # external edge\n                if actor1_degree == 0 and actor2_degree > 0:\n                    pa_external.append((actor2_degree, bin))\n                if actor1_degree > 0 and actor2_degree == 0:\n                    pa_external.append((actor1_degree, bin))\n\n                # internal edge\n                if actor1_degree > 0 and actor2_degree > 0:\n                    pa_internal.append((actor1_degree*actor2_degree, bin))\n\n            g.add_vertices(list(new_actors))\n            g.add_edges(zip(df_edges_bin.actor1_fbid, df_edges_bin.actor2_fbid))\n\n        df_pa_external = pd.DataFrame(pa_external, columns=['k', 'bin'])\n        df_pa_internal = pd.DataFrame(pa_internal, columns=['k1k2', 'bin'])\n        df_degree_distribution = (pd.DataFrame(degree_distribution)\n            .melt(var_name='bin', value_name='p_k', ignore_index=False)\n            .reset_index(names='k')\n            .dropna()\n        )\n\n        if True:  # save to disk\n            df_pa_external.to_pickle(f'../data/generated/graph/network_growth/pa_external_{binning_method}.pkl')\n            df_pa_internal.to_pickle(f'../data/generated/graph/network_growth/pa_internal_{binning_method}.pkl')\n            df_degree_distribution.to_pickle(f'../data/generated/graph/network_growth/degree_distribution_{binning_method}.pkl')\n\n\ndf_pa_external_year_bin = pd.read_pickle('../data/generated/graph/network_growth/pa_external_year_bin.pkl')\ndf_pa_internal_year_bin = pd.read_pickle('../data/generated/graph/network_growth/pa_internal_year_bin.pkl')\ndf_degree_distribution_year_bin = pd.read_pickle('../data/generated/graph/network_growth/degree_distribution_year_bin.pkl')\ndf_pa_external_year_qbin = pd.read_pickle('../data/generated/graph/network_growth/pa_external_year_qbin.pkl')\ndf_pa_internal_year_qbin = pd.read_pickle('../data/generated/graph/network_growth/pa_internal_year_qbin.pkl')\ndf_degree_distribution_year_qbin = pd.read_pickle('../data/generated/graph/network_growth/degree_distribution_year_qbin.pkl')\n\n\n# assert the distributions are correctly normalized\nassert (df_degree_distribution_year_bin.groupby('bin').p_k.sum() > 0.999).all()\nassert (df_degree_distribution_year_qbin.groupby('bin').p_k.sum() > 0.999).all()\n\n\n# inspect dataframe\ndf_degree_distribution_year_bin\n\n\n\n\n\n  \n    \n      \n      k\n      bin\n      p_k\n    \n  \n  \n    \n      0\n      1\n      (1920, 1930]\n      0.060832\n    \n    \n      1\n      2\n      (1920, 1930]\n      0.045526\n    \n    \n      2\n      3\n      (1920, 1930]\n      0.055338\n    \n    \n      3\n      4\n      (1920, 1930]\n      0.069859\n    \n    \n      4\n      5\n      (1920, 1930]\n      0.062402\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      6148\n      1117\n      (2000, 2010]\n      0.000012\n    \n    \n      6150\n      1143\n      (2000, 2010]\n      0.000012\n    \n    \n      6151\n      1233\n      (2000, 2010]\n      0.000012\n    \n    \n      6154\n      1339\n      (2000, 2010]\n      0.000012\n    \n    \n      6155\n      1387\n      (2000, 2010]\n      0.000012\n    \n  \n\n3303 rows × 3 columns\n\n\n\n\nfor df_degree_distribution in [df_degree_distribution_year_bin, df_degree_distribution_year_qbin]:\n    fig, axs = plt.subplots(figsize=(4, df_degree_distribution.bin.nunique()*1.5),\n        nrows=df_degree_distribution.bin.nunique(), sharex=True, sharey=True, constrained_layout=True)\n    for (bin, df_degree_distribution_bin), ax in zip(df_degree_distribution.groupby('bin'), axs.flat):\n        ax.plot(df_degree_distribution_bin.k, df_degree_distribution_bin.p_k, '.', markersize=2)\n        ax.set_xscale('log')\n        ax.set_yscale('log')\n        ax.label_outer()\n        ax.set_title(f'Degree distribution used for the bin {bin}', fontsize='small')\n    plt.show() \n\n\n\n\n\n\n\n\ndef compute_pi_kappa(df_pa_external, df_degree_distribution):\n    pi_external = (\n        df_pa_external.groupby('bin')\n        .k.value_counts(normalize=True)  # histogram of edge connections as a function of k\n        .sort_index()\n        .rename('pi')  # distribution of edge connections\n    )\n    for bin in df_pa_external.bin.unique():\n        # we need to divide by the distribution of edges,\n        # because otherwise the histogram is biased toward the more frequent degrees\n        pi_external[bin] /= (\n            df_degree_distribution.query('bin == @bin')  # the distribution of degrees right before the bin of edges was added\n            .set_index('k').p_k\n        )\n        # normalize\n        pi_external[bin] /= pi_external[bin].sum()\n\n    # compute cumulative sum\n    kappa_external = pi_external.groupby('bin').cumsum().rename('kappa')\n\n    pi_external = pi_external.reset_index()\n    kappa_external = kappa_external.reset_index()\n\n    return pi_external, kappa_external\n\n\nfor df_pa_external, df_degree_distribution in [(df_pa_external_year_bin, df_degree_distribution_year_bin), (df_pa_external_year_qbin, df_degree_distribution_year_qbin)]:\n    pi_external, kappa_external = compute_pi_kappa(df_pa_external, df_degree_distribution)\n\n    fig, axs = plt.subplots(nrows=2, figsize=(8, 6), constrained_layout=True)\n\n    sns.scatterplot(pi_external, x='k', y='pi', hue='bin', linewidth=0, alpha=0.8, s=10, palette='viridis', ax=axs[0])\n    sns.move_legend(axs[0], loc='upper left', bbox_to_anchor=(1, 1))\n    axs[0].set_ylabel('Preferential attachment $\\\\Pi(k)$')\n    axs[0].set_xlabel('Degree $k$')\n    axs[0].set_yscale('log')\n    axs[0].set_xscale('log')\n    axs[0].set_title('Preferential attachment by year bin, and on entire dataset')\n\n    sns.lineplot(kappa_external, x='k', y='kappa', hue='bin', palette='viridis', legend=False, ax=axs[1])\n    # sns.move_legend(axs[1], loc='lower right', fontsize='small')\n    axs[1].set_ylabel('Cumulative function $\\\\kappa(k)$')\n    axs[1].set_xlabel('Degree $k$')\n    axs[1].set_yscale('log')\n    axs[1].set_xscale('log')\n    axs[1].set_title('Cumulative function by year bin')\n\n    plt.show()\n\n\n\n\n\n\n\n\ndef compute_fitcoefs(df_kappa):\n    values = []\n\n    for bin, df_kappa_bin in df_kappa.groupby('bin'):\n        if len(df_kappa_bin) >= 2:\n            model = smf.ols(formula=\"log_kappa ~ log_k\", data=dict(log_k=np.log10(df_kappa_bin.k), log_kappa=np.log10(df_kappa_bin.kappa)))\n            res = model.fit()\n            # display(res.summary())\n            alpha_plus_one = (res.params['log_k'], *res.conf_int().loc['log_k'])\n            values.append((bin, alpha_plus_one[0]-1, alpha_plus_one[1]-1, alpha_plus_one[2]-1))\n        else:\n            values.append((bin, np.nan, np.nan, np.nan))\n\n    return pd.DataFrame(values, columns=['bin', 'alpha', 'ci_lower', 'ci_upper'])\n\n\nfitcoefs = {\n    'year_bin': compute_fitcoefs(compute_pi_kappa(df_pa_external_year_bin, df_degree_distribution_year_bin)[1]),\n    'year_qbin': compute_fitcoefs(compute_pi_kappa(df_pa_external_year_qbin, df_degree_distribution_year_qbin)[1])\n}\n\n\nfitcoefs\n\n{'year_bin':             bin     alpha  ci_lower  ci_upper\n 0  (1920, 1930]  0.276953  0.210638  0.343267\n 1  (1930, 1940]  0.210021  0.169118  0.250924\n 2  (1940, 1950]  0.510332  0.476495  0.544169\n 3  (1950, 1960]  0.336974  0.318484  0.355464\n 4  (1960, 1970]  0.386186  0.366752  0.405619\n 5  (1970, 1980]  0.433514  0.417537  0.449490\n 6  (1980, 1990]  0.419824  0.405383  0.434265\n 7  (1990, 2000]  0.522066  0.502441  0.541692\n 8  (2000, 2010]  0.639683  0.624278  0.655087,\n 'year_qbin':                 bin     alpha  ci_lower  ci_upper\n 0  (1920.0, 1952.0]  0.099836  0.045346  0.154326\n 1  (1952.0, 1970.0]  0.333914  0.311530  0.356299\n 2  (1970.0, 1982.0]  0.442242  0.426329  0.458155\n 3  (1982.0, 1991.0]  0.431424  0.417991  0.444858\n 4  (1991.0, 1998.0]  0.547346  0.527997  0.566695\n 5  (1998.0, 2002.0]  0.614853  0.597720  0.631986\n 6  (2002.0, 2005.0]  0.626947  0.611793  0.642101\n 7  (2005.0, 2007.0]  0.670241  0.655415  0.685068\n 8  (2007.0, 2010.0]  0.669631  0.658343  0.680920\n 9  (2010.0, 2016.0]  0.801972  0.789656  0.814289}\n\n\n\nfig, ax = plt.subplots(figsize=(6, 3), constrained_layout=True)\n\nmid_years = fitcoefs['year_bin'].bin.map(lambda x: (x.left + x.right)/2)\nqmid_years = fitcoefs['year_qbin'].bin.map(lambda x: (x.left + x.right)/2)\n\nalpha_mean, alpha_std = fitcoefs['year_qbin'].alpha.mean(), fitcoefs['year_qbin'].alpha.std()\nax.axhline(alpha_mean, alpha=0.6, color='tab:gray')\nax.annotate(f'qcut mean : $\\\\alpha \\\\approx {alpha_mean:.2f} \\pm {alpha_std:.2f}$',\n    xy=(0.02, alpha_mean+0.02), va='bottom', xycoords=('axes fraction', 'data'), color='tab:gray')\nax.fill_between(mid_years, fitcoefs['year_bin'].ci_lower, fitcoefs['year_bin'].ci_upper, alpha=0.2)\nax.fill_between(qmid_years, fitcoefs['year_qbin'].ci_lower, fitcoefs['year_qbin'].ci_upper, alpha=0.2)\nax.plot(mid_years, fitcoefs['year_bin'].alpha, 'o--', label='cut')\nax.plot(qmid_years, fitcoefs['year_qbin'].alpha, 'o--', label='qcut')\n\nax.set_xticks([ int(x.left) for x in fitcoefs['year_bin'].bin ] + [ int(fitcoefs['year_bin'].bin.iloc[-1].right) ])\nax.set_xticklabels(ax.get_xticks(), rotation=45)\nax.legend(loc='lower right')\n\nax.set_ylim((0.0, 1.1))\nax.set_title('Evolution of preferential attachment exponent, $\\\\Pi(k) \\\\sim k^{\\\\alpha}$')\nplt.show()\n\n\n\n\nThis also confirms actors do indeed grow faster as they enter later into the graph !\n\n\n\n\n\n\ndef p_k_th(alpha: float, k_mean: float):\n    def p_k(k: np.ndarray) -> np.ndarray:\n        p = k**(-alpha) * np.exp(-2/(k_mean*(1-alpha)) * k**(1-alpha))\n        p /= p.sum()\n        return p\n    return p_k\n\n\nfig, ax = plt.subplots()\n\ndegrees_final = degrees[:, -1].copy()\ndegrees_final.sort()\npdf = pd.Series(degrees_final).value_counts(normalize=True).sort_index()\nax.plot(pdf.index, pdf.values, linewidth=1, label='True PDF')\n# sns.histplot(degrees_final, log_scale=(True, True), stat='probability', label='true distribution')\n\n# year_to_idx = { year: i for i, year in enumerate(years) }\n# k_means = [ np.nanmean(degrees[:, year_to_idx[int(qmid_year)]]) for qmid_year in qmid_years ]\nk = np.logspace(0, 3, 100)\nsns.lineplot(x=k, y=p_k_th(fitcoefs['year_qbin'].alpha.mean(), degrees_final.mean())(k),\n    label='mean fit based\\non the B-A model')\n\nax.set_xscale('log')\nax.set_yscale('log')\n\nax.legend()\nplt.show()\n\n\n\n\nProblem : the cutoff is too sharp with our fits. it looks like the true alpha is larger\n-> We have not taken into account internal links\n\n\n\n\n\n\nkmax = np.nanmax(degrees, axis=0)\n\n\nfig, ax = plt.subplots()\nax.plot(years, kmax)\nax.twinx().plot(years, np.log(years-years.min()+1)**(1/(1-0.52)), color='tab:orange')\nax.set_xlabel('year')\nax.set_ylabel('$k_{max}$')\nax.set_title('Evolution of the maximum actor network degree')\nplt.show()\n\n\n\n\n\n\n\n\nif False:\n    g = ig.Graph(directed=False)\n    total_movies = 0\n    components = []\n\n    for year, df_edges_year in tqdm(df_edges.groupby('movie_release_year', sort=False)):\n        current_actors = set([ v['name'] for v in g.vs ])\n        actors_from_edges = set(df_edges_year.actor1_fbid.unique()) | set(df_edges_year.actor2_fbid.unique())\n        new_actors = actors_from_edges - current_actors\n\n        g.add_vertices(list(new_actors))\n        g.add_edges(zip(df_edges_year.actor1_fbid, df_edges_year.actor2_fbid))\n\n        total_movies += df_edges_year.movie_fbid.nunique()\n        components.append((len(g.connected_components(mode='weak')), total_movies, year))\n\n    df_components = pd.DataFrame(components, columns=['num_connected', 'num_movies', 'year'])\n\n    if True:  # save to disk\n        df_components.to_pickle('../data/generated/graph/network_growth/components.pkl')\n\nelse:\n    df_components = pd.read_pickle('../data/generated/graph/network_growth/components.pkl')\n\n100%|██████████| 122/122 [00:07<00:00, 16.77it/s]\n\n\n\n# at the start, every movie adds a new unique component, then the clusters start to appear !\nfig, ax = plt.subplots()\nax.plot(df_components.year, df_components.num_connected, label='number of connected components')\nax.plot(df_components.year, df_components.num_movies, label='number of movies')\nax.set_yscale('log')\nax.legend()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The world of actors",
    "section": "",
    "text": "Our great project resides here :)"
  }
]