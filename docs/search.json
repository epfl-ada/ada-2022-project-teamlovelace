[
  {
    "objectID": "full_network.html",
    "href": "full_network.html",
    "title": "The world of actors",
    "section": "",
    "text": "The actor network is modeled by a graph, where each actor represents as node \\(i \\in \\{1 \\cdots N\\}\\). Edges \\((i,j) \\in E\\) are placed between two actors if they have appeared in the same movie. We additionally weigh each edge by the number of movies the two actors have in common.\nBefore we dive head first into computing the graph edges, we need to estimate how much space it will take. An upper bound for the number of edges is \\(\\mathcal{O}(\\text{number of movies} \\times \\text{number of actors per movie}^2)\\), since each movie links two actors together.\nWe therefore compute the number of actors per movie, and report the histogram in Figure 1.\n\n\n\n\n\n\n  \n    \n      \n      numactors_per_movie\n      movie_name\n    \n  \n  \n    \n      0\n      115\n      Hemingway & Gellhorn\n    \n    \n      1\n      87\n      Taking Woodstock\n    \n    \n      2\n      81\n      Captain Corelli's Mandolin\n    \n    \n      3\n      81\n      Terror in the Aisles\n    \n    \n      4\n      72\n      Walk Hard: The Dewey Cox Story\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      64324\n      1\n      Primo\n    \n    \n      64325\n      1\n      The Stranger Who Looks Like Me\n    \n    \n      64326\n      1\n      On the Wing\n    \n    \n      64327\n      1\n      Robot Bastard!\n    \n    \n      64328\n      1\n      Incident\n    \n  \n\n64329 rows × 2 columns\n\n\n\n\n\n\n\n\nFigure 1: Distribution of actors per movie\n\n\n\n\nWith a total of 64329 movies, the distribution seems to follow an exponential law (with a somewhat long tail due to outliers). We pick the median at 6.0 to represent the distribution. We can now compute the upper bound :\n\n\nnum_edges ~ 2.3e+06\n\n\nWith order \\(10^6\\) edges we will not be doing any full graph visualization, however we can do some analysis on the large graph.\nAfter explicit computation, we get the following graph :\n\n\nGraph with 135061 nodes and 2080273 edges\n\n\nThus we end up with around 200k edges. (and around 224k edges if we keep edges between the two same actors distinct).\nThe graph contains around 135k actors, spanning movies released between 1888 and 2016, as shown in Figure 2. We note that after approximatively 2010, the number of movies in the dataset drops fast, even though the original paper with the data was published in August 2013.\n\n\n\n\n\nFigure 2: Number of movies released each year\n\n\n\n\n\n\n\nAmong other real-life networks, social networks typically exhibit power law distribution of degrees. These types of networks are said to be “scale-free”, due to the scale-invariance of the power law. We compute the distribution for the network at hand.\n\n\n\n\n\nFigure 3: Degree distribution of the whole network\n\n\n\n\nThe degree distribution shown in Figure 3 clearly follows a power law \\(p(k) \\sim k^{-\\gamma}\\), and we recover the exponent \\(\\gamma = 1+\\mu \\approx 2.15 \\pm 0.07\\).\nAccording to the Molly-Reed condition (Molloy and Reed 1995), the entire network is fully connected to a giant component if \\(\\langle k^2 \\rangle < 2 \\langle k \\rangle\\). In our case with a power law of \\(\\mu \\approx 1.15\\), the left hand side integral diverges, and so we expect to find a giant cluster. Although since \\(\\langle k \\rangle < \\infty\\), we expect the giant component has not yet absorbed all clusters (“percolation”), and some smaller localized clusters remain (Bouchaud J. P. 2015).\nTo put this result into perspective, the Internet has in-degree distribution with \\(\\mu \\approx 1\\), scientific papers have \\(\\mu \\approx 2\\), human sexual contacts have \\(\\mu \\approx 2.4\\) (Bouchaud J. P. 2015). We are therefore in the presence of a network in which large clusters play an important role.\n\n\n\nWe compute the connected components of the graph (clusters), as well as their diameter. As the true diameter computation scales as \\(\\mathcal O(\\text{number of nodes})^3\\), we resort to using an approximative algorithm. Data about the 1272 clusters is plotted on Figure 4.\n\n\n\n\n\nFigure 4: Distribution of cluster sizes and diameter\n\n\n\n\nAs expected, we indeed have a giant cluster containing \\(\\sim 10^5\\) actors, accounting for about 94% of the total number of actors in the graph.\nWe also remark that even though the cluster size spans orders of magnitude, the cluster diameter spans between 1 and 14. The smaller clusters contain up to \\(10^2\\) actors have diameter at most 3, and their size seem to roughly follow a power law with exponential cutoff (Bouchaud J. P. 2015). Albeit its exponentially large size, the giant component has a diameter of 14, which is in-line with the small-world characteristic of scale-free networks : the diameter of the network scales only logarithmically with its size, i.e. \\(d \\sim \\log N\\) (Barabasi, Albert, and Jeong 2000)."
  },
  {
    "objectID": "belgian.html",
    "href": "belgian.html",
    "title": "The world of actors",
    "section": "",
    "text": "The belgian subnetwork is formed by slicing the original graph, keeping only nodes corresponding to belgian actors, and the edges between them.\nWe end up with the following graph :\n\n\nGraph with 191 nodes and 558 edges\n\n\nHowever, looking into the graph, we see that it is composed of a large central cluster and a few smaller graphs of at most 5 nodes. These are the result of slicing the original graph, where belgian actors that incidentally played together in an international movie still appear, despite being isolated.\nWe list out the connected components, and keep only the largest one.\n\n\nGraph with 156 nodes and 526 edges\nGraph with 3 nodes and 2 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 3 edges\nGraph with 5 nodes and 10 edges\nGraph with 3 nodes and 3 edges\nGraph with 3 nodes and 2 edges\nGraph with 3 nodes and 2 edges\nGraph with 3 nodes and 3 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 2 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 2 edges\n\n\nBefore we jump into the graph analysis and visualization, we make a small detour to examine the data. In particular, we would like to detect some cliques by naively looking at the number of movies actors have in common. This is done in Figure 1, which shows that most actors in the belgian graph feature together in only one movie.\n\n\n\n\n\nFigure 1: Number of shared movies between belgian actors\n\n\n\n\nWe list out the actors that feature together in more than 3 movies :\n\n\n\n\n\n\n  \n    \n      \n      actor1_name\n      actor2_name\n      movie_count\n    \n  \n  \n    \n      73\n      Patrick Descamps\n      Lucas Belvaux\n      3\n    \n    \n      347\n      Bouli Lanners\n      Benoît Poelvoorde\n      3\n    \n    \n      422\n      Olivier Gourmet\n      Jérémie Renier\n      4\n    \n    \n      508\n      Serge Larivière\n      Yolande Moreau\n      3\n    \n  \n\n\n\n\nFrom this data, it is not immediately clear whether there is a clique formed by or containing these actors. Time to visualize the graph !\n\n\n\nWe visualize the actor graph in Figure 2, coloring edges based on the 72 different movies, and making edges proportionally larger to the number of movies two actors have in common.\n\n\n\n\n\nFigure 2: First visualization of the belgian actor graph\n\n\n\n\nFrom this visualization we can see the emergence of small communities corresponding to one movie played in common. In the lower left, we can see Barbara Sarafian bridging the gap between two small communities.\nSome individuals, such as Jan Decleir or Jérémie Renier stand out by the sheer number of edges connected to them.\nJust left of Jérémie Renier, we can also visually inspect and confirm the small clique composed by Yolande Moreau, Serge Larivière, Benoît Poelvoorde, and Bouli Lanners.\nJean-Claude Van Damme, a well-known actor, appears marginal here. This again is an effect of slicing the original graph, where Jean-Claude Van Damme was well-connected, but appeared in more international movies, with non-belgian actors.\n\n\n\nInstead of relying on visualization to evaluate centrality, we can also compute diffent metrics, such as degree, eigenvector and betweenness centrality.\nFor each centrality metric, we print out the top-5 actors.\n\n\n\n\n\n\n  \n    \n      \n      degree_centrality\n      actor_name\n    \n  \n  \n    \n      106\n      0.264516\n      Jan Decleir\n    \n    \n      107\n      0.129032\n      Jérémie Renier\n    \n    \n      45\n      0.122581\n      François Beukelaers\n    \n    \n      149\n      0.116129\n      Filip Peeters\n    \n    \n      100\n      0.116129\n      Matthias Schoenaerts\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      eigenvector_centrality\n      actor_name\n    \n  \n  \n    \n      106\n      0.408367\n      Jan Decleir\n    \n    \n      10\n      0.209502\n      Werner De Smedt\n    \n    \n      56\n      0.205922\n      Bert Haelvoet\n    \n    \n      149\n      0.204257\n      Filip Peeters\n    \n    \n      58\n      0.193717\n      Gert Portael\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      betweenness_centrality\n      actor_name\n    \n  \n  \n    \n      106\n      0.395843\n      Jan Decleir\n    \n    \n      100\n      0.254062\n      Matthias Schoenaerts\n    \n    \n      45\n      0.116199\n      François Beukelaers\n    \n    \n      107\n      0.110377\n      Jérémie Renier\n    \n    \n      69\n      0.108920\n      Bouli Lanners\n    \n  \n\n\n\n\nJan Decleir comes out on top each time. Matthias Schoenaerts, François Beukelaers as well as Filip Peeters appear two times. The different methods result in quite different results. We try to explain this by plotting the distribution of the centrality measures in Figure 3, as well a visualizing in Figure 4.\n\n\n\n\n\nFigure 3: Distribution of different centrality measures\n\n\n\n\n\n\n\n\n\nFigure 4: Visual comparison of different centrality measures\n\n\n\n\nThis yields the following observations.\nDegree centrality\nDegree centrality only yields local information, since it is just proportional to the local node degree. Therefore, it might not be as insightful as compared to measures which take into account the entire graph\nVisually, we see that Jan Decleir appears as a the only very large node. This points to the problem that degree centrality only looks “one hop away”, where we would intuitively think that neighbours of Jan Decleir would have larger centrality, just because they are directly connected to a hub.\nEigenvector centrality\nMathematically, this adresses the issue of “one-hop” degree centrality, as this centrality is derived from a self-consistent equation taking into account the whole graph. Nodes connected to the high eigenvector centrality nodes are more likely to have a high eigenvector centrality. Eigenvector centrality produces a powerlaw-like distribution : only a few actors are “central”, with many actors having a small centrality.\nBetweenness centrality\nMany nodes have zero betweenness centrality : these correspond to actors on the “edge of the graph”, and therefore do not contribute to the general connectedness of the graph. And indeed, this is confirmed visually, as nodes far away from the core appear very small (and should not appear at all, if it weren’t for the minimal node size set for the visualization). Barbara Sarafian, despite living on the edge of the graph, has a nonzero betweenness centrality, as this actor connects two small communities.\n\n\n\nAnother important aspect of social networks is the formation of clusters. Again, there exist algorithms to try to find these clusters. We study two such algorithms, the Clauset-Newman-Moore and Louvain algorithms. Their comparison is interesting, in that their approach are opposite.\nThe Louvain algorithm tries to join clusters together to increase modularity, which measures how internally well-connected the cluster is compared to the outside.\nThe Clauset-Newman-Moore algorithm splits the graph, removing edges with high betweenness. The reasoning behind this is that communities are separated by a few edges, through which the majority of the “traffic” between the two communities flow.\n\n\n\n\n\nFigure 5: Comparison of the two clustering algorithms. Nodes are colored according to community, and size according to eigenvalue centrality. The minimum vertex size is set to be large, such that colours are easily visible.\n\n\n\n\nFigure 5 visualizes the different clusterings. Communities often group actors which seem to have similar eigenvalue centrality, and have played in the same movies. For instance, the green community is well connected by majoritarily two movies, colored cyan and purple.\nIn the Louvain clustering, three communities (blue, gray, brown in the right of the graph) stand out as corresponding to the “popular” belgian actors, all having large eigenvalue centrality. The five other communities (green, red, pink, orange, purple) are that of smaller actors. These might correspond to more “niche” communities. We note that the niche communities rely on highly connected actors to connect to the rest of the graph. Notably in this case, Matthieu Schoenaerts and Johan Heldenbergh provide a bridge to and from the pink community.\nFor this particular graph, the Louvain clustering seems more reasonable. The blue community from the Clauset-Newman-Moore clustering seems to be way too large, especially when a very small gray community in the upper-left is composed of only 5 nodes. Louvain clustering on the other hand, seems to produce more evenly distributed communities.\nOne explanation for this could be that the Louvain algorithm works on node degree, instead of the edges. In this case, the node degree should be taken into account, as we would expect to group “popular” and “niche” actors in separate communities. The Clauset-Newman-Moore places importance on edge via the betweenness measure, however we argue that the edges generated by each movie should be equally important, rendering this algorithm less effective for our problem."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "growth.html",
    "href": "growth.html",
    "title": "The world of actors",
    "section": "",
    "text": "In this section we try to understand how the network grows, more precisely how edges are formed and where new actors connect to. For this, we timestamp each edge of the actor graph with the corresponding movie release date. In this operation, about 0.75% of the original edges are dropped.\n\n\nIn the Barabási-Albert model (Barabási and Pósfai 2016), the preferential attachment factor \\(\\Pi(k_i)\\) describes the probability with which a new node connects to an already existing node of degree \\(k_i\\). Therefore, the rate at which node \\(i\\)’s degree grows is proportional to \\(\\Pi(k_i)\\).\n\\[\n\\frac{\\mathrm d k_i}{\\mathrm d t} \\propto \\Pi(k_i)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIn reality the proportionality constant depends on \\(t\\), and in the large \\(t\\) limit we get (Barabási and Pósfai 2016)\n\\[\n\\frac{\\mathrm d k_i}{k_i} = \\frac 12 \\frac{\\mathrm dt}{t}\n\\]\nIntegrating and letting \\(k_i^0\\) be the degree with which node \\(i\\) first joined the network at time \\(t_i^0\\), we get :\n\\[\nk_i(t) = k_i^0 \\left( \\frac{t}{t_i^0} \\right)^\\beta\n\\]\nWhere the dynamical exponent \\(\\beta = 1/2\\) describes the degree growth speed.\nIn the subsequent analysis of degree evolution, we neglect these subtleties in order to give a first impression. We later do a more formal analysis based on (Jeong, Néda, and Barabási 2003).\n\n\nIn a naive numerical approach, we inspect whether there could be preferential attachment by computing the year-to-year degree evolution of each node in the network. Figure 1 gives us a first impression of the results, where we see the actors (individual lines) grow through time, until their growth becomes stagnant due to their end of career. We notice that actors who enter late in the graph tend to grow faster !\n\n\n\n\n\nFigure 1: Actor degree as a function of time. Each line models one actor. This figure shows only a random sample of 256 actors to give a general impression and to avoid cluttering the plot.\n\n\n\n\nIn Figure 2, we plot a 2D histogram of the degree evolution \\(\\Delta k_i\\) versus the original degree \\(k_i\\). Darker colours correspond to a higher density of samples. If there were no preferential attachement, that is \\(\\Pi = \\textrm{constant}\\), we would expect that \\(\\Delta k_i\\) be intependant of \\(k_i\\). However, the general trend of this figure shows that the higher the original degree \\(k_i(t)\\), the more the degree grows to \\(k_i(t+1) = k_i(t) + \\Delta k_i(t)\\), which shows that this network exhibits preferential attachment. And in accordance with Figure 1, we see that this preferential attachment is stronger when actors join the graph later.\n\n\n\n\n\nFigure 2: Preliminary evidence of preferential attachment through degree evolution. This figure shows degree evolution \\(\\Delta k_i\\) as a function of degree \\(k_i\\), binned every decade starting 1911.\n\n\n\n\n\n\n\n\n\nIn order to formalize the results of the previous section, we compute the preferential attachment factor based on the methods explained in (Jeong, Néda, and Barabási 2003).\nFor the following, we consider adding edges to the graph progressively. If the edge adds a new actor to the network (“external edge”), we record the degree of the actors it attaches to. Note that we therefore do not report the degrees between two existing actors (“internal edges”), but we still add them progressively.\nWe want to estimate :\n\\[\\Pi(k_i) = \\frac{k_i}{\\sum_j k_j} \\approx \\frac{\\text{number of new connections to node } i}{\\text{total number of new connections}}\\]\nNumerically, we perform this by histogramming. Therefore, the algorithm is as follows :\n\nSince we need an already existing population in order to estimate preferential attachment, we consider the initial graph at time \\(t_0 = 1920\\).\nBin edges together based on their date.\nFor each bin \\((t_-, t_+)\\), repeat :\n\nRecord the degree histogram \\(p(k)\\) at the start \\(t_-\\) of each bin.\nAdd the edges in the bin, and record the degree histogram \\(q(k)\\) of the actors that new actors connect to.\nThe probability distribution \\(\\Pi(k)\\) is obtained by dividing \\(q(k)\\) by \\(p(k)\\), and renormalizing. The denominator accounts for the fact that high degrees are more rare. If we didn’t perform this division, then we would observe little preferential attachment, as new actors connect to many low degree actors just due to the sheer number of them.\n\n\nWe perform binning using two methods, and plot the bin sizes in Figure 3. Binning edges together every decade gives us more control over the date at which we perform the estimation, but yields very uneven bin sizes. Binning edges together every 9th quantile (but rounding at integer years) yields more even bin sizes, but the dates are non-uniformly distributed.\n\n\n\n\n\nFigure 3: Sample sizes for the two edge binning methods\n\n\n\n\n\n\n\nThe Barabási-Albert model can be extended to other regimes of preferential attachment. (Jeong, Néda, and Barabási 2003) defines the scaling exponent \\(\\alpha\\) as :\n\\[\n\\Pi(k) \\sim k^\\alpha\n\\]\nIf \\(\\alpha = 1\\), we recover the original Barabási-Albert model (linear preferential attachment). Regimes \\(\\alpha < 1\\) are said to be sub-linear, and \\(\\alpha > 1\\) is super-linear. Larger values of \\(\\alpha\\) therefore correspond to a stronger preferential attachment, and one can intuitively expect the appearance of giant hubs.\nPerforming the computations, we scatter the results for \\(\\Pi(k)\\) in Figure 4. While it is clear that preferential attachment follows a power law, there is a lot of noise in the data. To address this, the paper suggests to use the cumulative function :\n\\[\n\\kappa(k) = \\int^k \\Pi(k') \\; \\mathrm d k' \\sim k^{\\alpha+1}\n\\]\n\n\n\n\n\n\n\n(a) Binning every decade\n\n\n\n\n\n\n\n(b) Binning every 9th quantile\n\n\n\n\nFigure 4: Preferential attachment \\(\\Pi(k)\\) and cumulative function \\(\\kappa(k)\\) for the two binning methods.\n\n\nPerforming the fit, we report the obtained scaling exponents and their 95% confidence interval in Figure 5. Using the quantile bins, we can also compute an “average exponent” to be around \\(\\alpha \\approx 0.5\\), putting this network in the sublinear regime. Both binning methods show that the exponent tends to increase with time, confirming the observation that actors grow faster as they enter later into the graph.\n\n\n\n\n\nFigure 5: Evolution of scaling exponent \\(\\alpha\\).\n\n\n\n\n\n\n\nWe now ask whether this model is able to predict the observed degree distribution. In the sub-linear regime, the probability distribution follows a stretched exponential distribution :\n\\[\np(k) \\sim k^{-\\alpha} \\exp \\left(\\frac{-2 k^{1-\\alpha}}{\\langle k \\rangle (1-\\alpha)} \\right)\n\\]\nFigure 6 compares the predicted and true distributions. Unfortunately, the match is poor : what happened ? When we computed the preferential attachment, recall we ignored internal edges, however they make up about 32% of the total edges, and are therefore not neglectible.\nTherefore, we predict that the true exponent must actually be greater than one, putting the model in the super-linear regime. This is further supported by Figure 7, in which we see that the largest degree grows roughly linearly, \\(k_{\\text{max}} \\sim t\\), as one would find on the super-linear regime (Barabási and Pósfai 2016).\n\n\n\n\n\nFigure 6: Comparing the true degree distribution to the one predicted by the Barabási-Albert model.\n\n\n\n\n\n\n\n\n\nFigure 7: Evolution of the maximum actor network degree"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The world of actors",
    "section": "",
    "text": "Abstract\nThe well-known concept of six degrees of separation descibes that one can link anyone to any other person by a “friend of a friend” chain of length six. Our project stems from this idea : we aim to analyze the graph of movie actors formed by linking actors having played in the same movie, or by various similarity metrics. This field is called SNA, standing for Social Network Analysis (Tabassum et al. 2018) and has been quite active in the last decade. With this in mind, we want to look at how our social graph can relate to real world’s social interactions. Our main approach consists in investigating the connectedness of actors in different subsets of our graph, according to different features such as countries and release year. We can ask ourselves how similarities between actors reflect themselves in the connectedness of actors. Can the time evolution of the social graph reflect geopolitical events, and reveal information about the influence of star actors on the future career of newcomers ?\n\n\n\n\n\nReferences\n\nTabassum, Shazia, Fabiola Pereira, Sofia Fernandes, and João Gama. 2018. “Social Network Analysis: An Overview.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8 (April): e1256. https://doi.org/10.1002/widm.1256."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Source data for this project.\n\nhttp://www.cs.cmu.edu/~ark/personas"
  },
  {
    "objectID": "centrality_and_success.html",
    "href": "centrality_and_success.html",
    "title": "The world of actors",
    "section": "",
    "text": "We want to use wikipedia pageviews as a proxy for popularity, and try to find a correlation the centrality of actors in the actor graph. For this we used the dataset of pageviews from Homework 2. Since Wikipedia was founded at the start of the century, pageviews might not be relevant for older actors. To eliminate this possible bias we only considered only actors having played in recent movies and formed the actor-graph only based on the recent movies.\nWe therefore consider 49481 actors accross 41039 movies, 4631 of which (the actors) we have the pageview count.\n\n\n\n\n\n\n\nWe focused on three metrics of centrality: - Degree centrality: With how many other actors have the actors played - Eigenvector centrality - Betweenness: how much an actor bridges communities of actors\n\n\nFor those interested, we list here the most central actors.\n\n\n\n\n\n\n  \n    \n      \n      degree_centrality\n      actor_name\n      fb_actor_id\n    \n  \n  \n    \n      70\n      0.011601\n      Anupam Kher\n      /m/0292l3\n    \n    \n      744\n      0.010853\n      Jane Lynch\n      /m/07m77x\n    \n    \n      748\n      0.010287\n      Samuel L. Jackson\n      /m/0f5xn\n    \n    \n      710\n      0.009903\n      David Koechner\n      /m/059j1m\n    \n    \n      202\n      0.009863\n      Justin Long\n      /m/07cjqy\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      eigenvector_centrality\n      actor_name\n    \n  \n  \n    \n      1150\n      0.095907\n      David Strathairn\n    \n    \n      6434\n      0.095699\n      Nicole Kidman\n    \n    \n      4889\n      0.095603\n      Clive Owen\n    \n    \n      6102\n      0.094613\n      Parker Posey\n    \n    \n      4467\n      0.094151\n      Rodrigo Santoro\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      betweenness_centrality\n      actor_name\n    \n  \n  \n    \n      70\n      0.019429\n      Anupam Kher\n    \n    \n      5728\n      0.011046\n      Lee Byung-Hun\n    \n    \n      10\n      0.010051\n      Nassar\n    \n    \n      6198\n      0.009872\n      Peter Stormare\n    \n    \n      5257\n      0.009498\n      Vera Farmiga\n    \n  \n\n\n\n\n\n\n\n\n\n\nWe first try to visualise the two values together.\nThe correlations we found were very modest with R^2 values of 0.016, 0.013, and 0.007 respectively\n\n\n\n\n\nFor movie count.\n    We find an average increase of 50.742% in pageviews per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       4.7326      0.015    324.735      0.000       4.704       4.761\nmovie_count     0.1782      0.018      9.787      0.000       0.143       0.214\n===============================================================================\n\n\n\n\n\n\n\nFor degree centrality.\n    We find an average increase of 3.7x in centrality per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      -3.6732      0.007   -529.463      0.000      -3.687      -3.660\nmovie_count     0.5656      0.009     65.250      0.000       0.549       0.583\n===============================================================================\n\nFor eigenvector centrality.\n    We find an average increase of 19.8x in centrality per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      -4.8026      0.026   -187.939      0.000      -4.853      -4.753\nmovie_count     1.2970      0.032     40.620      0.000       1.234       1.360\n===============================================================================\n\nFor betweenness centrality.\n    We find an average increase of 10.7x in centrality per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      -5.3659      0.042   -126.906      0.000      -5.449      -5.283\nmovie_count     1.0284      0.040     25.452      0.000       0.949       1.108\n===============================================================================\n\n\n\n\n\n\n\n\n\n\n\n\nHere the values are better correlated, with R^2 values of 0.479, 0.263, and 0.212 respectively. Movie count having a significant correlation with pageviews as well as with centrality, it acts as a confounder. We therefore aim to isolate the effects of centrality from those of movie count by using A/B testing\n\n\n\nThe A/B test is done by making pairs of actors with similar numbers of movies (Here similar means that their number of movies are less than 5% different) such that the first actor has a lower centrality than the second.\n\n\nFor degree centrality.\n    We find an average increase of -2.43% in pageviews per 2x increase in centrality\n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept               0.0007      0.020      0.033      0.974      -0.038       0.040\nlogratio_centrality    -0.0355      0.008     -4.231      0.000      -0.052      -0.019\n=======================================================================================\n\nFor eigenvector centrality.\n    We find an average increase of 4.73% in pageviews per 2x increase in centrality\n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept              -0.0351      0.019     -1.884      0.060      -0.072       0.001\nlogratio_centrality     0.0667      0.003     23.301      0.000       0.061       0.072\n=======================================================================================\n\nFor betweenness centrality.\n    We find an average increase of 2.58% in pageviews per 2x increase in centrality\n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept               0.2812      0.026     10.966      0.000       0.231       0.331\nlogratio_centrality     0.0367      0.004      9.174      0.000       0.029       0.045\n=======================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\nIn the end we find very little increases in pageviews when varying the centrality of actors. There is even a slight decrease concerning the degree centrality, although it is not statistically significant. The largest increase is with the eigenvalue centrality, with an average increase of 4.73% in pageviews per 2x increase in centrality, but with an R^2 value of 0.007. We conclude that if there is a relationship between the centrality of actors and their popularity, it is barely noticeable and not interesting."
  },
  {
    "objectID": "viz.html",
    "href": "viz.html",
    "title": "Shorthest path vizualization",
    "section": "",
    "text": "Show the code\nfunction Graph() {\n  var neighbors = this.neighbors = {};\n  this.addEdge = function (u, v, val) {\n    if (neighbors[u] === undefined) {\n      neighbors[u] = [];\n    }\n    neighbors[u].push(({id: v, value: val}));\n    if (neighbors[v] === undefined) {\n      neighbors[v] = [];\n    }           \n    neighbors[v].push(({id: u, value: val}));\n  };\n\n  return this;\n}\n\nfunction shortestPath(graph, source, target) {\n  if (source == target) {\n    return source;\n  }\n  var queue = [ source ],\n      visited = { [source]: true },\n      predecessor = {},\n      tail = 0;\n  while (tail < queue.length) {\n    var u = queue[tail++],\n        neighbors = graph.neighbors[u];\n    for (var i = 0; i < neighbors.length; ++i) {\n      var v = neighbors[i];\n      if (visited[v.id]) {\n        continue;\n      }\n      visited[v.id] = true;\n      if (v.id === target) {\n        var path = [ v ];\n        if (u !== source) {\n          path.push(({id: u, value: v.value}));\n          u = predecessor[u]; \n        }\n        console.log(u)\n        while (u.id !== source) {\n          path.push(u);\n          u = predecessor[u.id];          \n        }\n        path.push(u);\n        path.reverse();\n        return path;\n      }\n      predecessor[v.id] = ({id: u, value: v.value});\n      queue.push(v.id);\n    }\n  }\n  return [];\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nd3 = require(\"d3\")\nunderscore = require(\"underscore\")\nnodes = FileAttachment(\"/data/js_graph/nodes.json\").json()\nnames = nodes.map(x => x.name)\nedges1 = FileAttachment(\"/data/js_graph/edges1.json\").json()\nedges2 = FileAttachment(\"/data/js_graph/edges2.json\").json()\nedges = edges1.concat(edges2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction get_graph(edges) {\n  var g = new Graph();\n  edges.forEach(x => g.addEdge(x.source, x.target, x.value));\n  return g;\n}\ngraph = get_graph(edges)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction transform_dict_id(nodes) {\n  var dict = {};\n  nodes.forEach(x => dict[x.id] = x);\n  return dict;\n}\nid_to_node = transform_dict_id(nodes)\nfunction transform_dict_name(nodes) {\n  var dict = {};\n  nodes.forEach(x => dict[x.name] = x);\n  return dict;\n}\nname_to_node = transform_dict_name(nodes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoose the first Actor\n\n\nShow the code\nviewof search1 = Inputs.search(names)\nviewof actor1 = Inputs.select(underscore.sample(search1, 100))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoose the second Actor\n\n\nShow the code\nviewof search2 = Inputs.search(names)\nviewof actor2 = Inputs.select(underscore.sample(search2, 100))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nactor1_node = name_to_node[actor1]\nactor2_node = name_to_node[actor2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npath = {\n  if (graph.neighbors !== undefined && actor1 !== null && actor2 !== null) {\n    return shortestPath(graph, actor1_node.id, actor2_node.id);\n  } else {\n    return [];\n  }\n}\npathNodes = path.map(x => id_to_node[x.id])\npathNodes.forEach(x => x[\"group\"] = \"Shortest path\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npathEdges = path.reduce((xs, x) => {\n  var last = xs.pop()\n  if (last != undefined) {\n    xs.push({source: last.id, target: x.id, value: last.value, group: \"Shortest path\"})\n  }\n  xs.push(x)\n  return xs\n}, [])\nuseless = pathEdges.pop()\ndata = ({\n    nodes: pathNodes,\n    links: pathEdges,\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npathNeigh = d3.group(pathNodes.flatMap(x => graph.neighbors[x.id]), d => d.id)\npathNeigh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction output_path_exist() {\n  if (graph.neighbors !== undefined && path !== undefined && path.length === 0) {\n    return \"There is no path between \" + actor1 + \" and \" + actor2 + \".\";\n  } else {\n    return \"\";\n  }\n}\nDOM.text(output_path_exist())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ncolors = d3.scaleOrdinal()\n  .domain([\"Shortest path\"])\n  .range(['#4797C9']);\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nchart = {\n  var height = 600;\n  var width = 900;\n  var svg = d3.create(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n    .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  var legend = svg.selectAll(\"legend\")\n    .data(colors.domain())\n    .enter()\n    .append(\"g\") \n    .attr(\"transform\", (d, i) => `translate(${width / 2 - 120},${i * 20 - height / 2 + 20})`); \n\n  legend.append(\"circle\")\n    .attr(\"cx\", 0)\n    .attr(\"cy\", 0)\n    .attr(\"r\", 5)\n    .attr(\"fill\", colors);\n\n  legend.append(\"text\")\n    .attr(\"x\", 10)\n    .attr(\"y\", 5)\n    .text(d => d);\n\n  var simulation = d3.forceSimulation()\n    .force(\"center\", d3.forceCenter())\n    .force(\"charge\", d3.forceManyBody().strength(-3000).distanceMax(450).distanceMin(85))\n    .force(\"link\", d3.forceLink().id(d => d.id));\n\n  var links = svg.selectAll(\"links\")\n    .data(data.links)\n    .enter()\n    .append(\"line\")\n    .attr(\"stroke\", \"#BDBDBD\")\n    .attr(\"stroke-width\", l => l.value.length * 5);\n\n  links.append(\"title\").text(l => l.value);\n\n  var linkText = svg.selectAll(\"links\")\n    .data(data.links)\n    .enter()\n    .append(\"text\")\n      .attr(\"fill\", \"#0022B7\")\n    .text(l => l.value);\n\n  var nodes = svg.selectAll(\"nodes\")\n    .data(data.nodes)\n    .enter()\n    .append(\"g\")\n    .call(d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended));\n\n  nodes.append(\"title\").text(d => d.name);\n\n  var circles = nodes.append(\"circle\")\n    .attr(\"class\", \"circle\")\n    .attr(\"r\", 30)\n    .attr(\"fill\", d => colors(d.group))\n    .attr(\"stroke\", \"#ffffff\")\n    .attr(\"stroke-width\", 2)\n\n  var text = nodes.append(\"text\")\n    .style(\"fill\", \"black\")\n    .style(\"font-weight\", \"bold\")\n    .attr(\"dx\", 0)\n    .attr(\"dy\", 5)\n    .attr(\"text-anchor\",\"middle\")\n    .text(d => d.name);\n\n  simulation.nodes(data.nodes);\n  simulation.force(\"link\").links(data.links)\n  simulation.on(\"tick\", function () {\n    links.attr(\"x1\", d => d.source.x)\n      .attr(\"y1\", d => d.source.y)\n      .attr(\"x2\", d => d.target.x)\n      .attr(\"y2\", d => d.target.y);\n\n    nodes.attr(\"transform\", d => \"translate(\" + d.x + \",\" + d.y + \")\")\n\n    linkText.attr(\"x\", function(d) {\n            if (d.target.x > d.source.x) { return (d.source.x + (d.target.x - d.source.x)/2); }\n            else { return (d.target.x + (d.source.x - d.target.x)/2); }\n        })\n        .attr(\"y\", function(d) {\n            if (d.target.y > d.source.y) { return (d.source.y + (d.target.y - d.source.y)/2); }\n            else { return (d.target.y + (d.source.y - d.target.y)/2); }\n        });\n  });\n\n  function dragstarted(event) {\n    if (!event.active) simulation.alphaTarget(0.3).restart();\n    event.subject.fx = event.subject.x;\n    event.subject.fy = event.subject.y;\n  };\n  \n  function dragged(event) {\n    event.subject.fx = event.x;\n    event.subject.fy = event.y;\n  };\n  \n  function dragended(event) {\n    if (!event.active) simulation.alphaTarget(0);\n    event.subject.fx = null;\n    event.subject.fy = null;\n  };\n\n  return Object.assign(svg.node(), {scales: d3.schemeTableau10});\n}"
  }
]