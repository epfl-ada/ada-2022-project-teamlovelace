[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Source data for this project.\n\nhttp://www.cs.cmu.edu/~ark/personas"
  },
  {
    "objectID": "full_network.html",
    "href": "full_network.html",
    "title": "The world of actors",
    "section": "",
    "text": "The actor network is modeled by a graph, where each actor represents as node \\(i \\in \\{1 \\cdots N\\}\\). Edges \\((i,j) \\in E\\) are placed between two actors if they have appeared in the same movie. We additionally weigh each edge by the number of movies the two actors have in common.\nBefore we dive head first into computing the graph edges, we need to estimate how much space it will take. An upper bound for the number of edges is \\(\\mathcal{O}(\\text{number of movies} \\times \\text{number of actors per movie}^2)\\), since each movie links two actors together.\nWe therefore compute the number of actors per movie, and report the histogram in Figure 1.\n\n\n\n\n\n\n  \n    \n      \n      numactors_per_movie\n      movie_name\n    \n  \n  \n    \n      0\n      115\n      Hemingway & Gellhorn\n    \n    \n      1\n      87\n      Taking Woodstock\n    \n    \n      2\n      81\n      Captain Corelli's Mandolin\n    \n    \n      3\n      81\n      Terror in the Aisles\n    \n    \n      4\n      72\n      Walk Hard: The Dewey Cox Story\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      64324\n      1\n      Primo\n    \n    \n      64325\n      1\n      The Stranger Who Looks Like Me\n    \n    \n      64326\n      1\n      On the Wing\n    \n    \n      64327\n      1\n      Robot Bastard!\n    \n    \n      64328\n      1\n      Incident\n    \n  \n\n64329 rows × 2 columns\n\n\n\n\n\n\n\n\nFigure 1: Distribution of actors per movie\n\n\n\n\nWith a total of 64329 movies, the distribution seems to follow an exponential law (with a somewhat long tail due to outliers). We pick the median at 6.0 to represent the distribution. We can now compute the upper bound :\n\n\nnum_edges ~ 2.3e+06\n\n\nWith order \\(10^6\\) edges we will not be doing any full graph visualization, however we can do some analysis on the large graph.\nAfter explicit computation, we get the following graph :\n\n\nGraph with 135061 nodes and 2080273 edges\n\n\nThus we end up with around 200k edges. (and around 224k edges if we keep edges between the two same actors distinct).\nThe graph contains around 135k actors, spanning movies released between 1888 and 2016.\n\n\n\nAmong other real-life networks, social networks typically exhibit power law distribution of degrees. These types of networks are said to be “scale-free”, due to the scale-invariance of the power law. We compute the distribution for the network at hand.\n\n\n\n\n\nFigure 2: Degree distribution of the whole network\n\n\n\n\nThe degree distribution shown in Figure 2 clearly follows a power law \\(p(k) \\sim k^{-\\gamma}\\), and we recover the exponent \\(\\gamma = 1+\\mu \\approx 2.15 \\pm 0.07\\).\nAccording to the Molly-Reed condition (Molloy and Reed 1995), the entire network is fully connected to a giant component if \\(\\langle k^2 \\rangle < 2 \\langle k \\rangle\\). In our case with a power law of \\(\\mu \\approx 1.15\\), the left hand side integral diverges, and so we expect to find a giant cluster. Although since \\(\\langle k \\rangle < \\infty\\), we expect the giant component has not yet absorbed all clusters (“percolation”), and some smaller localized clusters remain (Bouchaud J. P. 2015).\nTo put this result into perspective, the Internet has in-degree distribution with \\(\\mu \\approx 1\\), scientific papers have \\(\\mu \\approx 2\\), human sexual contacts have \\(\\mu \\approx 2.4\\) (Bouchaud J. P. 2015). We are therefore in the presence of a network in which large clusters play an important role.\n\n\n\nWe compute the connected components of the graph (clusters), as well as their diameter. As the true diameter computation scales as \\(\\mathcal O(\\text{number of nodes})^3\\), we resort to using an approximative algorithm. Data about the 1272 clusters is plotted on Figure 3.\n\n\n\n\n\nFigure 3: Distribution of cluster sizes and diameter\n\n\n\n\nAs expected, we indeed have a giant cluster containing \\(\\sim 10^5\\) actors, accounting for about 94% of the total number of actors in the graph.\nWe also remark that even though the cluster size spans orders of magnitude, the cluster diameter spans between 1 and 14. The smaller clusters contain up to \\(10^2\\) actors have diameter at most 3, and their size seem to roughly follow a power law with exponential cutoff (Bouchaud J. P. 2015). Albeit its exponentially large size, the giant component has a diameter of 14, which is in-line with the small-world characteristic of scale-free networks : the diameter of the network scales only logarithmically with its size, i.e. \\(d \\sim \\log N\\) (Barabasi, Albert, and Jeong 2000)."
  },
  {
    "objectID": "belgian.html",
    "href": "belgian.html",
    "title": "The world of actors",
    "section": "",
    "text": "The belgian subnetwork is formed by slicing the original graph, keeping only nodes corresponding to belgian actors, and the edges between them.\nWe end up with the following graph :\n\n\nGraph with 191 nodes and 558 edges\n\n\nHowever, looking into the graph, we see that it is composed of a large central cluster and a few smaller graphs of at most 5 nodes. These are the result of slicing the original graph, where belgian actors that incidentally played together in an international movie still appear, despite being isolated.\nWe list out the connected components, and keep only the largest one.\n\n\nGraph with 156 nodes and 526 edges\nGraph with 3 nodes and 2 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 3 edges\nGraph with 5 nodes and 10 edges\nGraph with 3 nodes and 3 edges\nGraph with 3 nodes and 2 edges\nGraph with 3 nodes and 2 edges\nGraph with 3 nodes and 3 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 2 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 2 edges\n\n\nBefore we jump into the graph analysis and visualization, we make a small detour to examine the data. In particular, we would like to detect some cliques by naively looking at the number of movies actors have in common. This is done in Figure 1, which shows that most actors in the belgian graph feature together in only one movie.\n\n\n\n\n\nFigure 1: Number of shared movies between belgian actors\n\n\n\n\nWe list out the actors that feature together in more than 3 movies :\n\n\n\n\n\n\n  \n    \n      \n      actor1_name\n      actor2_name\n      movie_count\n    \n  \n  \n    \n      73\n      Patrick Descamps\n      Lucas Belvaux\n      3\n    \n    \n      347\n      Bouli Lanners\n      Benoît Poelvoorde\n      3\n    \n    \n      422\n      Olivier Gourmet\n      Jérémie Renier\n      4\n    \n    \n      508\n      Serge Larivière\n      Yolande Moreau\n      3\n    \n  \n\n\n\n\nFrom this data, it is not immediately clear whether there is a clique formed by or containing these actors. Time to visualize the graph !\n\n\n\nWe visualize the actor graph in Figure 2, coloring edges based on the 72 different movies, and making edges proportionally larger to the number of movies two actors have in common.\n\n\n\n\n\nFigure 2: First visualization of the belgian actor graph\n\n\n\n\nFrom this visualization we can see the emergence of small communities corresponding to one movie played in common. In the lower left, we can see Barbara Sarafian bridging the gap between two small communities.\nSome individuals, such as Jan Decleir or Jérémie Renier stand out by the sheer number of edges connected to them.\nJust left of Jérémie Renier, we can also visually inspect and confirm the small clique composed by Yolande Moreau, Serge Larivière, Benoît Poelvoorde, and Bouli Lanners.\nJean-Claude Van Damme, a well-known actor, appears marginal here. This again is an effect of slicing the original graph, where Jean-Claude Van Damme was well-connected, but appeared in more international movies, with non-belgian actors.\n\n\n\nInstead of relying on visualization to evaluate centrality, we can also compute diffent metrics, such as degree, eigenvector and betweenness centrality.\nFor each centrality metric, we print out the top-5 actors.\n\n\n\n\n\n\n  \n    \n      \n      degree_centrality\n      actor_name\n    \n  \n  \n    \n      106\n      0.264516\n      Jan Decleir\n    \n    \n      107\n      0.129032\n      Jérémie Renier\n    \n    \n      45\n      0.122581\n      François Beukelaers\n    \n    \n      149\n      0.116129\n      Filip Peeters\n    \n    \n      100\n      0.116129\n      Matthias Schoenaerts\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      eigenvector_centrality\n      actor_name\n    \n  \n  \n    \n      106\n      0.408367\n      Jan Decleir\n    \n    \n      10\n      0.209502\n      Werner De Smedt\n    \n    \n      56\n      0.205922\n      Bert Haelvoet\n    \n    \n      149\n      0.204257\n      Filip Peeters\n    \n    \n      58\n      0.193717\n      Gert Portael\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      betweenness_centrality\n      actor_name\n    \n  \n  \n    \n      106\n      0.395843\n      Jan Decleir\n    \n    \n      100\n      0.254062\n      Matthias Schoenaerts\n    \n    \n      45\n      0.116199\n      François Beukelaers\n    \n    \n      107\n      0.110377\n      Jérémie Renier\n    \n    \n      69\n      0.108920\n      Bouli Lanners\n    \n  \n\n\n\n\nJan Decleir comes out on top each time. Matthias Schoenaerts, François Beukelaers as well as Filip Peeters appear two times. The different methods result in quite different results. We try to explain this by plotting the distribution of the centrality measures in Figure 3, as well a visualizing in Figure 4.\n\n\n\n\n\nFigure 3: Distribution of different centrality measures\n\n\n\n\n\n\n\n\n\nFigure 4: Visual comparison of different centrality measures\n\n\n\n\nThis yields the following observations.\nDegree centrality\nDegree centrality only yields local information, since it is just proportional to the local node degree. Therefore, it might not be as insightful as compared to measures which take into account the entire graph\nVisually, we see that Jan Decleir appears as a the only very large node. This points to the problem that degree centrality only looks “one hop away”, where we would intuitively think that neighbours of Jan Decleir would have larger centrality, just because they are directly connected to a hub.\nEigenvector centrality\nMathematically, this adresses the issue of “one-hop” degree centrality, as this centrality is derived from a self-consistent equation taking into account the whole graph. Nodes connected to the high eigenvector centrality nodes are more likely to have a high eigenvector centrality. Eigenvector centrality produces a powerlaw-like distribution : only a few actors are “central”, with many actors having a small centrality.\nBetweenness centrality\nMany nodes have zero betweenness centrality : these correspond to actors on the “edge of the graph”, and therefore do not contribute to the general connectedness of the graph. And indeed, this is confirmed visually, as nodes far away from the core appear very small (and should not appear at all, if it weren’t for the minimal node size set for the visualization). Barbara Sarafian, despite living on the edge of the graph, has a nonzero betweenness centrality, as this actor connects two small communities.\n\n\n\nAnother important aspect of social networks is the formation of clusters. Again, there exist algorithms to try to find these clusters. We study two such algorithms, the Clauset-Newman-Moore and Louvain algorithms. Their comparison is interesting, in that their approach are opposite.\nThe Louvain algorithm tries to join clusters together to increase modularity, which measures how internally well-connected the cluster is compared to the outside.\nThe Clauset-Newman-Moore algorithm splits the graph, removing edges with high betweenness. The reasoning behind this is that communities are separated by a few edges, through which the majority of the “traffic” between the two communities flow.\n\n\n\n\n\nFigure 5: Comparison of the two clustering algorithms. Nodes are colored according to community, and size according to eigenvalue centrality. The minimum vertex size is set to be large, such that colours are easily visible.\n\n\n\n\nFigure 5 visualizes the different clusterings. Communities often group actors which seem to have similar eigenvalue centrality, and have played in the same movies. For instance, the green community is well connected by majoritarily two movies, colored cyan and purple.\nIn the Louvain clustering, three communities (blue, gray, brown in the right of the graph) stand out as corresponding to the “popular” belgian actors, all having large eigenvalue centrality. The five other communities (green, red, pink, orange, purple) are that of smaller actors. These might correspond to more “niche” communities. We note that the niche communities rely on highly connected actors to connect to the rest of the graph. Notably in this case, Matthieu Schoenaerts and Johan Heldenbergh provide a bridge to and from the pink community.\nFor this particular graph, the Louvain clustering seems more reasonable. The blue community from the Clauset-Newman-Moore clustering seems to be way too large, especially when a very small gray community in the upper-left is composed of only 5 nodes. Louvain clustering on the other hand, seems to produce more evenly distributed communities.\nOne explanation for this could be that the Louvain algorithm works on node degree, instead of the edges. In this case, the node degree should be taken into account, as we would expect to group “popular” and “niche” actors in separate communities. The Clauset-Newman-Moore places importance on edge via the betweenness measure, however we argue that the edges generated by each movie should be equally important, rendering this algorithm less effective for our problem."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "growth.html",
    "href": "growth.html",
    "title": "The world of actors",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt, matplotlib as mpl\nimport numpy as np\nimport igraph as ig\nimport random\nfrom tqdm import tqdm\nimport statsmodels.formula.api as smf\nimport itertools\nimport pickle\n\n\ndf_edges: pd.DataFrame = pd.read_pickle('../data/generated/graph/actor_graph_full.ungrouped.pkl')\ndf_movies: pd.DataFrame = pd.read_pickle('../data/generated/preprocessed/movies.pkl')\n\n\n\n\n\ndf_edges['movie_release'] = df_movies.set_index('fb_id').loc[df_edges.movie_fbid].movie_release.values\ndf_edges.sort_values('movie_release', inplace=True)\n\nprint(f'length before purge : {len(df_edges)}')\ndf_edges.dropna(subset='movie_release', inplace=True)  # some movies do not have dates associated to them\n# df_edges.drop(index=df_edges[((df_edges.movie_release <= min(years_bins)) | (df_edges.movie_release > max(years_bins)))].index, inplace=True)\n# df_edges.drop(index=df_edges[(df_edges.movie_release > 2015)].index, inplace=True)\nprint(f'length after purge : {len(df_edges)}')\n\n# assert min(years_bins) < df_edges.movie_release.min()\n# assert df_edges.movie_release.max() <= max(years_bins)\n\ndf_edges['movie_release_year'] = df_edges['movie_release'].map(lambda t: t.year).astype(pd.Int64Dtype())\n\n# we bin the years together to reduce noise on the subsequent analyses\n# we consider the graph at t=0 at year 1920\ndf_edges['year_bin'] = pd.cut(df_edges.query('movie_release_year > 1920').movie_release_year, range(1920, 2010+1, 10), precision=0)\ndf_edges['year_qbin'] = pd.qcut(df_edges.query('movie_release_year > 1920').movie_release_year, len(range(1920, 2010+1, 10))-1, precision=0)\n\ndf_edges\n\nlength before purge : 2241657\nlength after purge : 2224839\n\n\n\n\n\n\n  \n    \n      \n      actor1_fbid\n      actor2_fbid\n      movie_fbid\n      movie_release\n      movie_release_year\n      year_bin\n      year_qbin\n    \n  \n  \n    \n      678016\n      /m/0k2pyv\n      /m/0k2pz0\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n    \n    \n      678017\n      /m/0k2pyv\n      /m/0k2pz6\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n    \n    \n      678018\n      /m/0k2pyv\n      /m/0k2pzd\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n    \n    \n      678019\n      /m/0k2pz0\n      /m/0k2pz6\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n    \n    \n      678020\n      /m/0k2pz0\n      /m/0k2pzd\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1118305\n      /m/06k6ns\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n    \n    \n      1118306\n      /m/06k6ns\n      /m/0fdc74\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n    \n    \n      1118307\n      /m/0fdc74\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n    \n    \n      1118296\n      /m/0736qr\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n    \n    \n      1118302\n      /m/06k6ns\n      /m/0g7dfl\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n    \n  \n\n2224839 rows × 7 columns\n\n\n\n\nyears = df_edges.movie_release_year.unique().astype(int)\n\n\n\n\nWe consider the initial actor network at the year 1900\nhttps://barabasi.com/f/622.pdf\nIs there preferential attachment ?\n\n\nWe use numerical indices instead of freebase ids, in order to index into numpy arrays later\n\ncodes, uniques = pd.factorize(pd.concat([df_edges.actor1_fbid, df_edges.actor2_fbid]))\ndf_edges['actor1_numid'] = codes[:len(df_edges)]\ndf_edges['actor2_numid'] = codes[len(df_edges):]\ndf_edges\n\n\n\n\n\n  \n    \n      \n      actor1_fbid\n      actor2_fbid\n      movie_fbid\n      movie_release\n      movie_release_year\n      year_bin\n      year_qbin\n      actor1_numid\n      actor2_numid\n    \n  \n  \n    \n      678016\n      /m/0k2pyv\n      /m/0k2pz0\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n      0\n      1\n    \n    \n      678017\n      /m/0k2pyv\n      /m/0k2pz6\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n      0\n      2\n    \n    \n      678018\n      /m/0k2pyv\n      /m/0k2pzd\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n      0\n      119615\n    \n    \n      678019\n      /m/0k2pz0\n      /m/0k2pz6\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n      1\n      2\n    \n    \n      678020\n      /m/0k2pz0\n      /m/0k2pzd\n      /m/044ggd\n      1888-01-01\n      1888\n      NaN\n      NaN\n      1\n      119615\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1118305\n      /m/06k6ns\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n      61010\n      52747\n    \n    \n      1118306\n      /m/06k6ns\n      /m/0fdc74\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n      61010\n      80735\n    \n    \n      1118307\n      /m/0fdc74\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n      80735\n      52747\n    \n    \n      1118296\n      /m/0736qr\n      /m/0fqwzd\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n      41038\n      52747\n    \n    \n      1118302\n      /m/06k6ns\n      /m/0g7dfl\n      /m/064mmw7\n      2016-06-08\n      2016\n      NaN\n      (2009.0, 2016.0]\n      61010\n      74039\n    \n  \n\n2224839 rows × 9 columns\n\n\n\n\nprint(f'total {uniques.nunique()} nodes (actors)')\n\ntotal 134649 nodes (actors)\n\n\n\n\n\n\ndef get_edges_year(year: int) -> pd.DataFrame:\n    \"\"\"Return weighted edges for all movies released before and on ``year``\"\"\"\n    return (df_edges\n        .query(f'movie_release_year <= {year}')\n        .groupby(['actor1_numid', 'actor2_numid'])\n        [['movie_fbid']]\n        .nunique()\n        .reset_index()\n        .rename(columns={'movie_fbid': 'movie_count'})\n    )\n\n# test the function\nrandom.seed(0)  # seed the random layout\nfig, ax = plt.subplots()\ng0 = ig.Graph.DataFrame(get_edges_year(years[10])[['actor1_numid', 'actor2_numid', 'movie_count']], directed=False, use_vids=False)\nig.plot(g0, target=ax,\n    vertex_label=g0.vs['name'], vertex_label_size=7,\n    edge_width=g0.es['movie_count']\n);\n\n\n\n\n\n\nNote : actors keep their degree even after they die !\n\nif False:  # recompute\n    degrees = np.zeros((uniques.nunique(), df_edges.movie_release_year.nunique()))\n\n    for iyear, year in tqdm(enumerate(years), total=len(years)):\n        g = ig.Graph.DataFrame(get_edges_year(year)[['actor1_numid', 'actor2_numid', 'movie_count']], directed=False, use_vids=False)\n        # print(g.summary())\n        for node, k in zip(g.vs, g.degree()):\n            degrees[node['name'], iyear] = k\n    \n    np.savez('../data/generated/graph/network_growth/actor_graph_full_degrees.npz', degrees)\n\nelse:  # load from precomputed\n    degrees = np.load('../data/generated/graph/network_growth/actor_graph_full_degrees.npz')['arr_0']\n\n\ndelta_degrees = np.diff(degrees, axis=-1)\n\n\n# k = 0 corresponds to actors not yet born\ndegrees[degrees == 0] = np.nan\n# Delta k = 0 corresponds to actors that don't evolve\ndelta_degrees[delta_degrees == 0] = np.nan\n\n\n# convert the data to a dataframe\ndf_degree = pd.DataFrame(degrees, columns=years).melt(var_name='year', value_name='k', ignore_index=False)\ndf_degree.dropna(inplace=True)\ndf_degree.reset_index(inplace=True, names='actor_numid')\n\ndf_delta_degree = pd.DataFrame(delta_degrees, columns=years[:-1]).melt(var_name='year', value_name='dk', ignore_index=False)\ndf_delta_degree.dropna(inplace=True)\ndf_delta_degree.reset_index(inplace=True, names='actor_numid')\n\ndf_pref_attachment_delta = pd.merge(df_delta_degree, df_degree, how='inner', on=['actor_numid', 'year'])\n\n\ndf_pref_attachment_delta\n\n\n\n\n\n  \n    \n      \n      actor_numid\n      year\n      dk\n      k\n    \n  \n  \n    \n      0\n      7\n      1898\n      4.0\n      1.0\n    \n    \n      1\n      12\n      1898\n      4.0\n      1.0\n    \n    \n      2\n      7\n      1901\n      6.0\n      5.0\n    \n    \n      3\n      11\n      1901\n      6.0\n      5.0\n    \n    \n      4\n      12\n      1901\n      6.0\n      5.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      195683\n      51342\n      2015\n      1.0\n      149.0\n    \n    \n      195684\n      52861\n      2015\n      4.0\n      57.0\n    \n    \n      195685\n      61062\n      2015\n      1.0\n      114.0\n    \n    \n      195686\n      73797\n      2015\n      1.0\n      51.0\n    \n    \n      195687\n      79054\n      2015\n      1.0\n      76.0\n    \n  \n\n195688 rows × 4 columns\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nnp.random.seed(0)\n# we plot only 256 actors because plotting all 1.3M of them would take ages !!\nax.plot(years, degrees[np.random.randint(0, len(degrees), 256)].T, color='black', alpha=0.5, linewidth=1)\nax.set_xlabel('Year')\nax.set_ylabel('Degree')\nax.set_title('Actor degree as a function of time')\nplt.show()\n\n\n\n\nThe degree seems to grow linearly in time, but faster when actors enter late into the graph !\nSee the preliminary evidence based on degree growth\n\nfig, axs = plt.subplots(figsize=(6*3, 2*3), ncols=6, nrows=2, sharex=True, sharey=True, constrained_layout=True)\n\ndf_pref_attachment_delta['year_bin'] = pd.cut(df_pref_attachment_delta.year, range(1900, 2020+1, 10))\n\nfor (year_bin, df_pref_attachment_delta_bin), ax in zip(df_pref_attachment_delta.groupby('year_bin'), axs.flat):\n    # ax.plot(df_pref_attachment_delta_bin.k, df_pref_attachment_delta_bin.dk, '.', markersize=2, alpha=0.2)\n    ax.hist2d(\n        np.log10(df_pref_attachment_delta_bin.k), np.log10(df_pref_attachment_delta_bin.dk),\n        bins=[30,30], norm='log', vmin=1, vmax=100, cmap='viridis'\n    )\n    # NOTE : we don't use year_bin.right because then we get \"From 2011 to 2020\" \n    ax.set_title(f'From {year_bin.left+1} to {df_pref_attachment_delta_bin.year.max()}', fontsize='medium')\n    # ax.set_xscale('log')\n    # ax.set_yscale('log')\n    ax.set_xlabel('Log degree $\\\\log k$')\n    ax.set_ylabel('Log degree evolution $\\\\log \\\\Delta k$')\n    ax.label_outer()\n\nfig.suptitle('Preliminary evidence of preferential attachment through degree evolution')\n\nplt.show()\n\n\n\n\n\n\n\nSee also Jeong et al, 2003\nWe consider adding actors to the graph progressively, and not considering edges created between existing actors.\nEach actor appears for the first time in a movie, and using the first movie release date we can prune the edges that do not correspond to the first addition.\nWe want to estimate\n\\(\\Pi(k_i) = \\frac{k_i}{\\sum_j k_j} \\approx \\frac{\\text{number of new connections to node } i}{\\text{total number of new connections}}\\)\n\nTake the distribution of edges at t0\nAdd edges, and see where they connect\nThe probability Pi(k) = [histogrammed probability of new connections @ k] / [histogram probability of degree distribution @ k],\nthen normalize Pi(k)\nThe denominator normalizes the fact high degrees are more rare, but nodes proportionally connect more to them\n\nNote : we do not consider internal links\n\n\nIn order to have a good estimate, we batch edges together by movie release year, so that the probability estimate is not too noisy\ncut : binning every 10 years qcut : binning 12 quartiles\n\nif False:  # recompute\n    for binning_method in ['year_bin', 'year_qbin']:\n        g = ig.Graph.DataFrame(df_edges.query('movie_release_year <= 1920')[['actor1_fbid', 'actor2_fbid']], directed=False, use_vids=False)\n        pa_internal = []\n        pa_external = []\n        degree_distribution = {}\n\n        for bin, df_edges_bin in tqdm(df_edges.query('1920 < movie_release_year').groupby(binning_method, sort=False)):\n            degrees = g.degree()\n            degree_lookup = dict(zip([ v['name'] for v in g.vs ], degrees))\n            degree_distribution[bin] = pd.Series(degrees).value_counts(normalize=True).sort_index()\n            new_actors = set()\n\n            for _, edge in df_edges_bin.iterrows():\n                actor1_degree = degree_lookup.get(edge.actor1_fbid, 0)\n                actor2_degree = degree_lookup.get(edge.actor2_fbid, 0)\n                \n                if actor1_degree == 0:\n                    new_actors.add(edge.actor1_fbid)\n                if actor2_degree == 0:\n                    new_actors.add(edge.actor2_fbid)\n\n                # both added at the same time. ignore\n                if actor1_degree == 0 and actor2_degree == 0:\n                    continue\n\n                # external edge\n                if actor1_degree == 0 and actor2_degree > 0:\n                    pa_external.append((actor2_degree, bin))\n                if actor1_degree > 0 and actor2_degree == 0:\n                    pa_external.append((actor1_degree, bin))\n\n                # internal edge\n                if actor1_degree > 0 and actor2_degree > 0:\n                    pa_internal.append((actor1_degree*actor2_degree, bin))\n\n            g.add_vertices(list(new_actors))\n            g.add_edges(zip(df_edges_bin.actor1_fbid, df_edges_bin.actor2_fbid))\n\n        df_pa_external = pd.DataFrame(pa_external, columns=['k', 'bin'])\n        df_pa_internal = pd.DataFrame(pa_internal, columns=['k1k2', 'bin'])\n        df_degree_distribution = (pd.DataFrame(degree_distribution)\n            .melt(var_name='bin', value_name='p_k', ignore_index=False)\n            .reset_index(names='k')\n            .dropna()\n        )\n\n        if True:  # save to disk\n            df_pa_external.to_pickle(f'../data/generated/graph/network_growth/pa_external_{binning_method}.pkl')\n            df_pa_internal.to_pickle(f'../data/generated/graph/network_growth/pa_internal_{binning_method}.pkl')\n            df_degree_distribution.to_pickle(f'../data/generated/graph/network_growth/degree_distribution_{binning_method}.pkl')\n\n\ndf_pa_external_year_bin = pd.read_pickle('../data/generated/graph/network_growth/pa_external_year_bin.pkl')\ndf_pa_internal_year_bin = pd.read_pickle('../data/generated/graph/network_growth/pa_internal_year_bin.pkl')\ndf_degree_distribution_year_bin = pd.read_pickle('../data/generated/graph/network_growth/degree_distribution_year_bin.pkl')\ndf_pa_external_year_qbin = pd.read_pickle('../data/generated/graph/network_growth/pa_external_year_qbin.pkl')\ndf_pa_internal_year_qbin = pd.read_pickle('../data/generated/graph/network_growth/pa_internal_year_qbin.pkl')\ndf_degree_distribution_year_qbin = pd.read_pickle('../data/generated/graph/network_growth/degree_distribution_year_qbin.pkl')\n\n\n# assert the distributions are correctly normalized\nassert (df_degree_distribution_year_bin.groupby('bin').p_k.sum() > 0.999).all()\nassert (df_degree_distribution_year_qbin.groupby('bin').p_k.sum() > 0.999).all()\n\n\n# inspect dataframe\ndf_degree_distribution_year_bin\n\n\n\n\n\n  \n    \n      \n      k\n      bin\n      p_k\n    \n  \n  \n    \n      0\n      1\n      (1920, 1930]\n      0.060832\n    \n    \n      1\n      2\n      (1920, 1930]\n      0.045526\n    \n    \n      2\n      3\n      (1920, 1930]\n      0.055338\n    \n    \n      3\n      4\n      (1920, 1930]\n      0.069859\n    \n    \n      4\n      5\n      (1920, 1930]\n      0.062402\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      6148\n      1117\n      (2000, 2010]\n      0.000012\n    \n    \n      6150\n      1143\n      (2000, 2010]\n      0.000012\n    \n    \n      6151\n      1233\n      (2000, 2010]\n      0.000012\n    \n    \n      6154\n      1339\n      (2000, 2010]\n      0.000012\n    \n    \n      6155\n      1387\n      (2000, 2010]\n      0.000012\n    \n  \n\n3303 rows × 3 columns\n\n\n\n\nfor df_degree_distribution in [df_degree_distribution_year_bin, df_degree_distribution_year_qbin]:\n    fig, axs = plt.subplots(figsize=(4, df_degree_distribution.bin.nunique()*1.5),\n        nrows=df_degree_distribution.bin.nunique(), sharex=True, sharey=True, constrained_layout=True)\n    for (bin, df_degree_distribution_bin), ax in zip(df_degree_distribution.groupby('bin'), axs.flat):\n        ax.plot(df_degree_distribution_bin.k, df_degree_distribution_bin.p_k, '.', markersize=2)\n        ax.set_xscale('log')\n        ax.set_yscale('log')\n        ax.label_outer()\n        ax.set_title(f'Degree distribution used for the bin {bin}', fontsize='small')\n    plt.show() \n\n\n\n\n\n\n\n\ndef compute_pi_kappa(df_pa_external, df_degree_distribution):\n    pi_external = (\n        df_pa_external.groupby('bin')\n        .k.value_counts(normalize=True)  # histogram of edge connections as a function of k\n        .sort_index()\n        .rename('pi')  # distribution of edge connections\n    )\n    for bin in df_pa_external.bin.unique():\n        # we need to divide by the distribution of edges,\n        # because otherwise the histogram is biased toward the more frequent degrees\n        pi_external[bin] /= (\n            df_degree_distribution.query('bin == @bin')  # the distribution of degrees right before the bin of edges was added\n            .set_index('k').p_k\n        )\n        # normalize\n        pi_external[bin] /= pi_external[bin].sum()\n\n    # compute cumulative sum\n    kappa_external = pi_external.groupby('bin').cumsum().rename('kappa')\n\n    pi_external = pi_external.reset_index()\n    kappa_external = kappa_external.reset_index()\n\n    return pi_external, kappa_external\n\n\nfor df_pa_external, df_degree_distribution in [(df_pa_external_year_bin, df_degree_distribution_year_bin), (df_pa_external_year_qbin, df_degree_distribution_year_qbin)]:\n    pi_external, kappa_external = compute_pi_kappa(df_pa_external, df_degree_distribution)\n\n    fig, axs = plt.subplots(nrows=2, figsize=(8, 6), constrained_layout=True)\n\n    sns.scatterplot(pi_external, x='k', y='pi', hue='bin', linewidth=0, alpha=0.8, s=10, palette='viridis', ax=axs[0])\n    sns.move_legend(axs[0], loc='upper left', bbox_to_anchor=(1, 1))\n    axs[0].set_ylabel('Preferential attachment $\\\\Pi(k)$')\n    axs[0].set_xlabel('Degree $k$')\n    axs[0].set_yscale('log')\n    axs[0].set_xscale('log')\n    axs[0].set_title('Preferential attachment by year bin, and on entire dataset')\n\n    sns.lineplot(kappa_external, x='k', y='kappa', hue='bin', palette='viridis', legend=False, ax=axs[1])\n    # sns.move_legend(axs[1], loc='lower right', fontsize='small')\n    axs[1].set_ylabel('Cumulative function $\\\\kappa(k)$')\n    axs[1].set_xlabel('Degree $k$')\n    axs[1].set_yscale('log')\n    axs[1].set_xscale('log')\n    axs[1].set_title('Cumulative function by year bin')\n\n    plt.show()\n\n\n\n\n\n\n\n\ndef compute_fitcoefs(df_kappa):\n    values = []\n\n    for bin, df_kappa_bin in df_kappa.groupby('bin'):\n        if len(df_kappa_bin) >= 2:\n            model = smf.ols(formula=\"log_kappa ~ log_k\", data=dict(log_k=np.log10(df_kappa_bin.k), log_kappa=np.log10(df_kappa_bin.kappa)))\n            res = model.fit()\n            # display(res.summary())\n            alpha_plus_one = (res.params['log_k'], *res.conf_int().loc['log_k'])\n            values.append((bin, alpha_plus_one[0]-1, alpha_plus_one[1]-1, alpha_plus_one[2]-1))\n        else:\n            values.append((bin, np.nan, np.nan, np.nan))\n\n    return pd.DataFrame(values, columns=['bin', 'alpha', 'ci_lower', 'ci_upper'])\n\n\nfitcoefs = {\n    'year_bin': compute_fitcoefs(compute_pi_kappa(df_pa_external_year_bin, df_degree_distribution_year_bin)[1]),\n    'year_qbin': compute_fitcoefs(compute_pi_kappa(df_pa_external_year_qbin, df_degree_distribution_year_qbin)[1])\n}\n\n\nfitcoefs\n\n{'year_bin':             bin     alpha  ci_lower  ci_upper\n 0  (1920, 1930]  0.276953  0.210638  0.343267\n 1  (1930, 1940]  0.210021  0.169118  0.250924\n 2  (1940, 1950]  0.510332  0.476495  0.544169\n 3  (1950, 1960]  0.336974  0.318484  0.355464\n 4  (1960, 1970]  0.386186  0.366752  0.405619\n 5  (1970, 1980]  0.433514  0.417537  0.449490\n 6  (1980, 1990]  0.419824  0.405383  0.434265\n 7  (1990, 2000]  0.522066  0.502441  0.541692\n 8  (2000, 2010]  0.639683  0.624278  0.655087,\n 'year_qbin':                 bin     alpha  ci_lower  ci_upper\n 0  (1920.0, 1952.0]  0.099836  0.045346  0.154326\n 1  (1952.0, 1970.0]  0.333914  0.311530  0.356299\n 2  (1970.0, 1982.0]  0.442242  0.426329  0.458155\n 3  (1982.0, 1991.0]  0.431424  0.417991  0.444858\n 4  (1991.0, 1998.0]  0.547346  0.527997  0.566695\n 5  (1998.0, 2002.0]  0.614853  0.597720  0.631986\n 6  (2002.0, 2005.0]  0.626947  0.611793  0.642101\n 7  (2005.0, 2007.0]  0.670241  0.655415  0.685068\n 8  (2007.0, 2010.0]  0.669631  0.658343  0.680920\n 9  (2010.0, 2016.0]  0.801972  0.789656  0.814289}\n\n\n\nfig, ax = plt.subplots(figsize=(6, 3), constrained_layout=True)\n\nmid_years = fitcoefs['year_bin'].bin.map(lambda x: (x.left + x.right)/2)\nqmid_years = fitcoefs['year_qbin'].bin.map(lambda x: (x.left + x.right)/2)\n\nalpha_mean, alpha_std = fitcoefs['year_qbin'].alpha.mean(), fitcoefs['year_qbin'].alpha.std()\nax.axhline(alpha_mean, alpha=0.6, color='tab:gray')\nax.annotate(f'qcut mean : $\\\\alpha \\\\approx {alpha_mean:.2f} \\pm {alpha_std:.2f}$',\n    xy=(0.02, alpha_mean+0.02), va='bottom', xycoords=('axes fraction', 'data'), color='tab:gray')\nax.fill_between(mid_years, fitcoefs['year_bin'].ci_lower, fitcoefs['year_bin'].ci_upper, alpha=0.2)\nax.fill_between(qmid_years, fitcoefs['year_qbin'].ci_lower, fitcoefs['year_qbin'].ci_upper, alpha=0.2)\nax.plot(mid_years, fitcoefs['year_bin'].alpha, 'o--', label='cut')\nax.plot(qmid_years, fitcoefs['year_qbin'].alpha, 'o--', label='qcut')\n\nax.set_xticks([ int(x.left) for x in fitcoefs['year_bin'].bin ] + [ int(fitcoefs['year_bin'].bin.iloc[-1].right) ])\nax.set_xticklabels(ax.get_xticks(), rotation=45)\nax.legend(loc='lower right')\n\nax.set_ylim((0.0, 1.1))\nax.set_title('Evolution of preferential attachment exponent, $\\\\Pi(k) \\\\sim k^{\\\\alpha}$')\nplt.show()\n\n\n\n\nThis also confirms actors do indeed grow faster as they enter later into the graph !\n\n\n\n\n\n\ndef p_k_th(alpha: float, k_mean: float):\n    def p_k(k: np.ndarray) -> np.ndarray:\n        p = k**(-alpha) * np.exp(-2/(k_mean*(1-alpha)) * k**(1-alpha))\n        p /= p.sum()\n        return p\n    return p_k\n\n\nfig, ax = plt.subplots()\n\ndegrees_final = degrees[:, -1].copy()\ndegrees_final.sort()\npdf = pd.Series(degrees_final).value_counts(normalize=True).sort_index()\nax.plot(pdf.index, pdf.values, linewidth=1, label='True PDF')\n# sns.histplot(degrees_final, log_scale=(True, True), stat='probability', label='true distribution')\n\n# year_to_idx = { year: i for i, year in enumerate(years) }\n# k_means = [ np.nanmean(degrees[:, year_to_idx[int(qmid_year)]]) for qmid_year in qmid_years ]\nk = np.logspace(0, 3, 100)\nsns.lineplot(x=k, y=p_k_th(fitcoefs['year_qbin'].alpha.mean(), degrees_final.mean())(k),\n    label='mean fit based\\non the B-A model')\n\nax.set_xscale('log')\nax.set_yscale('log')\n\nax.legend()\nplt.show()\n\n\n\n\nProblem : the cutoff is too sharp with our fits. it looks like the true alpha is larger\n-> We have not taken into account internal links\n\n\n\n\n\n\nkmax = np.nanmax(degrees, axis=0)\n\n\nfig, ax = plt.subplots()\nax.plot(years, kmax)\nax.twinx().plot(years, np.log(years-years.min()+1)**(1/(1-0.52)), color='tab:orange')\nax.set_xlabel('year')\nax.set_ylabel('$k_{max}$')\nax.set_title('Evolution of the maximum actor network degree')\nplt.show()\n\n\n\n\n\n\n\n\nif False:\n    g = ig.Graph(directed=False)\n    total_movies = 0\n    components = []\n\n    for year, df_edges_year in tqdm(df_edges.groupby('movie_release_year', sort=False)):\n        current_actors = set([ v['name'] for v in g.vs ])\n        actors_from_edges = set(df_edges_year.actor1_fbid.unique()) | set(df_edges_year.actor2_fbid.unique())\n        new_actors = actors_from_edges - current_actors\n\n        g.add_vertices(list(new_actors))\n        g.add_edges(zip(df_edges_year.actor1_fbid, df_edges_year.actor2_fbid))\n\n        total_movies += df_edges_year.movie_fbid.nunique()\n        components.append((len(g.connected_components(mode='weak')), total_movies, year))\n\n    df_components = pd.DataFrame(components, columns=['num_connected', 'num_movies', 'year'])\n\n    if True:  # save to disk\n        df_components.to_pickle('../data/generated/graph/network_growth/components.pkl')\n\nelse:\n    df_components = pd.read_pickle('../data/generated/graph/network_growth/components.pkl')\n\n100%|██████████| 122/122 [00:07<00:00, 16.77it/s]\n\n\n\n# at the start, every movie adds a new unique component, then the clusters start to appear !\nfig, ax = plt.subplots()\nax.plot(df_components.year, df_components.num_connected, label='number of connected components')\nax.plot(df_components.year, df_components.num_movies, label='number of movies')\nax.set_yscale('log')\nax.legend()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The world of actors",
    "section": "",
    "text": "Abstract\nThe well-known concept of six degrees of separation descibes that one can link anyone to any other person by a “friend of a friend” chain of length six. Our project stems from this idea : we aim to analyze the graph of movie actors formed by linking actors having played in the same movie, or by various similarity metrics. This field is called SNA, for Social Network Analysis and has been quite active in the last decade. With this in mind, we want to look at how our social graph can relate to real world’s social interactions. Our main approach consists in investigating the connectedness of actors in different subsets of our graph, according to different features such as countries and release year. We can ask ourselves how similarities between actors reflect themselves in the connectedness of actors. Can the time evolution of the social graph reflect geopolitical events, and reveal information about the influence of star actors on the future career of newcomers ?"
  }
]