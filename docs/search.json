[
  {
    "objectID": "full_network.html",
    "href": "full_network.html",
    "title": "The world of actors",
    "section": "",
    "text": "The actor network is modeled by a graph, where each actor represents as node \\(i \\in \\{1 \\cdots N\\}\\). Edges \\((i,j) \\in E\\) are placed between two actors if they have appeared in the same movie. We additionally weigh each edge by the number of movies the two actors have in common.\nBefore we dive head first into computing the graph edges, we need to estimate how much space it will take. An upper bound for the number of edges is \\(\\mathcal{O}(\\text{number of movies} \\times \\text{number of actors per movie}^2)\\), since each movie links two actors together.\nWe therefore compute the number of actors per movie, and report the histogram in Figure 1.\n\n\n\n\n\n\n  \n    \n      \n      numactors_per_movie\n      movie_name\n    \n  \n  \n    \n      0\n      115\n      Hemingway & Gellhorn\n    \n    \n      1\n      87\n      Taking Woodstock\n    \n    \n      2\n      81\n      Captain Corelli's Mandolin\n    \n    \n      3\n      81\n      Terror in the Aisles\n    \n    \n      4\n      72\n      Walk Hard: The Dewey Cox Story\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      64324\n      1\n      Primo\n    \n    \n      64325\n      1\n      The Stranger Who Looks Like Me\n    \n    \n      64326\n      1\n      On the Wing\n    \n    \n      64327\n      1\n      Robot Bastard!\n    \n    \n      64328\n      1\n      Incident\n    \n  \n\n64329 rows × 2 columns\n\n\n\n\n\n\n\n\nFigure 1: Distribution of actors per movie\n\n\n\n\nWith a total of 64329 movies, the distribution seems to follow an exponential law (with a somewhat long tail due to outliers). We pick the median at 6.0 to represent the distribution. We can now compute the upper bound :\n\n\nnum_edges ~ 2.3e+06\n\n\nWith order \\(10^6\\) edges we will not be doing any full graph visualization, however we can do some analysis on the large graph.\nAfter explicit computation, we get the following graph :\n\n\nGraph with 135061 nodes and 2080273 edges\n\n\nThus we end up with around 2M edges (and around 2.2M edges if we keep edges between the two same actors distinct), which matches the order of magnitude estimation !\nThe graph contains around 135k actors, spanning movies released between 1888 and 2016, as shown in Figure 2. We note that after approximatively 2010, the number of movies in the dataset drops fast, even though the original paper with the data was published in August 2013.\n\n\n\n\n\nFigure 2: Number of movies released each year\n\n\n\n\n\n\n\nAmong other real-life networks, social networks typically exhibit power law distribution of degrees. These types of networks are said to be “scale-free”, due to the scale-invariance of the power law. We compute the distribution for the network at hand.\n\n\n\n\n\nFigure 3: Degree distribution of the whole network\n\n\n\n\nThe degree distribution shown in Figure 3 clearly follows a power law \\(p(k) \\sim k^{-\\gamma}\\), and we recover the exponent \\(\\gamma = 1+\\mu \\approx 2.15 \\pm 0.07\\).\nAccording to the Molly-Reed condition (Molloy and Reed 1995), the entire network is fully connected to a giant component if \\(\\langle k^2 \\rangle < 2 \\langle k \\rangle\\). In our case with a power law of \\(\\mu \\approx 1.15\\), the left hand side integral diverges, and so we expect to find a giant cluster. Although since \\(\\langle k \\rangle < \\infty\\), we expect the giant component has not yet absorbed all clusters (“percolation”), and some smaller localized clusters remain (Bouchaud J. P. 2015).\nTo put this result into perspective, the Internet has in-degree distribution with \\(\\mu \\approx 1\\), scientific papers have \\(\\mu \\approx 2\\), human sexual contacts have \\(\\mu \\approx 2.4\\) (Bouchaud J. P. 2015). We are therefore in the presence of a network in which large clusters play an important role.\n\n\n\nWe compute the connected components of the graph (clusters), as well as their diameter. As the true diameter computation scales as \\(\\mathcal O(\\text{number of nodes})^3\\), we resort to using an approximative algorithm. Data about the 1272 clusters is plotted on Figure 4.\n\n\n\n\n\nFigure 4: Distribution of cluster sizes and diameter\n\n\n\n\nAs expected, we indeed have a giant cluster containing \\(\\sim 10^5\\) actors, accounting for about 94% of the total number of actors in the graph.\nWe also remark that even though the cluster size spans orders of magnitude, the cluster diameter spans between 1 and 14. The smaller clusters contain up to \\(10^2\\) actors have diameter at most 3, and their size seem to roughly follow a power law with exponential cutoff (Bouchaud J. P. 2015). Albeit its exponentially large size, the giant component has a diameter of 14, which is in-line with the small-world characteristic of scale-free networks : the diameter of the network scales only logarithmically with its size, i.e. \\(d \\sim \\log N\\) (Barabasi, Albert, and Jeong 2000)."
  },
  {
    "objectID": "belgian.html",
    "href": "belgian.html",
    "title": "The world of actors",
    "section": "",
    "text": "The belgian subnetwork is formed by slicing the original graph, keeping only nodes corresponding to belgian actors, and the edges between them.\nWe end up with the following graph :\n\n\nGraph with 191 nodes and 558 edges\n\n\nHowever, looking into the graph, we see that it is composed of a large central cluster and a few smaller graphs of at most 5 nodes. These are the result of slicing the original graph, where belgian actors that incidentally played together in an international movie still appear, despite being isolated.\nWe list out the connected components, and keep only the largest one.\n\n\nGraph with 156 nodes and 526 edges\nGraph with 3 nodes and 2 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 3 edges\nGraph with 5 nodes and 10 edges\nGraph with 3 nodes and 3 edges\nGraph with 3 nodes and 2 edges\nGraph with 3 nodes and 2 edges\nGraph with 3 nodes and 3 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 2 edges\nGraph with 2 nodes and 1 edges\nGraph with 3 nodes and 2 edges\n\n\nBefore we jump into the graph analysis and visualization, we make a small detour to examine the data. In particular, we would like to detect some cliques by naively looking at the number of movies actors have in common. This is done in Figure 1, which shows that most actors in the belgian graph feature together in only one movie.\n\n\n\n\n\nFigure 1: Number of shared movies between belgian actors\n\n\n\n\nWe list out the actors that feature together in more than 3 movies :\n\n\n\n\n\n\n  \n    \n      \n      actor1_name\n      actor2_name\n      movie_count\n    \n  \n  \n    \n      73\n      Patrick Descamps\n      Lucas Belvaux\n      3\n    \n    \n      347\n      Bouli Lanners\n      Benoît Poelvoorde\n      3\n    \n    \n      422\n      Olivier Gourmet\n      Jérémie Renier\n      4\n    \n    \n      508\n      Serge Larivière\n      Yolande Moreau\n      3\n    \n  \n\n\n\n\nFrom this data, it is not immediately clear whether there is a clique formed by or containing these actors. Time to visualize the graph !\n\n\n\nWe visualize the actor graph in Figure 2, coloring edges based on the 72 different movies, and making edges proportionally larger to the number of movies two actors have in common.\n\n\n\n\n\nFigure 2: First visualization of the belgian actor graph\n\n\n\n\nFrom this visualization we can see the emergence of small communities corresponding to one movie played in common. In the lower left, we can see Barbara Sarafian bridging the gap between two small communities.\nSome individuals, such as Jan Decleir or Jérémie Renier stand out by the sheer number of edges connected to them.\nJust left of Jérémie Renier, we can also visually inspect and confirm the small clique composed by Yolande Moreau, Serge Larivière, Benoît Poelvoorde, and Bouli Lanners.\nJean-Claude Van Damme, a well-known actor, appears marginal here. This again is an effect of slicing the original graph, where Jean-Claude Van Damme was well-connected, but appeared in more international movies, with non-belgian actors.\n\n\n\nInstead of relying on visualization to evaluate centrality, we can also compute diffent metrics, such as degree, eigenvector and betweenness centrality.\nFor each centrality metric, we print out the top-5 actors.\n\n\n\n\n\n\n  \n    \n      \n      degree_centrality\n      actor_name\n    \n  \n  \n    \n      106\n      0.264516\n      Jan Decleir\n    \n    \n      107\n      0.129032\n      Jérémie Renier\n    \n    \n      45\n      0.122581\n      François Beukelaers\n    \n    \n      149\n      0.116129\n      Filip Peeters\n    \n    \n      100\n      0.116129\n      Matthias Schoenaerts\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      eigenvector_centrality\n      actor_name\n    \n  \n  \n    \n      106\n      0.408367\n      Jan Decleir\n    \n    \n      10\n      0.209502\n      Werner De Smedt\n    \n    \n      56\n      0.205922\n      Bert Haelvoet\n    \n    \n      149\n      0.204257\n      Filip Peeters\n    \n    \n      58\n      0.193717\n      Gert Portael\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      betweenness_centrality\n      actor_name\n    \n  \n  \n    \n      106\n      0.395843\n      Jan Decleir\n    \n    \n      100\n      0.254062\n      Matthias Schoenaerts\n    \n    \n      45\n      0.116199\n      François Beukelaers\n    \n    \n      107\n      0.110377\n      Jérémie Renier\n    \n    \n      69\n      0.108920\n      Bouli Lanners\n    \n  \n\n\n\n\nJan Decleir comes out on top each time. Matthias Schoenaerts, François Beukelaers as well as Filip Peeters appear two times. The different methods result in quite different results. We try to explain this by plotting the distribution of the centrality measures in Figure 3, as well a visualizing in Figure 4.\n\n\n\n\n\nFigure 3: Distribution of different centrality measures\n\n\n\n\n\n\n\n\n\nFigure 4: Visual comparison of different centrality measures\n\n\n\n\nThis yields the following observations.\nDegree centrality\nDegree centrality only yields local information, since it is just proportional to the local node degree. Therefore, it might not be as insightful as compared to measures which take into account the entire graph\nVisually, we see that Jan Decleir appears as a the only very large node. This points to the problem that degree centrality only looks “one hop away”, where we would intuitively think that neighbours of Jan Decleir would have larger centrality, just because they are directly connected to a hub.\nEigenvector centrality\nMathematically, this adresses the issue of “one-hop” degree centrality, as this centrality is derived from a self-consistent equation taking into account the whole graph. Nodes connected to the high eigenvector centrality nodes are more likely to have a high eigenvector centrality. Eigenvector centrality produces a powerlaw-like distribution : only a few actors are “central”, with many actors having a small centrality.\nBetweenness centrality\nMany nodes have zero betweenness centrality : these correspond to actors on the “edge of the graph”, and therefore do not contribute to the general connectedness of the graph. And indeed, this is confirmed visually, as nodes far away from the core appear very small (and should not appear at all, if it weren’t for the minimal node size set for the visualization). Barbara Sarafian, despite living on the edge of the graph, has a nonzero betweenness centrality, as this actor connects two small communities.\n\n\n\nAnother important aspect of social networks is the formation of clusters. Again, there exist algorithms to try to find these clusters. We study two such algorithms, the Clauset-Newman-Moore and Louvain algorithms. Their comparison is interesting, in that their approach are opposite.\nThe Louvain algorithm tries to join clusters together to increase modularity, which measures how internally well-connected the cluster is compared to the outside.\nThe Clauset-Newman-Moore algorithm splits the graph, removing edges with high betweenness. The reasoning behind this is that communities are separated by a few edges, through which the majority of the “traffic” between the two communities flow.\n\n\n\n\n\nFigure 5: Comparison of the two clustering algorithms. Nodes are colored according to community, and size according to eigenvalue centrality. The minimum vertex size is set to be large, such that colours are easily visible.\n\n\n\n\nFigure 5 visualizes the different clusterings. Communities often group actors which seem to have similar eigenvalue centrality, and have played in the same movies. For instance, the green community is well connected by majoritarily two movies, colored cyan and purple.\nIn the Louvain clustering, three communities (blue, gray, brown in the right of the graph) stand out as corresponding to the “popular” belgian actors, all having large eigenvalue centrality. The five other communities (green, red, pink, orange, purple) are that of smaller actors. These might correspond to more “niche” communities. We note that the niche communities rely on highly connected actors to connect to the rest of the graph. Notably in this case, Matthieu Schoenaerts and Johan Heldenbergh provide a bridge to and from the pink community.\nFor this particular graph, the Louvain clustering seems more reasonable. The blue community from the Clauset-Newman-Moore clustering seems to be way too large, especially when a very small gray community in the upper-left is composed of only 5 nodes. Louvain clustering on the other hand, seems to produce more evenly distributed communities.\nOne explanation for this could be that the Louvain algorithm works on node degree, instead of the edges. In this case, the node degree should be taken into account, as we would expect to group “popular” and “niche” actors in separate communities. The Clauset-Newman-Moore places importance on edge via the betweenness measure, however we argue that the edges generated by each movie should be equally important, rendering this algorithm less effective for our problem."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "centrality_and_success.html",
    "href": "centrality_and_success.html",
    "title": "The world of actors",
    "section": "",
<<<<<<< HEAD
    "text": "We want to use wikipedia pageviews as a proxy for popularity, and try to find a correlation the centrality of actors in the actor graph. For this we used the dataset of pageviews from Homework 2. Since Wikipedia was founded at the start of the century, pageviews might not be relevant for older actors. To eliminate this possible bias we only considered only actors having played in recent movies and formed the actor-graph only based on the recent movies.\nWe therefore consider 49481 actors accross 41039 movies, 4631 of which (the actors) we have the pageview count.\n\n\n\n\n\n\n\nWe focused on three metrics of centrality: - Degree centrality: With how many other actors have the actors played - Eigenvector centrality - Betweenness: how much an actor bridges communities of actors\n\n\nFor those interested, we list here the most central actors.\n\n\n\n\n\n\n  \n    \n      \n      degree_centrality\n      actor_name\n    \n  \n  \n    \n      70\n      0.011601\n      Anupam Kher\n    \n    \n      744\n      0.010853\n      Jane Lynch\n    \n    \n      748\n      0.010287\n      Samuel L. Jackson\n    \n    \n      710\n      0.009903\n      David Koechner\n    \n    \n      202\n      0.009863\n      Justin Long\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      eigenvector_centrality\n      actor_name\n    \n  \n  \n    \n      1150\n      0.095907\n      David Strathairn\n    \n    \n      6434\n      0.095699\n      Nicole Kidman\n    \n    \n      4889\n      0.095603\n      Clive Owen\n    \n    \n      6102\n      0.094613\n      Parker Posey\n    \n    \n      4467\n      0.094151\n      Rodrigo Santoro\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      betweenness_centrality\n      actor_name\n    \n  \n  \n    \n      70\n      0.022059\n      Anupam Kher\n    \n    \n      313\n      0.013711\n      Michael Madsen\n    \n    \n      5728\n      0.013613\n      Lee Byung-Hun\n    \n    \n      5257\n      0.011191\n      Vera Farmiga\n    \n    \n      10\n      0.009740\n      Nassar\n    \n  \n\n\n\n\n\n\n\n\n\n\nWe first try to visualise the two values together.\nThe correlations we found were very modest with R^2 values of 0.016, 0.013, and 0.007 respectively\n\n\n\n\n\nFor movie count.\n    We find an average increase of 50.742% in pageviews per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       4.7326      0.015    324.735      0.000       4.704       4.761\nmovie_count     0.1782      0.018      9.787      0.000       0.143       0.214\n===============================================================================\n\n\n\n\n\n\n\nFor degree centrality.\n    We find an average increase of 3.7x in centrality per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      -3.6732      0.007   -529.463      0.000      -3.687      -3.660\nmovie_count     0.5656      0.009     65.250      0.000       0.549       0.583\n===============================================================================\n\nFor eigenvector centrality.\n    We find an average increase of 19.8x in centrality per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      -4.8026      0.026   -187.939      0.000      -4.853      -4.753\nmovie_count     1.2970      0.032     40.620      0.000       1.234       1.360\n===============================================================================\n\nFor betweenness centrality.\n    We find an average increase of 10.7x in centrality per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      -5.3659      0.042   -126.906      0.000      -5.449      -5.283\nmovie_count     1.0284      0.040     25.452      0.000       0.949       1.108\n===============================================================================\n\n\n\n\n\n\n\n\n\n\n\n\nHere the values are better correlated, with R^2 values of 0.479, 0.263, and 0.212 respectively. Movie count having a significant correlation with pageviews as well as with centrality, it acts as a confounder. We therefore aim to isolate the effects of centrality from those of movie count by using A/B testing\n\n\n\nThe A/B test is done by making pairs of actors with similar numbers of movies (Here similar means that their number of movies are less than 5% different) such that the first actor has a lower centrality than the second.\n\n\nFor degree centrality.\n    We find an average increase of -2.43% in pageviews per 2x increase in centrality\n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept               0.0007      0.020      0.033      0.974      -0.038       0.040\nlogratio_centrality    -0.0355      0.008     -4.231      0.000      -0.052      -0.019\n=======================================================================================\n\nFor eigenvector centrality.\n    We find an average increase of 4.73% in pageviews per 2x increase in centrality\n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept              -0.0351      0.019     -1.884      0.060      -0.072       0.001\nlogratio_centrality     0.0667      0.003     23.301      0.000       0.061       0.072\n=======================================================================================\n\nFor betweenness centrality.\n    We find an average increase of 2.58% in pageviews per 2x increase in centrality\n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept               0.2812      0.026     10.966      0.000       0.231       0.331\nlogratio_centrality     0.0367      0.004      9.174      0.000       0.029       0.045\n=======================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\nIn the end we find very little increases in pageviews when varying the centrality of actors. There is even a slight decrease concerning the degree centrality, although it is not statistically significant. The largest increase is with the eigenvalue centrality, with an average increase of 4.73% in pageviews per 2x increase in centrality, but with an R^2 value of 0.007. We conclude that if there is a relationship between the centrality of actors and their popularity, it is barely noticeable and not interesting."
=======
    "text": "We want to use wikipedia pageviews as a proxy for popularity, and try to find a correlation the centrality of actors in the actor graph. For this we used the dataset of pageviews from Homework 2. Since Wikipedia was founded at the start of the century, pageviews might not be relevant for older actors. To eliminate this possible bias we considered only actors having played in recent movies and formed the actor-graph only based on the recent movies.\nWe therefore consider 49481 actors accross 41039 movies, 4631 of which (the actors) we have the pageview count.\n\n\n\n\n\n\n\nWe focused on three metrics of centrality: - Degree centrality: With how many other actors have the actors played - Eigenvector centrality - Betweenness: how much an actor bridges communities of actors\n\n\nFor those interested, we list here the most central actors.\n\n\n\n\n\n\n  \n    \n      \n      degree_centrality\n      actor_name\n      fb_actor_id\n    \n  \n  \n    \n      70\n      0.011601\n      Anupam Kher\n      /m/0292l3\n    \n    \n      744\n      0.010853\n      Jane Lynch\n      /m/07m77x\n    \n    \n      748\n      0.010287\n      Samuel L. Jackson\n      /m/0f5xn\n    \n    \n      710\n      0.009903\n      David Koechner\n      /m/059j1m\n    \n    \n      202\n      0.009863\n      Justin Long\n      /m/07cjqy\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      eigenvector_centrality\n      actor_name\n    \n  \n  \n    \n      1150\n      0.095907\n      David Strathairn\n    \n    \n      6434\n      0.095699\n      Nicole Kidman\n    \n    \n      4889\n      0.095603\n      Clive Owen\n    \n    \n      6102\n      0.094613\n      Parker Posey\n    \n    \n      4467\n      0.094151\n      Rodrigo Santoro\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      betweenness_centrality\n      actor_name\n    \n  \n  \n    \n      70\n      0.019429\n      Anupam Kher\n    \n    \n      5728\n      0.011046\n      Lee Byung-Hun\n    \n    \n      10\n      0.010051\n      Nassar\n    \n    \n      6198\n      0.009872\n      Peter Stormare\n    \n    \n      5257\n      0.009498\n      Vera Farmiga\n    \n  \n\n\n\n\n\n\n\n\n\n\nWe first try to visualise the two values together.\nThe correlations we found were very modest with R^2 values of 0.016, 0.013, and 0.007 respectively\n\n\n\n\n\nFor movie count.\n    We find an average increase of 50.742% in pageviews per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       4.7326      0.015    324.735      0.000       4.704       4.761\nmovie_count     0.1782      0.018      9.787      0.000       0.143       0.214\n===============================================================================\n\n\n\n\n\n\n\nFor degree centrality.\n    We find an average increase of 3.7x in centrality per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      -3.6732      0.007   -529.463      0.000      -3.687      -3.660\nmovie_count     0.5656      0.009     65.250      0.000       0.549       0.583\n===============================================================================\n\nFor eigenvector centrality.\n    We find an average increase of 19.8x in centrality per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      -4.8026      0.026   -187.939      0.000      -4.853      -4.753\nmovie_count     1.2970      0.032     40.620      0.000       1.234       1.360\n===============================================================================\n\nFor betweenness centrality.\n    We find an average increase of 10.7x in centrality per 10x increase in movie count\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      -5.3659      0.042   -126.906      0.000      -5.449      -5.283\nmovie_count     1.0284      0.040     25.452      0.000       0.949       1.108\n===============================================================================\n\n\n\n\n\n\n\n\n\n\n\n\nHere the values are better correlated, with R^2 values of 0.479, 0.263, and 0.212 respectively. Movie count having a significant correlation with pageviews as well as with centrality, it acts as a confounder. We therefore aim to isolate the effects of centrality from those of movie count by using A/B testing\n\n\n\nThe A/B test is done by making pairs of actors with similar numbers of movies (Here similar means that their number of movies are less than 5% different) such that the first actor has a lower centrality than the second.\n\n\nFor degree centrality.\n    We find an average increase of -2.43% in pageviews per 2x increase in centrality\n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept               0.0007      0.020      0.033      0.974      -0.038       0.040\nlogratio_centrality    -0.0355      0.008     -4.231      0.000      -0.052      -0.019\n=======================================================================================\n\nFor eigenvector centrality.\n    We find an average increase of 4.73% in pageviews per 2x increase in centrality\n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept              -0.0351      0.019     -1.884      0.060      -0.072       0.001\nlogratio_centrality     0.0667      0.003     23.301      0.000       0.061       0.072\n=======================================================================================\n\nFor betweenness centrality.\n    We find an average increase of 2.58% in pageviews per 2x increase in centrality\n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept               0.2812      0.026     10.966      0.000       0.231       0.331\nlogratio_centrality     0.0367      0.004      9.174      0.000       0.029       0.045\n=======================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\nIn the end we find very little increases in pageviews when varying the centrality of actors. There is even a slight decrease concerning the degree centrality, although it is not statistically significant. The largest increase is with the eigenvalue centrality, with an average increase of 4.73% in pageviews per 2x increase in centrality, but with an R^2 value of 0.007. We conclude that if there is a relationship between the centrality of actors and their popularity, it is barely noticeable and not interesting."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "In this project, we analyzed a network of actors linked by the movies they played in. We found that the network exhibits a power-law degree distribution and that it is mainly composed of a single giant cluster, which is a common characteristic of many social networks.\nOur analysis of a subset of Belgian actors using classical SNA metrics provided further insight into the structure and dynamics of such an actor network. We calculated metrics such as degree centrality, betweenness centrality, eigenvector centrality and clustering methods, which capture the importance and influence of individual actors within the network. We also examined how the network grows and changes as new actors join and new movies are made //@TODO add a sentence.\nIn addition, we analyzed homophily between nodes based on attributes such as gender, country, and age. Our analysis showed that these attributes may influence the formation and maintenance of relationships within the network. For example, we found that actors are more likely to be connected to other actors from the same country but their age is not relevant.\nFinally, we explored the relationship between a node’s centrality in the network and the popularity of the actor. Our analysis showed that there is a weak, non-significant correlation between these two variables. This finding suggests that other factors may influence an actor’s popularity. More various data would be needed to better understand the complex relationship between an actor’s node in the network and its popularity.\nTo conclude, we have seen that the actor’s network we generated, displayed as we suggested, some of the same properties as real-life social networks.\n\n\n\nAlso feel free to try the shortest path visualization in the next section !"
>>>>>>> 33a5844 (added conclusion)
  },
  {
    "objectID": "viz.html",
    "href": "viz.html",
    "title": "Shorthest path visualization",
    "section": "",
    "text": "On this page you can explore the links between you favorite actors ! Just choose in the inputs below and the shortest path between the both will be displayed, if it exists.\nThe canvas here is limited so some node might not be displayed correctly, in that case you can just move the nodes around. Also you can hover over the nodes and links, if the text is difficult to read.\nQuick disclaimer: this page might take some time to load because we need to load the entire graph to do real time computation.\n\n\nShow the code\nfunction Graph() {\n  var neighbors = this.neighbors = {};\n  this.addEdge = function (u, v, val) {\n    if (neighbors[u] === undefined) {\n      neighbors[u] = [];\n    }\n    neighbors[u].push(({id: v, value: val}));\n    if (neighbors[v] === undefined) {\n      neighbors[v] = [];\n    }           \n    neighbors[v].push(({id: u, value: val}));\n  };\n\n  return this;\n}\n\nfunction shortestPath(graph, source, target) {\n  if (source == target) {\n    return source;\n  }\n  console.log(\"Start\")\n  var queue = [ source ],\n      visited = { [source]: true },\n      predecessor = {},\n      tail = 0;\n  while (tail < queue.length) {\n    var u = queue[tail++],\n        neighbors = graph.neighbors[u];\n    for (var i = 0; i < neighbors.length; ++i) {\n      var v = neighbors[i];\n      if (visited[v.id]) {\n        continue;\n      }\n      visited[v.id] = true;\n      if (v.id === target) {\n        var path = [ v ];\n        if (u !== source) {\n          path.push(({id: u, value: v.value}));\n          u = predecessor[u]; \n        } else {\n          path.push(({id: u, value: v.value}));\n          return path;\n        }\n        while (u.id !== source) {\n          path.push(u);\n          u = predecessor[u.id];      \n        }\n        path.push(u);\n        path.reverse();\n        return path;\n      }\n      predecessor[v.id] = ({id: u, value: v.value});\n      queue.push(v.id);\n    }\n  }\n  console.log(\"End\")\n  return [];\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nd3 = require(\"d3\")\nunderscore = require(\"underscore\")\nnodes_zip = FileAttachment(\"/data/js_graph/nodes.json.zip\").zip()\nnames = nodes.map(x => x.name)\nedges_zip = FileAttachment(\"/data/js_graph/edges.json.zip\").zip()\nnodes = nodes_zip.file(nodes_zip.filenames[0]).json()\nedges = edges_zip.file(edges_zip.filenames[0]).json()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction get_graph(edges) {\n  var g = new Graph();\n  edges.forEach(x => g.addEdge(x.source, x.target, x.value));\n  return g;\n}\ngraph = get_graph(edges)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction transform_dict_id(nodes) {\n  var dict = {};\n  nodes.forEach(x => dict[x.id] = x);\n  return dict;\n}\nid_to_node = transform_dict_id(nodes)\nfunction transform_dict_name(nodes) {\n  var dict = {};\n  nodes.forEach(x => dict[x.name] = x);\n  return dict;\n}\nname_to_node = transform_dict_name(nodes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoose the first Actor:\n\n\nShow the code\nviewof search1 = Inputs.search(names)\nviewof actor1 = Inputs.select(underscore.sample(search1, 100))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoose the second Actor:\n\n\nShow the code\nviewof search2 = Inputs.search(names)\nviewof actor2 = Inputs.select(underscore.sample(search2, 100))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nactor1_node = name_to_node[actor1]\nactor2_node = name_to_node[actor2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npath = {\n  if (graph.neighbors !== undefined && actor1 !== null && actor2 !== null && actor1 !== actor2) {\n    return shortestPath(graph, actor1_node.id, actor2_node.id);\n  } else {\n    return [];\n  }\n}\npathNodes = path.map(x => id_to_node[x.id])\npathNodes.forEach(x => x[\"group\"] = \"Shortest path\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npathEdges = path.reduce((xs, x) => {\n  var last = xs.pop()\n  if (last != undefined) {\n    xs.push({source: last.id, target: x.id, value: last.value, group: \"Shortest path\"})\n  }\n  xs.push(x)\n  return xs\n}, [])\nuseless = pathEdges.pop()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction get_neigh_node(pathNodes) {\n  var pathNId = pathNodes.map(x => x.id);\n  var pathNeighDict = pathNodes.flatMap(x => graph.neighbors[x.id]).filter(x => !pathNId.includes(x.id));\n  var pathNeigh = {};\n  pathNeighDict.map(x => {\n    if (pathNeigh[x.id] === undefined) {\n      var n = id_to_node[x.id];\n      n[\"count\"] = 1;\n      pathNeigh[x.id] = n;\n    } else {\n      pathNeigh[x.id][\"count\"] += 1;\n    }\n  })\n  var r = [];\n  Object.entries(pathNeigh).forEach(function([key, value]) {\n    var group = \"\"\n    if (value.count >= 4) {\n      group = \"4+-connected\";\n    } else {\n      group = value.count + \"-connected\";\n    }\n    r.push({id: value.id, name: value.name, group: group})\n  }); \n  return r;\n}\npathNeighNodes = get_neigh_node(pathNodes).filter(x => x.group !== \"1-connected\" && x.group !== \"2-connected\")\nfunction get_neigh_edges(pathNodes, pathNeighNodes) {\n  var pathNeighNodesId = pathNeighNodes.map(x => x.id);\n  var pathNeighDict = pathNodes.flatMap(x => graph.neighbors[x.id].filter(z => pathNeighNodesId.includes(z.id)).map(y => ({source: x.id, target: y.id, value: y.value, group: \"Neighbors\"})));\n  return pathNeighDict;\n}\npathNeighEdges = get_neigh_edges(pathNodes, pathNeighNodes)\ndata = ({\n    nodes: pathNodes.concat(pathNeighNodes),\n    links: pathEdges.concat(pathNeighEdges),\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction output_path_exist() {\n  if (graph.neighbors !== undefined && path !== undefined && path.length === 0) {\n    return \"There is no path between \" + actor1 + \" and \" + actor2 + \".\";\n  } else {\n    return \"\";\n  }\n}\nDOM.text(output_path_exist())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ncolors = d3.scaleOrdinal()\n  .domain([\"Shortest path\", \"3-connected\", \"4+-connected\"])\n  .range(['#4797C9','#fff686','#9e79db']);\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nchart = {\n  var height = 900;\n  var width = 900;\n  var svg = d3.create(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n    .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  var legend = svg.selectAll(\"legend\")\n    .data(colors.domain())\n    .enter()\n    .append(\"g\") \n    .attr(\"transform\", (d, i) => `translate(${width / 2 - 120},${i * 20 - height / 2 + 20})`); \n\n  legend.append(\"circle\")\n    .attr(\"cx\", 0)\n    .attr(\"cy\", 0)\n    .attr(\"r\", 5)\n    .attr(\"fill\", colors);\n\n  legend.append(\"text\")\n    .attr(\"x\", 10)\n    .attr(\"y\", 5)\n    .text(d => d);\n\n  var simulation = d3.forceSimulation()\n    .force(\"center\", d3.forceCenter())\n    .force(\"charge\", d3.forceManyBody().strength(d => {\n      if (d.group === \"Shortest path\") {\n        return -3000;\n      } else {\n        return -50;\n      }\n    }).distanceMax(450).distanceMin(85))\n    .force(\"link\", d3.forceLink().id(d => d.id));\n\n  var links = svg.selectAll(\"links\")\n    .data(data.links)\n    .enter()\n    .append(\"line\")\n    .attr(\"stroke\", \"#BDBDBD\")\n    .attr(\"stroke-width\", l => {\n      if (l.group === \"Shortest path\") {\n        return l.value.length * 5;\n      } else {\n        return 1;\n      }\n    });\n\n  links.append(\"title\").text(l => l.value);\n\n  var linkText = svg.selectAll(\"links\")\n    .data(data.links)\n    .enter()\n    .append(\"text\")\n      .attr(\"fill\", \"#0022B7\")\n    .text(l => {\n      if (l.group === \"Shortest path\") {\n        return l.value;\n      } else {\n        return \"\";\n      }\n    });\n\n  var nodes = svg.selectAll(\"nodes\")\n    .data(data.nodes)\n    .enter()\n    .append(\"g\")\n    .call(d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended));\n\n  nodes.append(\"title\").text(d => d.name);\n\n  var circles = nodes.append(\"circle\")\n    .attr(\"class\", \"circle\")\n    .attr(\"r\", d => {\n      if (d.group === \"Shortest path\") {\n        return 30;\n      } else {\n        return 5;\n      }\n    })\n    .attr(\"fill\", d => colors(d.group))\n    .attr(\"stroke\", \"#ffffff\")\n    .attr(\"stroke-width\", 2)\n\n  var text = nodes.append(\"text\")\n    .style(\"fill\", \"black\")\n    .style(\"font-weight\", \"bold\")\n    .attr(\"dx\", 0)\n    .attr(\"dy\", 5)\n    .attr(\"text-anchor\",\"middle\")\n    .text(d => {\n      if (d.group === \"Shortest path\") {\n        return d.name;\n      } else {\n        return \"\";\n      }\n    });\n\n  simulation.nodes(data.nodes);\n  simulation.force(\"link\").links(data.links)\n  simulation.on(\"tick\", function () {\n    links.attr(\"x1\", d => d.source.x)\n      .attr(\"y1\", d => d.source.y)\n      .attr(\"x2\", d => d.target.x)\n      .attr(\"y2\", d => d.target.y);\n\n    nodes.attr(\"transform\", d => \"translate(\" + d.x + \",\" + d.y + \")\")\n\n    linkText.attr(\"x\", function(d) {\n            if (d.target.x > d.source.x) { return (d.source.x + (d.target.x - d.source.x)/2); }\n            else { return (d.target.x + (d.source.x - d.target.x)/2); }\n        })\n        .attr(\"y\", function(d) {\n            if (d.target.y > d.source.y) { return (d.source.y + (d.target.y - d.source.y)/2); }\n            else { return (d.target.y + (d.source.y - d.target.y)/2); }\n        });\n  });\n\n  function dragstarted(event) {\n    if (!event.active) simulation.alphaTarget(0.3).restart();\n    event.subject.fx = event.subject.x;\n    event.subject.fy = event.subject.y;\n  };\n  \n  function dragged(event) {\n    event.subject.fx = event.x;\n    event.subject.fy = event.y;\n  };\n  \n  function dragended(event) {\n    if (!event.active) simulation.alphaTarget(0);\n    event.subject.fx = null;\n    event.subject.fy = null;\n  };\n\n  \n  \n  return Object.assign(svg.node(), {scales: d3.schemeTableau10});\n}"
  },
  {
    "objectID": "homophily.html",
    "href": "homophily.html",
    "title": "The world of actors",
    "section": "",
    "text": "One purpose of this project was to focus on the analysis of how much the similarity of two actors influences the likeliness of playing the same movie. For this, we assess similarily by using the homophily measure (Newman 2003), intuitively understood as the tendency for people to be attracted to those similar to themselves.\nKnowing that we are working on a dataset of actor who are linked between each other through movies, we decided to consider only actors with a minimum degree of fifteen. This threshold is based on results from the full actor network analysis, where we saw that the power law regime of the degree distribution starts around \\(k \\gtrsim 15\\). Another justification for this is that due to their low connectivity, low degree vertices do not carry much information about homophily.\nIn our dataset, each actor is associated with a number of attributes. We begin our analysis by quantifying the global distribution of each attribute.\n\nFor the actor nationality, we have the following interactive plot Figure 1. We observe a majority (around 45%) of american actors, followed by some country like Canada, India, Great britain, … We need to keep this distribution in mind for the rest of our study, especially for the comparison part.\n\n\n\n\n\n                                                \nFigure 1: World Map with number of Actors per Country\n\n\n\nOther attribute’s distribution are plotted in Figure 2, Figure 3, Figure 4.\n\n\n\n\n\nFigure 2: Distribution of (binary) gender in the dataset\n\n\n\n\n\n\n\n\n\nFigure 3: Number of actors born each year\n\n\n\n\n\n\n\n\n\nFigure 4: Height distibution among actors and distribution of movies played among actors\n\n\n\n\nSome of these distributions are likely to be gaussian (height) or even a power law distribution for the number of movies played. We can mention that the gender attribute is unbalanced in our dataset and need to be also taken with caution for further interpretations.\n\n\n\nWe will now pay attention to quantify the homophily score of each actor : - For categorical attribute, it can be describe as follow : \\[\\text{Homophily}_{i} = \\frac{1}{k_i} \\sum \\limits _j \\delta_{ij} \\] with \\(\\delta_{ij}\\) = 1 if same attribute, else -1 and \\(k_i\\) is the degree of the node \\(i\\)\nTherefore, we can calculate the homophily score of each actor. As interpretation, if its value tends to be close to 1, the concerned actor is more likely to play with similar actor. On the other hand, if its value becomes close to -1, it is the opposite case and the actor tends to play with dissimilar actors. (Golub and Jackson 2012)\nPay attention to the fact that we don’t take into account multiple edges between the same two actors (even though actors played several times with another one, we still consider it as one edge).\n\nFor scalar attribute, we can create a linear equation, meaning that the further the values are apart, the lower the value will be. We illustrated this defintion through the following plot :\n\n\n\n\n\n\nThe value for which we consider the homophily score to -1 depends on the standard deviation of the distribution (we target to aim the 95% threshold). \\[ \\Delta_{max} = 2 \\times \\sigma \\]\n\nWe then compute these homophily scores for each actors in order to vizualise the distribution. We end up with the following distributions according to the different attributes. The four first distributions are the categorical attributes especially with the distribution if the take only the homophily score of men actors (gender men) compared to the case where we take only the homophily score of women actress (gender women). The three next distributions are dedicated to the scalar attributes :\n\n\n\n\n\n\nThe results are showing some underlying piece of information. Indeed, we see firstly that total gender homophily is mainly driven by two distinguished distributions : the distribution of gender homophily for men and women. Knowing that the dataset contains more men than women, it explains partially that women’s homophily has the tendency to be negative in comparison to men’s homophily (the “probability” to have a women-men edge is higher than women-women). Moreover, we see that country attribute leads to quite high values of homophily (One first supposition we can make is that movies are more likely to gather actors from the same country, i.e. cluster of countries). Age and height attributes seem quite distributed around the zero (slightly shifted to positive values, thus perhaps a small underliying similarity). Finally, the number of movies played doesn’t lead to a direct conclusion as we have a quite flat distribution (we can mention that they are mainly positive values)\n\n\n\nThe raw distribution we gathered above could be not relevant and could lead to wrong interpretation because the dataset is not equally balanced among all attributes (for example the gender distribution). Therefore, if we want to make better observations, we need to compare it with what we expected to see for random edges between actors. We worked on two different possibility for the comparison that we will present to you below.\n\n\nThe first idea is to think about probabilities. For categorical attributes, we can calculate the probability for an actor to randomly be assigned to this attribute knowing the initial distribution over the dataset. Therefore, we decided to create a “mean actor” which is the hypothetical actor that we could get randomly. Each actor of the dataset will be linked to this mean actor and their associated homophily score shall be detailled as followed : \\[  \\text{Homophily}_{i} = p_{attribute} - (1- p_{attribute}) \\] with $ p_{attribute} $ being the probability that “mean actor” have the same attribute as the actor \\(i\\).\n\n\n\n\n\n\n\n\n\n\nAs first comparison observation, nationaties seems to be an attribute relevant for similarity in actor connections. On the other hand, we can’t get much information from gender comparison except that homophily score for men is slightly higher than the random case and oppositely women actress tends to have lower homophily score in the true dataset compared to the “mean” case.\n\n\n\nThe second option for comparison is the randomize the edges (La Fond and Neville 2010). We keep the degree of each actor, but instead of their true links, each edge will be repaced by an edge with a random actor in the dataset. With that, we can once again compute their homophily score. The advantage here is that we obtain a distribution that could be compared to the raw distributions we obtained earlier (and we can also perfom it on scalar attribute comapared to the “mean actor”)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_value for the mann_whitney_u_test on the distributions :\n\n\n\n\n\n\n\n  \n    \n      \n      attribute\n      p-values\n    \n  \n  \n    \n      0\n      number of movies\n      2.461082e-31\n    \n    \n      1\n      gender\n      0.000000e+00\n    \n    \n      2\n      nationalities\n      1.803844e-255\n    \n    \n      3\n      age\n      3.805015e-09\n    \n    \n      4\n      height\n      0.000000e+00\n    \n  \n\n\n\n\nBased on the results illustrated above, we can make different conclusions. Firstly, we obtained more or less the same interpretation for the categorical attributes. It seems like actors from a certain country are more quite likely to play together (we can think about cinemar studios as hollywood, bollyhood, …). Moreover, the age of the actor looks like this is not a relevant attribute to link actor in movies. The only observation we can made is that the true edges are slightly shifted to the left meaning that is decrease the total homophily of the network compared to the random edges. Furthermore, even though gender initialy seems to be significant for similarity, it appears that on our dataset, it is not a relevant attribute that connect more actor (We can think about a certain diversity of gender in the movies. Maybe in a further analysis, we could have a look on how this homophily evolve depending on the movie’s genre). Finally, height is not a significant attribute to connect people as we can see in the plot above, especially because we obtain the same distribution either with random edges and the true edges."
  },
  {
    "objectID": "growth.html",
    "href": "growth.html",
    "title": "The world of actors",
    "section": "",
    "text": "In this section we try to understand how the network grows, more precisely how edges are formed and where new actors connect to. For this, we timestamp each edge of the actor graph with the corresponding movie release date. In this operation, about 0.75% of the original edges are dropped.\n\n\nIn the Barabási-Albert model (Barabási and Pósfai 2016), the preferential attachment factor \\(\\Pi(k_i)\\) describes the probability with which a new node connects to an already existing node of degree \\(k_i\\). Therefore, the rate at which node \\(i\\)’s degree grows is proportional to \\(\\Pi(k_i)\\).\n\\[\n\\frac{\\mathrm d k_i}{\\mathrm d t} \\propto \\Pi(k_i)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIn reality the proportionality constant depends on \\(t\\), and in the large \\(t\\) limit we get (Barabási and Pósfai 2016)\n\\[\n\\frac{\\mathrm d k_i}{k_i} = \\frac 12 \\frac{\\mathrm dt}{t}\n\\]\nIntegrating and letting \\(k_i^0\\) be the degree with which node \\(i\\) first joined the network at time \\(t_i^0\\), we get :\n\\[\nk_i(t) = k_i^0 \\left( \\frac{t}{t_i^0} \\right)^\\beta\n\\]\nWhere the dynamical exponent \\(\\beta = 1/2\\) describes the degree growth speed.\nIn the subsequent analysis of degree evolution, we neglect these subtleties in order to give a first impression. We later do a more formal analysis based on (Jeong, Néda, and Barabási 2003).\n\n\nIn a naive numerical approach, we inspect whether there could be preferential attachment by computing the year-to-year degree evolution of each node in the network. Figure 1 gives us a first impression of the results, where we see the actors (individual lines) grow through time, until their growth becomes stagnant due to their end of career. We notice that actors who enter late in the graph tend to grow faster !\n\n\n\n\n\nFigure 1: Actor degree as a function of time. Each line models one actor. This figure shows only a random sample of 256 actors to give a general impression and to avoid cluttering the plot.\n\n\n\n\nIn Figure 2, we plot a 2D histogram of the degree evolution \\(\\Delta k_i\\) versus the original degree \\(k_i\\). Darker colours correspond to a higher density of samples. If there were no preferential attachement, that is \\(\\Pi = \\textrm{constant}\\), we would expect that \\(\\Delta k_i\\) be intependant of \\(k_i\\). However, the general trend of this figure shows that the higher the original degree \\(k_i(t)\\), the more the degree grows to \\(k_i(t+1) = k_i(t) + \\Delta k_i(t)\\), which shows that this network exhibits preferential attachment. And in accordance with Figure 1, we see that this preferential attachment is stronger when actors join the graph later.\n\n\n\n\n\nFigure 2: Preliminary evidence of preferential attachment through degree evolution. This figure shows degree evolution \\(\\Delta k_i\\) as a function of degree \\(k_i\\), binned every decade starting 1911.\n\n\n\n\n\n\n\n\n\nIn order to formalize the results of the previous section, we compute the preferential attachment factor based on the methods explained in (Jeong, Néda, and Barabási 2003).\nFor the following, we consider adding edges to the graph progressively. If the edge adds a new actor to the network (“external edge”), we record the degree of the actors it attaches to. Note that we therefore do not report the degrees between two existing actors (“internal edges”), but we still add them progressively.\nWe want to estimate :\n\\[\\Pi(k_i) = \\frac{k_i}{\\sum_j k_j} \\approx \\frac{\\text{number of new connections to node } i}{\\text{total number of new connections}}\\]\nNumerically, we perform this by histogramming. Therefore, the algorithm is as follows :\n\nSince we need an already existing population in order to estimate preferential attachment, we consider the initial graph at time \\(t_0 = 1920\\).\nBin edges together based on their date.\nFor each bin \\((t_-, t_+)\\), repeat :\n\nRecord the degree histogram \\(p(k)\\) at the start \\(t_-\\) of each bin.\nAdd the edges in the bin, and record the degree histogram \\(q(k)\\) of the actors that new actors connect to.\nThe probability distribution \\(\\Pi(k)\\) is obtained by dividing \\(q(k)\\) by \\(p(k)\\), and renormalizing. The denominator accounts for the fact that high degrees are more rare. If we didn’t perform this division, then we would observe little preferential attachment, as new actors connect to many low degree actors just due to the sheer number of them.\n\n\nWe perform binning using two methods, and plot the bin sizes in Figure 3. Binning edges together every decade gives us more control over the date at which we perform the estimation, but yields very uneven bin sizes. Binning edges together every 9th quantile (but rounding at integer years) yields more even bin sizes, but the dates are non-uniformly distributed.\n\n\n\n\n\nFigure 3: Sample sizes for the two edge binning methods\n\n\n\n\n\n\n\nThe Barabási-Albert model can be extended to other regimes of preferential attachment. (Jeong, Néda, and Barabási 2003) defines the scaling exponent \\(\\alpha\\) as :\n\\[\n\\Pi(k) \\sim k^\\alpha\n\\]\nIf \\(\\alpha = 1\\), we recover the original Barabási-Albert model (linear preferential attachment). Regimes \\(\\alpha < 1\\) are said to be sub-linear, and \\(\\alpha > 1\\) is super-linear. Larger values of \\(\\alpha\\) therefore correspond to a stronger preferential attachment, and one can intuitively expect the appearance of giant hubs.\nPerforming the computations, we scatter the results for \\(\\Pi(k)\\) in Figure 4. While it is clear that preferential attachment follows a power law, there is a lot of noise in the data. To address this, the paper suggests to use the cumulative function :\n\\[\n\\kappa(k) = \\int^k \\Pi(k') \\; \\mathrm d k' \\sim k^{\\alpha+1}\n\\]\n\n\n\n\n\n\n\n(a) Binning every decade\n\n\n\n\n\n\n\n(b) Binning every 9th quantile\n\n\n\n\nFigure 4: Preferential attachment \\(\\Pi(k)\\) and cumulative function \\(\\kappa(k)\\) for the two binning methods.\n\n\nPerforming the fit, we report the obtained scaling exponents and their 95% confidence interval in Figure 5. Using the quantile bins, we can also compute an “average exponent” to be around \\(\\alpha \\approx 0.5\\), putting this network in the sublinear regime. Both binning methods show that the exponent tends to increase with time, confirming the observation that actors grow faster as they enter later into the graph.\n\n\n\n\n\nFigure 5: Evolution of scaling exponent \\(\\alpha\\).\n\n\n\n\n\n\n\nWe now ask whether this model is able to predict the observed degree distribution. In the sub-linear regime, the probability distribution follows a stretched exponential distribution :\n\\[\np(k) \\sim k^{-\\alpha} \\exp \\left(\\frac{-2 k^{1-\\alpha}}{\\langle k \\rangle (1-\\alpha)} \\right)\n\\]\nFigure 6 compares the predicted and true distributions. Unfortunately, the match is poor : what happened ? When we computed the preferential attachment, recall we ignored internal edges, however they make up about 32% of the total edges, and are therefore not neglectible.\nTherefore, we predict that the true exponent must actually be greater than one, putting the model in the super-linear regime. This is further supported by Figure 7, in which we see that the largest degree grows roughly linearly, \\(k_{\\text{max}} \\sim t\\), as one would find on the super-linear regime (Barabási and Pósfai 2016).\n\n\n\n\n\nFigure 6: Comparing the true degree distribution to the one predicted by the Barabási-Albert model.\n\n\n\n\n\n\n\n\n\nFigure 7: Evolution of the maximum actor network degree"
  },
  {
<<<<<<< HEAD
=======
    "objectID": "homophily.html",
    "href": "homophily.html",
    "title": "The world of actors",
    "section": "",
    "text": "One purpose of this project was to focus on the analysis of how connected actors could be influenced to play in the same movie based on their distinguished attributes. In other words, does “similar” actors are more likely connected into same movies. We need therefore a definition of the similarity. According to several papers, one main tool to assess the similarity is called the homophily. It is the tendency for people to be attracted to those who seems similar to themselves.\nKnowing that we are working on a dataset of actor who are linked between each other through movies, we decided to consider only actors with a minimum degree of fifteen (which is quiet representative to movies in general as the movie story is based on interaction between a quiet large number of characters). Moreover, we also took our decision because low degree vertices shall add some unmeaningful values, particularly for categorical attribute (which can even become binary for very low link degree).\nFirst of all, we know that we will based our study on the attributes of each actors of the dataset. Then, as a data scientist, we need to begin our analysis by quantify each attributes among the different actors and observe the repartition over the dataset.\n\nFor the actor nationality, we have the following interactive plot. We observe a majority (around 45%) of american actors, followed by some country like Canada, India, Great britain, … We need to keep this distribution in mind for the rest of our study, especially for the comparison part.\n\n\n\n\n                                                \n\n\n\nFurthermore, we need to be attentive for the other attributes distribution. We highlight them according to the dataset as following :\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\nSome of these distributions are likely to be gaussian (height) or even a power low distribution for the number of movies played. We can mention that the gender attribute is unbalanced in our dataset and need to be also taken with caution for further interpretations.\nWe will now pay attention to quantify the homophily score of each actor : - For categorical attribute, it can be describe as follow : \\[Homophily_{i} = \\frac{\\sum \\limits _j \\delta_{ij}}{k_i} \\] with \\(\\delta_{ij}\\) = 1 if same attribute, else -1 and $ k_i $ is the degree of the node \\(i\\)\nTherefore, we can calculte the homophily score of each actor. As interpretation, if its value tends to be close to 1, the concerned actor is more likely to play with similar actor. On the other hand, if its value becomes close to -1, it is the opposite case and the actor tends to play with dissimilar actors. (Golub and Jackson 2012)\nPay attention to the fact that we don’t take into account multiple same edges (even though actors played several times with another one, we still consider it as one edge).\n\nFor scalar attribute, we can create a linear equation, meaning that the further the values are apart, the lower the value will be. We illustrated this defintion through the following plot :\n\n\n\n\n\n\nThe value for which we consider the homophily score to -1 depends on the standard deviation of the distribution (we target to aim the 95% threshold). \\[ \\Delta_{max} = 2 \\times \\sigma \\]\n\nWe then compute these homophily scores for each actors in order to vizualise the distribution. We end up with the following distributions according to the different attributes. The four first distributions are the categorical attributes especially with the distribution if the take only the homophily score of men actors (gender men) compared to the case where we take only the homophily score of women actress (gender women). The three next distributions are dedicated to the scalar attributes :\n\n\n\n\n\n\nThe results are showing some underlying piece of information. Indeed, we see firstly that total gender homophily is mainly driven by two distinguished distributions : the distribution of gender homophily for men and women. Knowing that the dataset contains more men than women, it explains partially that women’s homophily has the tendency to be negative in comparison to men’s homophily (the “probability” to have a women-men edge is higher than women-women). Moreover, we see that country attribute leads to quite high values of homophily (One first supposition we can make is that movies are more likely to gather actors from the same country, i.e. cluster of countries). Age and height attributes seem quite distributed around the zero (slightly shifted to positive values, thus perhaps a small underliying similarity). Finally, the number of movies played doesn’t lead to a direct conclusion as we have a quite flat distribution (we can mention that they are mainly positive values)\n\n\n\nThe raw distribution we gathered above could be not relevant and could lead to wrong interpretation because the dataset is not equally balanced among all attributes (for example the gender distribution). Therefore, if we want to make better observations, we need to compare it with what we expected to see for random edges between actors. We worked on two different possibility for the comparison that we will present to you below.\n\n\nThe first idea is to think about probabilities. For categorical attributes, we can calculate the probability for an actor to randomly be assigned to this attribute knowing the initial distribution over the dataset. Therefore, we decided to create a “mean actor” which is the hypothetical actor that we could get randomly. Each actor of the dataset will be linked to this mean actor and their associated homophily score shall be detailled as followed : \\[  Homophily_{i} = p_{attribute} - (1- p_{attribute}) \\] with $ p_{attribute} $ being the probability that “mean actor” have the same attribute as the actor \\(i\\).\n\n\n\n\n\n\n\n\n\n\nAs first comparison observation, nationaties seems to be an attribute relevant for similarity in actor connections. On the other hand, we can’t get much information from gender comparison except that homophily score for men is slightly higher than the random case and oppositely women actress tends to have lower homophily score in the true dataset compared to the “mean” case.\n\n\n\nThe second option for comparison is the randomize the edges (La Fond and Neville 2010). We keep the degree of each actor, but instead of their true links, each edge will be repaced by an edge with a random actor in the dataset. With that, we can once again compute their homophily score. The advantage here is that we obtain a distribution that could be compared to the raw distributions we obtained earlier (and we can also perfom it on scalar attribute comapared to the “mean actor”)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_value for the mann_whitney_u_test on the distributions :\n\n\n\n\n\n\n\n  \n    \n      \n      attribute\n      p-values\n    \n  \n  \n    \n      0\n      number of movies\n      2.461082e-31\n    \n    \n      1\n      gender\n      0.000000e+00\n    \n    \n      2\n      nationalities\n      1.803844e-255\n    \n    \n      3\n      age\n      3.805015e-09\n    \n    \n      4\n      height\n      0.000000e+00\n    \n  \n\n\n\n\nBased on the results illustrated above, we can make different conclusions. Firstly, we obtained more or less the same interpretation for the categorical attributes. It seems like actors from a certain country are more quite likely to play together (we can think about cinemar studios as hollywood, bollyhood, …). Moreover, the age of the actor looks like this is not a relevant attribute to link actor in movies. The only observation we can made is that the true edges are slightly shifted to the left meaning that is decrease the total homophily of the network compared to the random edges. Furthermore, even though gender initialy seems to be significant for similarity, it appears that on our dataset, it is not a relevant attribute that connect more actor (We can think about a certain diversity of gender in the movies. Maybe in a further analysis, we could have a look on how this homophily evolve depending on the movie’s genre). Finaly, height is not a significant attribute to connect people as we can see in the plot above, especially because we obtain the same distribution either with random edges and the true edges.\n\n# NetworkX Research, not relevant for the web story\n#| echo: false\nimport networkx as nx"
  },
  {
>>>>>>> 33a5844 (added conclusion)
    "objectID": "index.html",
    "href": "index.html",
    "title": "The world of actors",
    "section": "",
    "text": "Abstract\nThe well-known concept of six degrees of separation descibes that one can link anyone to any other person by a “friend of a friend” chain of length six. Our project stems from this idea : we aim to analyze the graph of movie actors formed by linking actors having played in the same movie, or by various similarity metrics. This field is called SNA, standing for Social Network Analysis (Tabassum et al. 2018) and has been quite active in the last decade. With this in mind, we want to look at how our social graph can relate to real world’s social interactions. Our main approach consists in investigating the connectedness of actors in different subsets of our graph, according to different features such as countries and release year. We can ask ourselves how similarities between actors reflect themselves in the connectedness of actors. Can the time evolution of the social graph reflect geopolitical events, and reveal information about the influence of star actors on the future career of newcomers ?\n\n\n\n\n\nReferences\n\nTabassum, Shazia, Fabiola Pereira, Sofia Fernandes, and João Gama. 2018. “Social Network Analysis: An Overview.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8 (April): e1256. https://doi.org/10.1002/widm.1256."
  },
  {
<<<<<<< HEAD
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Source data for this project.\n\nhttp://www.cs.cmu.edu/~ark/personas"
=======
    "objectID": "viz.html",
    "href": "viz.html",
    "title": "Shorthest path visualization",
    "section": "",
    "text": "On this page you can explore the links between you favorite actors ! Just choose in the inputs below and the shortest path between the both will be displayed, if it exists.\nThe canvas here is limited so some node might not be displayed correctly, in that case you can just move the nodes around. Also you can hover over the nodes and links, if the text is difficult to read.\nQuick disclaimer: this page might take some time to load because we need to load the entire graph to do real time computation.\n\n\nShow the code\nfunction Graph() {\n  var neighbors = this.neighbors = {};\n  this.addEdge = function (u, v, val) {\n    if (neighbors[u] === undefined) {\n      neighbors[u] = [];\n    }\n    neighbors[u].push(({id: v, value: val}));\n    if (neighbors[v] === undefined) {\n      neighbors[v] = [];\n    }           \n    neighbors[v].push(({id: u, value: val}));\n  };\n\n  return this;\n}\n\nfunction shortestPath(graph, source, target) {\n  if (source == target) {\n    return source;\n  }\n  var queue = [ source ],\n      visited = { [source]: true },\n      predecessor = {},\n      tail = 0;\n  while (tail < queue.length) {\n    var u = queue[tail++],\n        neighbors = graph.neighbors[u];\n    for (var i = 0; i < neighbors.length; ++i) {\n      var v = neighbors[i];\n      if (visited[v.id]) {\n        continue;\n      }\n      visited[v.id] = true;\n      if (v.id === target) {\n        var path = [ v ];\n        if (u !== source) {\n          path.push(({id: u, value: v.value}));\n          u = predecessor[u]; \n        } else {\n          path.push(({id: u, value: v.value}));\n          return path;\n        }\n        while (u.id !== source) {\n          path.push(u);\n          u = predecessor[u.id];      \n        }\n        path.push(u);\n        path.reverse();\n        return path;\n      }\n      predecessor[v.id] = ({id: u, value: v.value});\n      queue.push(v.id);\n    }\n  }\n  return [];\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nd3 = require(\"d3\")\nunderscore = require(\"underscore\")\nnodes_zip = FileAttachment(\"/data/js_graph/nodes.json.zip\").zip()\nnames = nodes.map(x => x.name)\nedges_zip = FileAttachment(\"/data/js_graph/edges.json.zip\").zip()\nnodes = nodes_zip.file(nodes_zip.filenames[0]).json()\nedges = edges_zip.file(edges_zip.filenames[0]).json()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction get_graph(edges) {\n  var g = new Graph();\n  edges.forEach(x => g.addEdge(x.source, x.target, x.value));\n  return g;\n}\ngraph = get_graph(edges)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction transform_dict_id(nodes) {\n  var dict = {};\n  nodes.forEach(x => dict[x.id] = x);\n  return dict;\n}\nid_to_node = transform_dict_id(nodes)\nfunction transform_dict_name(nodes) {\n  var dict = {};\n  nodes.forEach(x => dict[x.name] = x);\n  return dict;\n}\nname_to_node = transform_dict_name(nodes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoose the first Actor:\n\n\nShow the code\nviewof search1 = Inputs.search(names)\nviewof actor1 = Inputs.select(underscore.sample(search1, 100))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoose the second Actor:\n\n\nShow the code\nviewof search2 = Inputs.search(names)\nviewof actor2 = Inputs.select(underscore.sample(search2, 100))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nactor1_node = name_to_node[actor1]\nactor2_node = name_to_node[actor2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npath = {\n  if (graph.neighbors !== undefined && actor1 !== null && actor2 !== null && actor1 !== actor2) {\n    return shortestPath(graph, actor1_node.id, actor2_node.id);\n  } else {\n    return [];\n  }\n}\npathNodes = path.map(x => id_to_node[x.id])\npathNodes.forEach(x => x[\"group\"] = \"Shortest path\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npathEdges = path.reduce((xs, x) => {\n  var last = xs.pop()\n  if (last != undefined) {\n    xs.push({source: last.id, target: x.id, value: last.value, group: \"Shortest path\"})\n  }\n  xs.push(x)\n  return xs\n}, [])\nuseless = pathEdges.pop()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction get_neigh_node(pathNodes) {\n  var pathNId = pathNodes.map(x => x.id);\n  var pathNeighDict = pathNodes.flatMap(x => graph.neighbors[x.id]).filter(x => !pathNId.includes(x.id));\n  var pathNeigh = {};\n  pathNeighDict.map(x => {\n    if (pathNeigh[x.id] === undefined) {\n      var n = id_to_node[x.id];\n      n[\"count\"] = 1;\n      pathNeigh[x.id] = n;\n    } else {\n      pathNeigh[x.id][\"count\"] += 1;\n    }\n  })\n  var r = [];\n  Object.entries(pathNeigh).forEach(function([key, value]) {\n    var group = \"\"\n    if (value.count >= 4) {\n      group = \"4+-connected\";\n    } else {\n      group = value.count + \"-connected\";\n    }\n    r.push({id: value.id, name: value.name, group: group})\n  }); \n  return r;\n}\npathNeighNodes = get_neigh_node(pathNodes).filter(x => x.group !== \"1-connected\" && x.group !== \"2-connected\")\nfunction get_neigh_edges(pathNodes, pathNeighNodes) {\n  var pathNeighNodesId = pathNeighNodes.map(x => x.id);\n  var pathNeighDict = pathNodes.flatMap(x => graph.neighbors[x.id].filter(z => pathNeighNodesId.includes(z.id)).map(y => ({source: x.id, target: y.id, value: y.value, group: \"Neighbors\"})));\n  return pathNeighDict;\n}\npathNeighEdges = get_neigh_edges(pathNodes, pathNeighNodes)\ndata = ({\n    nodes: pathNodes.concat(pathNeighNodes),\n    links: pathEdges.concat(pathNeighEdges),\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfunction output_path_exist() {\n  if (graph.neighbors !== undefined && path !== undefined && path.length === 0) {\n    return \"There is no path between \" + actor1 + \" and \" + actor2 + \".\";\n  } else {\n    return \"\";\n  }\n}\nDOM.text(output_path_exist())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ncolors = d3.scaleOrdinal()\n  .domain([\"Shortest path\", \"3-connected\", \"4+-connected\"])\n  .range(['#4797C9','#fff686','#9e79db']);\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nchart = {\n  var height = 900;\n  var width = 900;\n  var svg = d3.create(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n    .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  var legend = svg.selectAll(\"legend\")\n    .data(colors.domain())\n    .enter()\n    .append(\"g\") \n    .attr(\"transform\", (d, i) => `translate(${width / 2 - 120},${i * 20 - height / 2 + 20})`); \n\n  legend.append(\"circle\")\n    .attr(\"cx\", 0)\n    .attr(\"cy\", 0)\n    .attr(\"r\", 5)\n    .attr(\"fill\", colors);\n\n  legend.append(\"text\")\n    .attr(\"x\", 10)\n    .attr(\"y\", 5)\n    .text(d => d);\n\n  var simulation = d3.forceSimulation()\n    .force(\"center\", d3.forceCenter())\n    .force(\"charge\", d3.forceManyBody().strength(d => {\n      if (d.group === \"Shortest path\") {\n        return -3000;\n      } else {\n        return -50;\n      }\n    }).distanceMax(450).distanceMin(85))\n    .force(\"link\", d3.forceLink().id(d => d.id));\n\n  var links = svg.selectAll(\"links\")\n    .data(data.links)\n    .enter()\n    .append(\"line\")\n    .attr(\"stroke\", \"#BDBDBD\")\n    .attr(\"stroke-width\", l => {\n      if (l.group === \"Shortest path\") {\n        return l.value.length * 5;\n      } else {\n        return 1;\n      }\n    });\n\n  links.append(\"title\").text(l => l.value);\n\n  var linkText = svg.selectAll(\"links\")\n    .data(data.links)\n    .enter()\n    .append(\"text\")\n      .attr(\"fill\", \"#0022B7\")\n    .text(l => {\n      if (l.group === \"Shortest path\") {\n        return l.value;\n      } else {\n        return \"\";\n      }\n    });\n\n  var nodes = svg.selectAll(\"nodes\")\n    .data(data.nodes)\n    .enter()\n    .append(\"g\")\n    .call(d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended));\n\n  nodes.append(\"title\").text(d => d.name);\n\n  var circles = nodes.append(\"circle\")\n    .attr(\"class\", \"circle\")\n    .attr(\"r\", d => {\n      if (d.group === \"Shortest path\") {\n        return 30;\n      } else {\n        return 5;\n      }\n    })\n    .attr(\"fill\", d => colors(d.group))\n    .attr(\"stroke\", \"#ffffff\")\n    .attr(\"stroke-width\", 2)\n\n  var text = nodes.append(\"text\")\n    .style(\"fill\", \"black\")\n    .style(\"font-weight\", \"bold\")\n    .attr(\"dx\", 0)\n    .attr(\"dy\", 5)\n    .attr(\"text-anchor\",\"middle\")\n    .text(d => {\n      if (d.group === \"Shortest path\") {\n        return d.name;\n      } else {\n        return \"\";\n      }\n    });\n\n  simulation.nodes(data.nodes);\n  simulation.force(\"link\").links(data.links)\n  simulation.on(\"tick\", function () {\n    links.attr(\"x1\", d => d.source.x)\n      .attr(\"y1\", d => d.source.y)\n      .attr(\"x2\", d => d.target.x)\n      .attr(\"y2\", d => d.target.y);\n\n    nodes.attr(\"transform\", d => \"translate(\" + d.x + \",\" + d.y + \")\")\n\n    linkText.attr(\"x\", function(d) {\n            if (d.target.x > d.source.x) { return (d.source.x + (d.target.x - d.source.x)/2); }\n            else { return (d.target.x + (d.source.x - d.target.x)/2); }\n        })\n        .attr(\"y\", function(d) {\n            if (d.target.y > d.source.y) { return (d.source.y + (d.target.y - d.source.y)/2); }\n            else { return (d.target.y + (d.source.y - d.target.y)/2); }\n        });\n  });\n\n  function dragstarted(event) {\n    if (!event.active) simulation.alphaTarget(0.3).restart();\n    event.subject.fx = event.subject.x;\n    event.subject.fy = event.subject.y;\n  };\n  \n  function dragged(event) {\n    event.subject.fx = event.x;\n    event.subject.fy = event.y;\n  };\n  \n  function dragended(event) {\n    if (!event.active) simulation.alphaTarget(0);\n    event.subject.fx = null;\n    event.subject.fy = null;\n  };\n\n  \n  \n  return Object.assign(svg.node(), {scales: d3.schemeTableau10});\n}"
>>>>>>> 33a5844 (added conclusion)
  }
]